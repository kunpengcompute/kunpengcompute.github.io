<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Kunpeng Compute Team Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://kunpengcompute.github.io/"/>
  <updated>2021-07-08T02:56:40.868Z</updated>
  <id>https://kunpengcompute.github.io/</id>
  
  <author>
    <name>鲲鹏计算开源生态团队</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>A quick sanity testing of PostgreSQL parallel query on Arm64</title>
    <link href="https://kunpengcompute.github.io/2021/07/08/a-quick-sanity-testing-of-postgresql-parallel-query-on-arm64/"/>
    <id>https://kunpengcompute.github.io/2021/07/08/a-quick-sanity-testing-of-postgresql-parallel-query-on-arm64/</id>
    <published>2021-07-08T02:52:38.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Amit Dattatray Khandekar<br>原文链接: <a href="https://amitdkhan-pg.blogspot.com/2021/04/a-quick-sanity-testing-of-parallel.html">https://amitdkhan-pg.blogspot.com/2021/04/a-quick-sanity-testing-of-parallel.html</a></p><p>PG在很久之前就已经支持并行查询，但是对于Arm64平台它是否有完备性是需要考究的。有很大的可能是还没有人真正完备的测试过，下面我们通过测试结果来看看它在ARM上的表现，同时理解并行查询的真正含义。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><h1 id="A-quick-sanity-testing-of-PostgreSQL-parallel-query-on-Arm64"><a href="#A-quick-sanity-testing-of-PostgreSQL-parallel-query-on-Arm64" class="headerlink" title="A quick sanity testing of PostgreSQL parallel query on Arm64"></a>A quick sanity testing of PostgreSQL parallel query on Arm64</h1><p>PostgreSQL已经支持查询并行很长时间了。PG社区中的人们称之为“Parallel query”，但现在它不仅限于SELECT查询。包括索引构建会均分到多个物理核心；甚至像VACUUM这样实用的程序现在也利用了并行技术。此外，社区正在努力并行化复制和插入操作。</p><p>我乐于在ARM64平台上对此功能进行“健全性”检查。让我们一起看看进展如何。同时，我们将了解如何解释计划输出的并行化部分。本文不涉及子查询和分区；我会在另一个博客中介绍。<br>为了运行查询，我使用脚本<a href="https://github.com/tvondra/pg_tpch.git">https://github.com/tvondra/pg_tpch.git</a> 生成了一个scale-5 TPC-H基准模型.测试机器是一个8 CPU虚拟机，15GB内存，Ubuntu 18.04，运行在“鲲鹏920”2.6 GHz主机上。PostgreSQL构建使用的是git master分支，因此您可以在PostgreSQL 13和14之间的某个节点来进行分析。所有测试都是在max_parallel_workers_per_gather = 4的情况下运行的。这些表是预热的，所以我将seq_page_cost和random_page_Cost降低到0.1。                                                                     </p><p>查询计划中省略了EXPLAIN输出中与JIT相关的部分，以保证关注的重点在主查询计划上。此外，为了使计划输出紧凑，我们也省略了执行估计成本。                                                      </p><h3 id="并行顺序扫描"><a href="#并行顺序扫描" class="headerlink" title="并行顺序扫描"></a>并行顺序扫描</h3><p>这是最简单的一个，也是PostgreSQL 9.6中引入查询并行性功能的其中一个。</p><p>仅仅一个简单的“select * from lineitem”不会执行并行扫描，因为所有元组都需要从workers传输到master后端。只有当这个元组传输成本足够小时，并行扫描才是有益的。因此，减少选定的行数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">tpch&#x3D;# explain (analyze, costs off)</span><br><span class="line">tpch-# select l_orderkey from lineitem  where l_shipmode &#x3D; &#39;AIR&#39; and l_shipinstruct &#x3D; &#39;TAKE BACK RETURN&#39;;</span><br><span class="line"></span><br><span class="line">                                            QUERY PLAN</span><br><span class="line">--------------------------------------------------------------------------------------------------</span><br><span class="line"> Gather (actual time&#x3D;6.264..1776.956 rows&#x3D;1070891 loops&#x3D;1)</span><br><span class="line">   Workers Planned: 4</span><br><span class="line">   Workers Launched: 4</span><br><span class="line">   -&gt;  Parallel Seq Scan on lineitem (actual time&#x3D;6.959..1640.647 rows&#x3D;214178 loops&#x3D;5)</span><br><span class="line">         Filter: ((l_shipmode &#x3D; &#39;AIR&#39;::bpchar) AND (l_shipinstruct &#x3D; &#39;TAKE BACK RETURN&#39;::bpchar))</span><br><span class="line">         Rows Removed by Filter: 5785781</span><br><span class="line"> Planning Time: 0.205 ms</span><br><span class="line"> Execution Time: 1823.987 ms</span><br><span class="line"></span><br><span class="line">So parallel sequential scan took 1824 ms to execute. Let&#39;s compare this with sequential scan :</span><br><span class="line"></span><br><span class="line">tpch&#x3D;# set max_parallel_workers_per_gather TO  0; -- Disable parallelism</span><br><span class="line"></span><br><span class="line">                                         QUERY PLAN</span><br><span class="line">--------------------------------------------------------------------------------------------</span><br><span class="line"> Seq Scan on lineitem (actual time&#x3D;117.795..5077.520 rows&#x3D;1070891 loops&#x3D;1)</span><br><span class="line">   Filter: ((l_shipmode &#x3D; &#39;AIR&#39;::bpchar) AND (l_shipinstruct &#x3D; &#39;TAKE BACK RETURN&#39;::bpchar))</span><br><span class="line">   Rows Removed by Filter: 28928904</span><br><span class="line"> Planning Time: 0.101 ms</span><br><span class="line"> Execution Time: 5123.774 ms</span><br></pre></td></tr></table></figure><p>并行顺序扫描会有2.5倍速率提升。</p><p>在进行其他查询之前，介绍一下我们需要了解的背景…并行性是通过将表的块数据分配给workers来实现的，然后并行的workers将从分给它们的块数据中读取和处理元组。但是，如何确保没有两个workers扫描同一个块呢？毕竟，它们是并行运行的，因此应该确保每个块只由一个特定的worker扫描，否则结果将返回重复的行。为了实现这一目标，workers之间要进行协调。所有的worker都需要<em>意识到</em>它们都是并行运行的，因此workers保留了一个共享的“下一个要读取的块”指针，每个worker在选择自己的下一个块后更新这个指针。将这种类型的并行计划节点称为“并行感知”；它在EXPLAIN输出中的计划名称之前有一个前缀“并行”段。处于这种并行感知的查询计划节点可能正在并行工作，但它自己可能不关心，而实际上，它只并行处理部分行集，因为并行顺序扫描正在处理其它的表块集。为了便于命名，这样的计划可以被称为“平行遗忘”(parallel-oblivious)计划。当我们讨论并行join和聚合操作时再介绍更多相关内容。</p><p>另一个经验是：聚集节点是伞形并行节点，在该节点下，子树中的所有节点都由workers来并行运行。聚集节点的工作是收集每个工作节点返回的元组，并将其传递到上层节点。聚集节点上面的所有节点通常都在其父节点后运行。不能有嵌套的聚集节点。</p><h3 id="索引扫描"><a href="#索引扫描" class="headerlink" title="索引扫描"></a>索引扫描</h3><p>以下查询没有触发并行索引扫描:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tpch&#x3D;# explain (analyze, costs off)</span><br><span class="line">select l_partkey from lineitem    where l_partkey &lt; 100000;</span><br><span class="line">                                                 QUERY PLAN</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Index Only Scan using idx_lineitem_part_supp on lineitem (actual time&#x3D;0.078..895.358 rows&#x3D;2999506 loops&#x3D;1)</span><br><span class="line">   Index Cond: (l_partkey &lt; 100000)</span><br><span class="line">   Heap Fetches: 0</span><br><span class="line"> Planning Time: 0.129 ms</span><br><span class="line"> Execution Time: 1012.693 ms</span><br></pre></td></tr></table></figure><p>因此，尝试降低parallel_tuple_cost，以便再现并行索引扫描：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tpch&#x3D;# set parallel_tuple_cost TO 0.0001;                                       </span><br><span class="line">tpch&#x3D;# explain (analyze, costs off) select l_partkey from lineitem    where l_partkey &lt; 100000;</span><br><span class="line">                                                        QUERY PLAN                                                        </span><br><span class="line">--------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Gather (actual time&#x3D;0.390..387.086 rows&#x3D;2999506 loops&#x3D;1)</span><br><span class="line">   Workers Planned: 4</span><br><span class="line">   Workers Launched: 4</span><br><span class="line">   -&gt;  Parallel Index Only Scan using idx_lineitem_part_supp on lineitem (actual time&#x3D;0.098..262.780 rows&#x3D;599901 loops&#x3D;5)</span><br><span class="line">         Index Cond: (l_partkey &lt; 100000)</span><br><span class="line">         Heap Fetches: 0</span><br><span class="line"> Planning Time: 0.802 ms</span><br><span class="line"> Execution Time: 509.306 ms</span><br></pre></td></tr></table></figure><p>注意：</p><p>parallel_tuple_cost是将元组从workers传输到leader后端的成本。注意，这里人为的设置.0001只是为了再现并行索引扫描。虽然设置它为加速索引扫描的执行时间，但不建议在没有系统的结论性统计数据的情况下更改这些成本计算参数。</p><p>Index-only扫描是一种特殊的索引扫描，因为索引已经具有select查询所需的数据，因此避免了单一次的堆扫描；仅仅扫描索引。完全的索引扫描或位图堆扫描也支持并行；我们将在聚合或表join示例中看到更多，是常见的操作。</p><p>与非并行索引扫描不同，并行索引扫描不会产生有序结果。多个workers并行读取索引块。因此，尽管每个worker返回自己排序的元组，但由于是并行无序读取索引块，结果集并不会排序。</p><p>并行索引扫描支持btree索引的扫描。</p><h3 id="并行聚合"><a href="#并行聚合" class="headerlink" title="并行聚合"></a>并行聚合</h3><p>与表行数相比，查询中的聚合表达式返回的行数通常要少得多，因为这些值是从行集返回的聚合值。因此，涉及的worker-leader元组传输成本非常低，聚合查询几乎总是由于并行性而受益。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">tpch&#x3D;# -- Check out sequential aggregate plan</span><br><span class="line">tpch&#x3D;# set max_parallel_workers_per_gather TO  0;</span><br><span class="line">tpch&#x3D;# explain (analyze, costs off) select max(l_tax) from lineitem;;</span><br><span class="line">                                   QUERY PLAN</span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line"> Aggregate (actual time&#x3D;11230.951..11230.951 rows&#x3D;1 loops&#x3D;1)</span><br><span class="line">   -&gt;  Seq Scan on lineitem (actual time&#x3D;0.009..2767.802 rows&#x3D;29999795 loops&#x3D;1)</span><br><span class="line"> Planning Time: 0.105 ms</span><br><span class="line"> Execution Time: 11231.739 ms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tpch&#x3D;# -- Check out parallel aggregate plan</span><br><span class="line">tpch&#x3D;# set max_parallel_workers_per_gather TO  4;</span><br><span class="line">tpch&#x3D;# explain (analyze, costs off) select max(l_tax) from lineitem;;</span><br><span class="line">                                            QUERY PLAN</span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line"> Finalize Aggregate (actual time&#x3D;2150.383..2190.898 rows&#x3D;1 loops&#x3D;1)</span><br><span class="line">   -&gt;  Gather (actual time&#x3D;2150.241..2190.883 rows&#x3D;5 loops&#x3D;1)</span><br><span class="line">         Workers Planned: 4</span><br><span class="line">         Workers Launched: 4</span><br><span class="line">         -&gt;  Partial Aggregate (actual time&#x3D;2137.664..2137.665 rows&#x3D;1 loops&#x3D;5)</span><br><span class="line">               -&gt;  Parallel Seq Scan on lineitem (actual time&#x3D;0.016..563.268 rows&#x3D;5999959 loops&#x3D;5)</span><br><span class="line"> Planning Time: 0.896 ms</span><br><span class="line"> Execution Time: 2202.304 ms</span><br></pre></td></tr></table></figure><p>速率提升5倍.</p><p>上面，我们可以看到，通常的Aggregate计划节点分为两种。聚集节点下面的一个是局部Aggregate节点，顾名思义，该节点仅对其自己的worker返回的值进行聚合。这意味着它还没有运行最终函数。最终的Aggregate组合了所有workers通过聚集节点返回的局部Aggregate。</p><h3 id="Joins"><a href="#Joins" class="headerlink" title="Joins"></a>Joins</h3><p>我们使用相同的查询来分析三种不同的join:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select avg(l_discount) from orders, lineitem</span><br><span class="line">where</span><br><span class="line">    l_orderkey &#x3D; o_orderkey</span><br><span class="line">    and o_orderdate &lt; date &#39;1995-03-09&#39;</span><br><span class="line">    and l_shipdate &gt; date &#39;1995-03-09&#39;;</span><br></pre></td></tr></table></figure><h3 id="Merge-Join"><a href="#Merge-Join" class="headerlink" title="Merge Join"></a>Merge Join</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">                                                                QUERY PLAN</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Finalize Aggregate (actual time&#x3D;5030.051..5241.390 rows&#x3D;1 loops&#x3D;1)</span><br><span class="line">   -&gt;  Gather (actual time&#x3D;5029.991..5241.370 rows&#x3D;5 loops&#x3D;1)</span><br><span class="line">         Workers Planned: 4</span><br><span class="line">         Workers Launched: 4</span><br><span class="line">         -&gt;  Partial Aggregate (actual time&#x3D;5015.939..5015.940 rows&#x3D;1 loops&#x3D;5)</span><br><span class="line">               -&gt;  Merge Join (actual time&#x3D;199.287..4987.159 rows&#x3D;149782 loops&#x3D;5)</span><br><span class="line">                     Merge Cond: (lineitem.l_orderkey &#x3D; orders.o_orderkey)</span><br><span class="line">                     -&gt;  Parallel Index Scan using idx_lineitem_orderkey on lineitem (actual time&#x3D;198.962..2095.747 rows&#x3D;3248732 loops&#x3D;5)</span><br><span class="line">                           Filter: (l_shipdate &gt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                           Rows Removed by Filter: 2751227</span><br><span class="line">                     -&gt;  Index Scan using orders_pkey on orders (actual time&#x3D;0.057..2343.402 rows&#x3D;3625756 loops&#x3D;5)</span><br><span class="line">                           Filter: (o_orderdate &lt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                           Rows Removed by Filter: 3874054</span><br><span class="line"> Planning Time: 0.290 ms</span><br><span class="line"> Execution Time: 5243.194 ms</span><br></pre></td></tr></table></figure><p>如上文所示，Merge Join位于聚集节点下。这意味着，Merge Join加入正在并行worker中执行。Merge Join是否正在与其他workers协调来执行？或者，换句话说，Merge Join是并行感知的吗？否。如上，Merge Join没有“并行”前缀。Merge Join需要对从外部和内部的输入进行排序，因此我们在内部和外部都进行索引扫描。现在，当在worker中执行Merge Join时，外部表的子集将与完整的内部表join。这是有可能的，因为外部是执行并行索引扫描的，而内部是正常的索引扫描，这意味着每个worker都会对内部进行完整的索引扫描。实际上，Merge Join的数据是被分割的，这是由于被并行索引扫描分割的数据，Merge Join甚至不知道它是正在并行运行！需要注意的是，每个worker必须对内侧进行冗余扫描，然后如果需要再进行排序。在我们的例子中，由于索引的原因，排序操作是不必要的。</p><p>通过适当的将两个表分区排序，并行执行分区数据集的Merge Join，使Merge Join并行感知，有一个改进点。但讨论这个问题需要另一篇博客。</p><h3 id="并行感知的-Hash-Join"><a href="#并行感知的-Hash-Join" class="headerlink" title="并行感知的 Hash Join"></a>并行感知的 Hash Join</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">                                                 QUERY PLAN                                                  </span><br><span class="line">-------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Finalize Aggregate (actual time&#x3D;3054.189..3099.784 rows&#x3D;1 loops&#x3D;1)</span><br><span class="line">   -&gt;  Gather (actual time&#x3D;3022.810..3099.755 rows&#x3D;5 loops&#x3D;1)</span><br><span class="line">         Workers Planned: 4</span><br><span class="line">         Workers Launched: 4</span><br><span class="line">         -&gt;  Partial Aggregate (actual time&#x3D;3007.931..3007.934 rows&#x3D;1 loops&#x3D;5)</span><br><span class="line">               -&gt;  Parallel Hash Join (actual time&#x3D;643.552..2980.305 rows&#x3D;149782 loops&#x3D;5)</span><br><span class="line">                     Hash Cond: (lineitem.l_orderkey &#x3D; orders.o_orderkey)</span><br><span class="line">                     -&gt;  Parallel Seq Scan on lineitem (actual time&#x3D;0.030..1685.258 rows&#x3D;3248732 loops&#x3D;5)</span><br><span class="line">                           Filter: (l_shipdate &gt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                           Rows Removed by Filter: 2751227</span><br><span class="line">                     -&gt;  Parallel Hash (actual time&#x3D;639.508..639.508 rows&#x3D;725169 loops&#x3D;5)</span><br><span class="line">                           Buckets: 4194304  Batches: 1  Memory Usage: 174688kB</span><br><span class="line">                           -&gt;  Parallel Seq Scan on orders (actual time&#x3D;14.083..384.196 rows&#x3D;725169 loops&#x3D;5)</span><br><span class="line">                                 Filter: (o_orderdate &lt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                                 Rows Removed by Filter: 774831</span><br><span class="line"> Planning Time: 0.300 ms</span><br><span class="line"> Execution Time: 3101.937 ms</span><br></pre></td></tr></table></figure><p>Hash Join内包含一个“并行哈希”节点。计划通过协调并行workers划分构建单一worker并共享哈希表。与顺序Hash Join一样，外层会等待哈希表的构建。当其构建后，相同的workers开始扫描外层表，并使用共享的哈希表执行join。外部扫描本质上是部分扫描，因为每个worker并行执行。所以在我们的例子中，这是一个并行顺序扫描。</p><h3 id="并行遗忘的-Hash-Join"><a href="#并行遗忘的-Hash-Join" class="headerlink" title="并行遗忘的 Hash Join"></a>并行遗忘的 Hash Join</h3><p>如果内部只是一个哈希节点，而不是一个“并行哈希”节点，那么这意味着：每个worker节点将构建一个单独完整的哈希表，而不是一个共享哈希表，由于没有划分哈希构建工作，这显然比并行哈希更昂贵：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">                                                            QUERY PLAN</span><br><span class="line">-----------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Finalize Aggregate (actual time&#x3D;5908.032..5971.214 rows&#x3D;1 loops&#x3D;1)</span><br><span class="line">   -&gt;  Gather (actual time&#x3D;5852.417..5971.167 rows&#x3D;5 loops&#x3D;1)</span><br><span class="line">         Workers Planned: 4</span><br><span class="line">         Workers Launched: 4</span><br><span class="line">         -&gt;  Partial Aggregate (actual time&#x3D;5850.930..5850.933 rows&#x3D;1 loops&#x3D;5)</span><br><span class="line">               -&gt;  Hash Join (actual time&#x3D;2309.307..5826.753 rows&#x3D;149782 loops&#x3D;5)</span><br><span class="line">                     Hash Cond: (lineitem.l_orderkey &#x3D; orders.o_orderkey)</span><br><span class="line">                     -&gt;  Parallel Seq Scan on lineitem (actual time&#x3D;12.631..1712.443 rows&#x3D;3248732 loops&#x3D;5)</span><br><span class="line">                           Filter: (l_shipdate &gt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                           Rows Removed by Filter: 2751227</span><br><span class="line">                     -&gt;  Hash (actual time&#x3D;2290.063..2290.065 rows&#x3D;3625845 loops&#x3D;5)</span><br><span class="line">                           Buckets: 2097152  Batches: 4  Memory Usage: 48222kB</span><br><span class="line">                           -&gt;  Bitmap Heap Scan on orders (actual time&#x3D;502.264..1512.424 rows&#x3D;3625845 loops&#x3D;5)</span><br><span class="line">                                 Recheck Cond: (o_orderdate &lt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                                 Heap Blocks: exact&#x3D;138113</span><br><span class="line">                                 -&gt;  Bitmap Index Scan on idx_orders_orderdate (actual time&#x3D;451.552..451.552 rows&#x3D;3625845 loops&#x3D;5)</span><br><span class="line">                                       Index Cond: (o_orderdate &lt; &#39;1995-03-09&#39;::date)</span><br><span class="line"> Planning Time: 0.291 ms</span><br><span class="line"> Execution Time: 5977.966 ms</span><br></pre></td></tr></table></figure><h3 id="嵌套循环-Join"><a href="#嵌套循环-Join" class="headerlink" title="嵌套循环 Join"></a>嵌套循环 Join</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">                                                      QUERY PLAN</span><br><span class="line">-----------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Finalize Aggregate (actual time&#x3D;7211.122..7258.289 rows&#x3D;1 loops&#x3D;1)</span><br><span class="line">   -&gt;  Gather (actual time&#x3D;7193.150..7258.259 rows&#x3D;5 loops&#x3D;1)</span><br><span class="line">         Workers Planned: 4</span><br><span class="line">         Workers Launched: 4</span><br><span class="line">         -&gt;  Partial Aggregate (actual time&#x3D;7129.209..7129.210 rows&#x3D;1 loops&#x3D;5)</span><br><span class="line">               -&gt;  Nested Loop (actual time&#x3D;13.924..7100.095 rows&#x3D;149782 loops&#x3D;5)</span><br><span class="line">                     -&gt;  Parallel Seq Scan on lineitem (actual time&#x3D;13.621..1919.712 rows&#x3D;3248732 loops&#x3D;5)</span><br><span class="line">                           Filter: (l_shipdate &gt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                           Rows Removed by Filter: 2751227</span><br><span class="line">                     -&gt;  Result Cache (actual time&#x3D;0.001..0.001 rows&#x3D;0 loops&#x3D;16243662)</span><br><span class="line">                           Cache Key: lineitem.l_orderkey</span><br><span class="line">                           Hits: 2450631  Misses: 844081  Evictions: 0  Overflows: 0  Memory Usage: 61379kB</span><br><span class="line">                           Worker 0:  Hits: 2443189  Misses: 841050  Evictions: 0  Overflows: 0  Memory Usage: 61158kB</span><br><span class="line">                           Worker 1:  Hits: 2350093  Misses: 808929  Evictions: 0  Overflows: 0  Memory Usage: 58824kB</span><br><span class="line">                           Worker 2:  Hits: 2424018  Misses: 833681  Evictions: 0  Overflows: 0  Memory Usage: 60615kB</span><br><span class="line">                           Worker 3:  Hits: 2417114  Misses: 830876  Evictions: 0  Overflows: 0  Memory Usage: 60407kB</span><br><span class="line">                           -&gt;  Index Scan using orders_pkey on orders (actual time&#x3D;0.004..0.004 rows&#x3D;0 loops&#x3D;4158617)</span><br><span class="line">                                 Index Cond: (o_orderkey &#x3D; lineitem.l_orderkey)</span><br><span class="line">                                 Filter: (o_orderdate &lt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                                 Rows Removed by Filter: 1</span><br><span class="line"> Planning Time: 0.294 ms</span><br><span class="line"> Execution Time: 7268.857 ms</span><br></pre></td></tr></table></figure><p>从本质上讲，嵌套循环join必须为每个外部元组扫描整个内层。因此，我们可以在workers之间划分外部扫描，并由每个worker进行完整的内部表扫描，这将为我们提供一个并行遗忘的嵌套循环join。没有必要使它具有并行意识。</p><h3 id="顺序-Join"><a href="#顺序-Join" class="headerlink" title="顺序 Join"></a>顺序 Join</h3><p>If we disable parallelism, we can see a sequential hash join. Note that all of the above parallel joins are reasonably fater than the below sequential join …<br>如果我们禁用并行功能，将会是顺序哈希join。注意，上面所有的并行Join都比下面的顺序join树状结构要胖。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">tpch&#x3D;# set max_parallel_workers_per_gather TO  0;</span><br><span class="line"></span><br><span class="line">                                                       QUERY PLAN</span><br><span class="line">-----------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Aggregate (actual time&#x3D;15714.776..15714.779 rows&#x3D;1 loops&#x3D;1)</span><br><span class="line">   -&gt;  Hash Join (actual time&#x3D;5134.219..15603.861 rows&#x3D;748912 loops&#x3D;1)</span><br><span class="line">         Hash Cond: (lineitem.l_orderkey &#x3D; orders.o_orderkey)</span><br><span class="line">         -&gt;  Bitmap Heap Scan on lineitem (actual time&#x3D;2837.938..7162.214 rows&#x3D;16243662 loops&#x3D;1)</span><br><span class="line">               Recheck Cond: (l_shipdate &gt; &#39;1995-03-09&#39;::date)</span><br><span class="line">               Heap Blocks: exact&#x3D;607593</span><br><span class="line">               -&gt;  Bitmap Index Scan on idx_lineitem_shipdate (actual time&#x3D;2556.845..2556.845 rows&#x3D;16243662 loops&#x3D;1)</span><br><span class="line">                     Index Cond: (l_shipdate &gt; &#39;1995-03-09&#39;::date)</span><br><span class="line">         -&gt;  Hash (actual time&#x3D;2290.201..2290.202 rows&#x3D;3625845 loops&#x3D;1)</span><br><span class="line">               Buckets: 2097152  Batches: 4  Memory Usage: 48222kB</span><br><span class="line">               -&gt;  Bitmap Heap Scan on orders (actual time&#x3D;563.536..1548.176 rows&#x3D;3625845 loops&#x3D;1)</span><br><span class="line">                     Recheck Cond: (o_orderdate &lt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                     Heap Blocks: exact&#x3D;138113</span><br><span class="line">                     -&gt;  Bitmap Index Scan on idx_orders_orderdate (actual time&#x3D;333.284..333.285 rows&#x3D;3625845 loops&#x3D;1)</span><br><span class="line">                           Index Cond: (o_orderdate &lt; &#39;1995-03-09&#39;::date)</span><br><span class="line"> Planning Time: 0.267 ms</span><br><span class="line"> Execution Time: 15727.275 ms</span><br></pre></td></tr></table></figure><h3 id="Gather-Merge"><a href="#Gather-Merge" class="headerlink" title="Gather Merge"></a>Gather Merge</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tpch&#x3D;# explain (analyze, costs off)</span><br><span class="line">select  l_orderkey from lineitem where l_suppkey &gt; 10000 order by l_suppkey ;</span><br><span class="line">                                          QUERY PLAN</span><br><span class="line">----------------------------------------------------------------------------------------------</span><br><span class="line"> Gather Merge (actual time&#x3D;3351.705..8310.367 rows&#x3D;23998124 loops&#x3D;1)</span><br><span class="line">   Workers Planned: 4</span><br><span class="line">   Workers Launched: 4</span><br><span class="line">   -&gt;  Sort (actual time&#x3D;3181.446..3896.115 rows&#x3D;4799625 loops&#x3D;5)</span><br><span class="line">         Sort Key: l_suppkey</span><br><span class="line">         Sort Method: external merge  Disk: 136216kB</span><br><span class="line">         Worker 0:  Sort Method: external merge  Disk: 120208kB</span><br><span class="line">         Worker 1:  Sort Method: external merge  Disk: 116392kB</span><br><span class="line">         Worker 2:  Sort Method: external merge  Disk: 123520kB</span><br><span class="line">         Worker 3:  Sort Method: external merge  Disk: 114264kB</span><br><span class="line">         -&gt;  Parallel Seq Scan on lineitem (actual time&#x3D;55.688..915.160 rows&#x3D;4799625 loops&#x3D;5)</span><br><span class="line">               Filter: (l_suppkey &gt; 10000)</span><br><span class="line">               Rows Removed by Filter: 1200334</span><br><span class="line"> Planning Time: 0.102 ms</span><br><span class="line"> Execution Time: 9654.078 ms</span><br></pre></td></tr></table></figure><p>Gather Merge是聚集计划的改进版。它基本上将排序并行了。因此，聚集从worker中获取排序输出，然后合并它们并返回排序后的输出。</p><p>顺序排序几乎花了两倍的时间:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">                                   QUERY PLAN                                    </span><br><span class="line">---------------------------------------------------------------------------------</span><br><span class="line"> Sort (actual time&#x3D;14399.200..18068.514 rows&#x3D;23998124 loops&#x3D;1)</span><br><span class="line">   Sort Key: l_suppkey</span><br><span class="line">   Sort Method: external merge  Disk: 610560kB</span><br><span class="line">   -&gt;  Seq Scan on lineitem (actual time&#x3D;16.346..4320.823 rows&#x3D;23998124 loops&#x3D;1)</span><br><span class="line">         Filter: (l_suppkey &gt; 10000)</span><br><span class="line">         Rows Removed by Filter: 6001671</span><br><span class="line"> Planning Time: 0.086 ms</span><br><span class="line"> Execution Time: 20015.980 ms</span><br></pre></td></tr></table></figure><p>还有很多其他的场景可以观察到并行性，但我可能会在后面的博客中讨论它。</p></div><div id="English" class="tab-content"><p>Query parallelism is supported in PostgreSQL since quite a while now. People in the PG community call it “Parallel query”, but by now it is not limited to just SELECT queries. Index build leverages multiple cores; and even utilities like VACUUM now make use of parallelism. Furthermore, community is working on parallelizing COPY and INSERTs.</p><p>I was interested to do kind-of “sanity” check of this capability specifically on ARM64 platform. Let’s see how it goes. And also at the same time, we will try to understand little bit of how to interpret the parallelism part of the plan output. Subqueries and partitions are not covered in this blog; probably I will add it in another blog.</p><p>For running the queries I generated a scale-5 TPC-H benchmark schema with the help of scripts taken from <a href="https://github.com/tvondra/pg_tpch.git">https://github.com/tvondra/pg_tpch.git</a>. My machine is an 8 CPU VM with 15GB memory and Ubuntu 18.04, running on a “Kunpeng 920” 2.6 GHz host. The PostgreSQL build was using git master branch,  so you can treat it somewhere between PostgreSQL 13 and 14. All the tests were run with max_parallel_workers_per_gather = 4. The tables were pre-warmed, so I reduced seq_page_cost and random_page_cost to as low as 0.1.</p><p>The JIT-related part of the EXPLAIN output is omitted from the plans to keep the focus on the main query plan. Also, estimated costs are omitted in order to make the plan output compact.</p><h3 id="Parallel-sequential-scan"><a href="#Parallel-sequential-scan" class="headerlink" title="Parallel sequential scan"></a>Parallel sequential scan</h3><p>This is the simplest one, and the one with which query parallelism got introduced in PostgreSQL 9.6.</p><p>Just a plain “select * from lineitem” won’t give us a parallel scan, because all the tuples need to be transferred from workers to the leader backend. Parallel scan is beneficial only when this tuple transfer cost is small enough. So let’s reduce the number of rows selected :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">tpch&#x3D;# explain (analyze, costs off)</span><br><span class="line">tpch-# select l_orderkey from lineitem  where l_shipmode &#x3D; &#39;AIR&#39; and l_shipinstruct &#x3D; &#39;TAKE BACK RETURN&#39;;</span><br><span class="line"></span><br><span class="line">                                            QUERY PLAN</span><br><span class="line">--------------------------------------------------------------------------------------------------</span><br><span class="line"> Gather (actual time&#x3D;6.264..1776.956 rows&#x3D;1070891 loops&#x3D;1)</span><br><span class="line">   Workers Planned: 4</span><br><span class="line">   Workers Launched: 4</span><br><span class="line">   -&gt;  Parallel Seq Scan on lineitem (actual time&#x3D;6.959..1640.647 rows&#x3D;214178 loops&#x3D;5)</span><br><span class="line">         Filter: ((l_shipmode &#x3D; &#39;AIR&#39;::bpchar) AND (l_shipinstruct &#x3D; &#39;TAKE BACK RETURN&#39;::bpchar))</span><br><span class="line">         Rows Removed by Filter: 5785781</span><br><span class="line"> Planning Time: 0.205 ms</span><br><span class="line"> Execution Time: 1823.987 ms</span><br><span class="line"></span><br><span class="line">So parallel sequential scan took 1824 ms to execute. Let&#39;s compare this with sequential scan :</span><br><span class="line"></span><br><span class="line">tpch&#x3D;# set max_parallel_workers_per_gather TO  0; -- Disable parallelism</span><br><span class="line"></span><br><span class="line">                                         QUERY PLAN</span><br><span class="line">--------------------------------------------------------------------------------------------</span><br><span class="line"> Seq Scan on lineitem (actual time&#x3D;117.795..5077.520 rows&#x3D;1070891 loops&#x3D;1)</span><br><span class="line">   Filter: ((l_shipmode &#x3D; &#39;AIR&#39;::bpchar) AND (l_shipinstruct &#x3D; &#39;TAKE BACK RETURN&#39;::bpchar))</span><br><span class="line">   Rows Removed by Filter: 28928904</span><br><span class="line"> Planning Time: 0.101 ms</span><br><span class="line"> Execution Time: 5123.774 ms</span><br></pre></td></tr></table></figure><p>So parallel seqscan was around 2.5 times faster.</p><p>Just a background before we go for other queries … Parallelism is achieved by distributing  the table blocks to the workers, and the parallel workers would then do their job of reading and processing tuples from the blocks they read. But how is it made sure that no two workers scan the same block ? After all, they are running in parallel, so they should make sure that each block should be scanned only by one particular worker, otherwise duplicate rows would be returned. To make this happen, there is a coordination between the workers. They all are <em>aware</em> that they are all running in parallel, so they keep a shared “next block to read” pointer, which each worker updates once it chooses it’s own next block. This type of parallel plan node is called “parallel-aware”; it has a prefix “Parallel” before the plan name in the EXPLAIN output. A plan node sitting on top of such parallel-aware node might itself be running in a parallel worker, but it may not be aware of it, while actually it is processing only a partial set of rows in parallel since the underlying parallel-seq scan is processing its own set of table blocks. Such plan can be called as “parallel-oblivious” plan, for the sake of naming it. We will talk about this more when we discuss parallel joins and aggregates.</p><p>Another thumb-rule is : A Gather node is the umbrella parallel node, under which all the nodes in the subtree are run in parallel by workers. A Gather node’s job is to gather the tuples returned by each worker, and pass it on to the upper node. All the nodes above Gather run in the usual parent backend. There cannot be nested Gather nodes.</p><h3 id="Index-Scan"><a href="#Index-Scan" class="headerlink" title="Index Scan"></a>Index Scan</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">The following query didn&#39;t produce a parallel index scan :                      </span><br><span class="line"></span><br><span class="line">tpch&#x3D;# explain (analyze, costs off)</span><br><span class="line">select l_partkey from lineitem    where l_partkey &lt; 100000;</span><br><span class="line">                                                 QUERY PLAN</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Index Only Scan using idx_lineitem_part_supp on lineitem (actual time&#x3D;0.078..895.358 rows&#x3D;2999506 loops&#x3D;1)</span><br><span class="line">   Index Cond: (l_partkey &lt; 100000)</span><br><span class="line">   Heap Fetches: 0</span><br><span class="line"> Planning Time: 0.129 ms</span><br><span class="line"> Execution Time: 1012.693 ms</span><br><span class="line"></span><br><span class="line">So let&#39;s try reducing parallel_tuple_cost, for the sake of reproducing a parallel index scan :</span><br><span class="line">tpch&#x3D;# set parallel_tuple_cost TO 0.0001;                                       </span><br><span class="line">tpch&#x3D;# explain (analyze, costs off) select l_partkey from lineitem    where l_partkey &lt; 100000;</span><br><span class="line">                                                        QUERY PLAN                                                        </span><br><span class="line">--------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Gather (actual time&#x3D;0.390..387.086 rows&#x3D;2999506 loops&#x3D;1)</span><br><span class="line">   Workers Planned: 4</span><br><span class="line">   Workers Launched: 4</span><br><span class="line">   -&gt;  Parallel Index Only Scan using idx_lineitem_part_supp on lineitem (actual time&#x3D;0.098..262.780 rows&#x3D;599901 loops&#x3D;5)</span><br><span class="line">         Index Cond: (l_partkey &lt; 100000)</span><br><span class="line">         Heap Fetches: 0</span><br><span class="line"> Planning Time: 0.802 ms</span><br><span class="line"> Execution Time: 509.306 ms</span><br></pre></td></tr></table></figure><p>Notes:</p><p>parallel_tuple_cost is the cost of transferring tuples from workers to leader backend. Note that I used a contrived value of .0001 just for the sake of reproducing a parallel index scan. Although setting it is giving us an index scan with faster execution time, it is not recommended to change these costing parameters without obtaining conclusive statistics on your system.</p><p>Index-only scan is a special kind of Index Scan, in that the index already has the data required by the select query, so a separate heap scan is avoided; only index is scanned. A plain index scan or a bitmap heap scan also supports parallelism; we will see more of these in aggregate or table join examples where they are more commonly seen.</p><p>A parallel index scan does not produce ordered results, unlike a non-parallel index scan. Multiple workers read index blocks in parallel. So although each worker returns its own tuples sorted, together the result set is not sorted due to parallel index block reads.</p><p>Parellel index scan is only supported for a btree index.</p><h3 id="Parallel-Aggregate"><a href="#Parallel-Aggregate" class="headerlink" title="Parallel Aggregate"></a>Parallel Aggregate</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">An aggregate expression in a query typically has drastically less number of rows returned, compared to the number of table rows, inheritently since the values are aggregate values returned from over a row set. So there is very less worker-leader tuple transfer cost involved, so aggregate query almost always gets benefited due to parallelism.</span><br><span class="line"></span><br><span class="line">tpch&#x3D;# -- Check out sequential aggregate plan</span><br><span class="line">tpch&#x3D;# set max_parallel_workers_per_gather TO  0;</span><br><span class="line">tpch&#x3D;# explain (analyze, costs off) select max(l_tax) from lineitem;;</span><br><span class="line">                                   QUERY PLAN</span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line"> Aggregate (actual time&#x3D;11230.951..11230.951 rows&#x3D;1 loops&#x3D;1)</span><br><span class="line">   -&gt;  Seq Scan on lineitem (actual time&#x3D;0.009..2767.802 rows&#x3D;29999795 loops&#x3D;1)</span><br><span class="line"> Planning Time: 0.105 ms</span><br><span class="line"> Execution Time: 11231.739 ms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tpch&#x3D;# -- Check out parallel aggregate plan</span><br><span class="line">tpch&#x3D;# set max_parallel_workers_per_gather TO  4;</span><br><span class="line">tpch&#x3D;# explain (analyze, costs off) select max(l_tax) from lineitem;;</span><br><span class="line">                                            QUERY PLAN</span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line"> Finalize Aggregate (actual time&#x3D;2150.383..2190.898 rows&#x3D;1 loops&#x3D;1)</span><br><span class="line">   -&gt;  Gather (actual time&#x3D;2150.241..2190.883 rows&#x3D;5 loops&#x3D;1)</span><br><span class="line">         Workers Planned: 4</span><br><span class="line">         Workers Launched: 4</span><br><span class="line">         -&gt;  Partial Aggregate (actual time&#x3D;2137.664..2137.665 rows&#x3D;1 loops&#x3D;5)</span><br><span class="line">               -&gt;  Parallel Seq Scan on lineitem (actual time&#x3D;0.016..563.268 rows&#x3D;5999959 loops&#x3D;5)</span><br><span class="line"> Planning Time: 0.896 ms</span><br><span class="line"> Execution Time: 2202.304 ms</span><br></pre></td></tr></table></figure><p>So it’s 5 times faster; pretty neat.</p><p>Above, we can see that the usual Aggregate plan node is divided into two kinds of Aggregate plan nodes. The one below Gather node is the Partial Aggregate node, which, as it name implies, does an aggregate of only the values returned by its own worker. It means that it has not run the finalize function yet. That is the task of the Finalize Aggregate, which combines the partial aggregates returned by all the workers through the Gather node.</p><h3 id="Joins-1"><a href="#Joins-1" class="headerlink" title="Joins"></a>Joins</h3><p>We will analyze the three different joins using this query :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select avg(l_discount) from orders, lineitem</span><br><span class="line">where</span><br><span class="line">    l_orderkey &#x3D; o_orderkey</span><br><span class="line">    and o_orderdate &lt; date &#39;1995-03-09&#39;</span><br><span class="line">    and l_shipdate &gt; date &#39;1995-03-09&#39;;</span><br></pre></td></tr></table></figure><h3 id="Merge-Join-1"><a href="#Merge-Join-1" class="headerlink" title="Merge Join"></a>Merge Join</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">                                                                QUERY PLAN</span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Finalize Aggregate (actual time&#x3D;5030.051..5241.390 rows&#x3D;1 loops&#x3D;1)</span><br><span class="line">   -&gt;  Gather (actual time&#x3D;5029.991..5241.370 rows&#x3D;5 loops&#x3D;1)</span><br><span class="line">         Workers Planned: 4</span><br><span class="line">         Workers Launched: 4</span><br><span class="line">         -&gt;  Partial Aggregate (actual time&#x3D;5015.939..5015.940 rows&#x3D;1 loops&#x3D;5)</span><br><span class="line">               -&gt;  Merge Join (actual time&#x3D;199.287..4987.159 rows&#x3D;149782 loops&#x3D;5)</span><br><span class="line">                     Merge Cond: (lineitem.l_orderkey &#x3D; orders.o_orderkey)</span><br><span class="line">                     -&gt;  Parallel Index Scan using idx_lineitem_orderkey on lineitem (actual time&#x3D;198.962..2095.747 rows&#x3D;3248732 loops&#x3D;5)</span><br><span class="line">                           Filter: (l_shipdate &gt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                           Rows Removed by Filter: 2751227</span><br><span class="line">                     -&gt;  Index Scan using orders_pkey on orders (actual time&#x3D;0.057..2343.402 rows&#x3D;3625756 loops&#x3D;5)</span><br><span class="line">                           Filter: (o_orderdate &lt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                           Rows Removed by Filter: 3874054</span><br><span class="line"> Planning Time: 0.290 ms</span><br><span class="line"> Execution Time: 5243.194 ms</span><br></pre></td></tr></table></figure><p>As you can see above, Merge Join is under a Gather node. That means, Merge Join is being executed in a parallel worker. Is the merge join being executed using some coordination with other workers ? Or, in other words, is Merge Join parallel-aware ? No. As you can see, the Merge Join does not have a “Parallel” prefix. Merge Join needs sorted input from both outer side and inner side, hence we have index scans both at inner side and outer side. Now, when a Merge Join is executed in a worker, a subset of outer side table is joined with full inner side. This is possible because the outer side is scanned using parallel Index Scan, and the inner side is a normal Index scan which means each worker does a full Index Scan of inner side. Effectively, the Merge join data is divided, thanks to the data that got divided by underlying Parallel Index Scan, and the Merge join does not even know that it is being run in parallel ! The caveat is that the inner side has to be redundantly scanned fully by each worker, followed by a sort if required. In our case the sort operation was not necessary becaues of the index.</p><p>There is a scope for improvement to make the Merge Join parallel-aware, by appropriately partitioning sorted data of both tables and do Merge Join of the pairs of partitioned data sets in parallel. But that discussion would need a separate blog.</p><h3 id="Parallel-aware-Hash-Join"><a href="#Parallel-aware-Hash-Join" class="headerlink" title="Parallel-aware Hash Join"></a>Parallel-aware Hash Join</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">                                                 QUERY PLAN                                                  </span><br><span class="line">-------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Finalize Aggregate (actual time&#x3D;3054.189..3099.784 rows&#x3D;1 loops&#x3D;1)</span><br><span class="line">   -&gt;  Gather (actual time&#x3D;3022.810..3099.755 rows&#x3D;5 loops&#x3D;1)</span><br><span class="line">         Workers Planned: 4</span><br><span class="line">         Workers Launched: 4</span><br><span class="line">         -&gt;  Partial Aggregate (actual time&#x3D;3007.931..3007.934 rows&#x3D;1 loops&#x3D;5)</span><br><span class="line">               -&gt;  Parallel Hash Join (actual time&#x3D;643.552..2980.305 rows&#x3D;149782 loops&#x3D;5)</span><br><span class="line">                     Hash Cond: (lineitem.l_orderkey &#x3D; orders.o_orderkey)</span><br><span class="line">                     -&gt;  Parallel Seq Scan on lineitem (actual time&#x3D;0.030..1685.258 rows&#x3D;3248732 loops&#x3D;5)</span><br><span class="line">                           Filter: (l_shipdate &gt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                           Rows Removed by Filter: 2751227</span><br><span class="line">                     -&gt;  Parallel Hash (actual time&#x3D;639.508..639.508 rows&#x3D;725169 loops&#x3D;5)</span><br><span class="line">                           Buckets: 4194304  Batches: 1  Memory Usage: 174688kB</span><br><span class="line">                           -&gt;  Parallel Seq Scan on orders (actual time&#x3D;14.083..384.196 rows&#x3D;725169 loops&#x3D;5)</span><br><span class="line">                                 Filter: (o_orderdate &lt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                                 Rows Removed by Filter: 774831</span><br><span class="line"> Planning Time: 0.300 ms</span><br><span class="line"> Execution Time: 3101.937 ms</span><br></pre></td></tr></table></figure><p>The inner side of Hash Join is a “Parallel Hash” node. This plan builds a shared hash table by dividing the work among parallel coordinating workers. As with a sequential Hash Join, the outer side waits until the hash table is built. Once it is built, the same workers now start scanning the outer table and doing the join using the shared hash table. The outer scan is essentially a partial scan because each worker does it in parallel. So in our case, it’s a parallel sequential scan.</p><h3 id="Parallel-oblivious-Hash-Join"><a href="#Parallel-oblivious-Hash-Join" class="headerlink" title="Parallel-oblivious Hash Join"></a>Parallel-oblivious Hash Join</h3><p>If the inner side is just a Hash node rather than a “Parallel Hash” node, then it means: a separate full hash table will be built by each of the workers rather than having a shared hash table, which obviously would be expensive than the parallel hash due to the absence of division of hash building work:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">                                                            QUERY PLAN</span><br><span class="line">-----------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Finalize Aggregate (actual time&#x3D;5908.032..5971.214 rows&#x3D;1 loops&#x3D;1)</span><br><span class="line">   -&gt;  Gather (actual time&#x3D;5852.417..5971.167 rows&#x3D;5 loops&#x3D;1)</span><br><span class="line">         Workers Planned: 4</span><br><span class="line">         Workers Launched: 4</span><br><span class="line">         -&gt;  Partial Aggregate (actual time&#x3D;5850.930..5850.933 rows&#x3D;1 loops&#x3D;5)</span><br><span class="line">               -&gt;  Hash Join (actual time&#x3D;2309.307..5826.753 rows&#x3D;149782 loops&#x3D;5)</span><br><span class="line">                     Hash Cond: (lineitem.l_orderkey &#x3D; orders.o_orderkey)</span><br><span class="line">                     -&gt;  Parallel Seq Scan on lineitem (actual time&#x3D;12.631..1712.443 rows&#x3D;3248732 loops&#x3D;5)</span><br><span class="line">                           Filter: (l_shipdate &gt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                           Rows Removed by Filter: 2751227</span><br><span class="line">                     -&gt;  Hash (actual time&#x3D;2290.063..2290.065 rows&#x3D;3625845 loops&#x3D;5)</span><br><span class="line">                           Buckets: 2097152  Batches: 4  Memory Usage: 48222kB</span><br><span class="line">                           -&gt;  Bitmap Heap Scan on orders (actual time&#x3D;502.264..1512.424 rows&#x3D;3625845 loops&#x3D;5)</span><br><span class="line">                                 Recheck Cond: (o_orderdate &lt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                                 Heap Blocks: exact&#x3D;138113</span><br><span class="line">                                 -&gt;  Bitmap Index Scan on idx_orders_orderdate (actual time&#x3D;451.552..451.552 rows&#x3D;3625845 loops&#x3D;5)</span><br><span class="line">                                       Index Cond: (o_orderdate &lt; &#39;1995-03-09&#39;::date)</span><br><span class="line"> Planning Time: 0.291 ms</span><br><span class="line"> Execution Time: 5977.966 ms</span><br></pre></td></tr></table></figure><h3 id="Nested-Loop-Join"><a href="#Nested-Loop-Join" class="headerlink" title="Nested Loop Join"></a>Nested Loop Join</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">                                                      QUERY PLAN</span><br><span class="line">-----------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Finalize Aggregate (actual time&#x3D;7211.122..7258.289 rows&#x3D;1 loops&#x3D;1)</span><br><span class="line">   -&gt;  Gather (actual time&#x3D;7193.150..7258.259 rows&#x3D;5 loops&#x3D;1)</span><br><span class="line">         Workers Planned: 4</span><br><span class="line">         Workers Launched: 4</span><br><span class="line">         -&gt;  Partial Aggregate (actual time&#x3D;7129.209..7129.210 rows&#x3D;1 loops&#x3D;5)</span><br><span class="line">               -&gt;  Nested Loop (actual time&#x3D;13.924..7100.095 rows&#x3D;149782 loops&#x3D;5)</span><br><span class="line">                     -&gt;  Parallel Seq Scan on lineitem (actual time&#x3D;13.621..1919.712 rows&#x3D;3248732 loops&#x3D;5)</span><br><span class="line">                           Filter: (l_shipdate &gt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                           Rows Removed by Filter: 2751227</span><br><span class="line">                     -&gt;  Result Cache (actual time&#x3D;0.001..0.001 rows&#x3D;0 loops&#x3D;16243662)</span><br><span class="line">                           Cache Key: lineitem.l_orderkey</span><br><span class="line">                           Hits: 2450631  Misses: 844081  Evictions: 0  Overflows: 0  Memory Usage: 61379kB</span><br><span class="line">                           Worker 0:  Hits: 2443189  Misses: 841050  Evictions: 0  Overflows: 0  Memory Usage: 61158kB</span><br><span class="line">                           Worker 1:  Hits: 2350093  Misses: 808929  Evictions: 0  Overflows: 0  Memory Usage: 58824kB</span><br><span class="line">                           Worker 2:  Hits: 2424018  Misses: 833681  Evictions: 0  Overflows: 0  Memory Usage: 60615kB</span><br><span class="line">                           Worker 3:  Hits: 2417114  Misses: 830876  Evictions: 0  Overflows: 0  Memory Usage: 60407kB</span><br><span class="line">                           -&gt;  Index Scan using orders_pkey on orders (actual time&#x3D;0.004..0.004 rows&#x3D;0 loops&#x3D;4158617)</span><br><span class="line">                                 Index Cond: (o_orderkey &#x3D; lineitem.l_orderkey)</span><br><span class="line">                                 Filter: (o_orderdate &lt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                                 Rows Removed by Filter: 1</span><br><span class="line"> Planning Time: 0.294 ms</span><br><span class="line"> Execution Time: 7268.857 ms</span><br></pre></td></tr></table></figure><p>By nature, nested loop join has to have the whole inner side scanned for each of the outer tuple. So we can divide the outer scan among workers, and have a complete inner table scan by each of the workers, which will give us a parallel-oblivious Nested Loop Join. There is no need to make it parallel-aware.</p><h3 id="Sequential-Join"><a href="#Sequential-Join" class="headerlink" title="Sequential Join"></a>Sequential Join</h3><p>If we disable parallelism, we can see a sequential hash join. Note that all of the above parallel joins are reasonably fater than the below sequential join …</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">tpch&#x3D;# set max_parallel_workers_per_gather TO  0;</span><br><span class="line"></span><br><span class="line">                                                       QUERY PLAN</span><br><span class="line">-----------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Aggregate (actual time&#x3D;15714.776..15714.779 rows&#x3D;1 loops&#x3D;1)</span><br><span class="line">   -&gt;  Hash Join (actual time&#x3D;5134.219..15603.861 rows&#x3D;748912 loops&#x3D;1)</span><br><span class="line">         Hash Cond: (lineitem.l_orderkey &#x3D; orders.o_orderkey)</span><br><span class="line">         -&gt;  Bitmap Heap Scan on lineitem (actual time&#x3D;2837.938..7162.214 rows&#x3D;16243662 loops&#x3D;1)</span><br><span class="line">               Recheck Cond: (l_shipdate &gt; &#39;1995-03-09&#39;::date)</span><br><span class="line">               Heap Blocks: exact&#x3D;607593</span><br><span class="line">               -&gt;  Bitmap Index Scan on idx_lineitem_shipdate (actual time&#x3D;2556.845..2556.845 rows&#x3D;16243662 loops&#x3D;1)</span><br><span class="line">                     Index Cond: (l_shipdate &gt; &#39;1995-03-09&#39;::date)</span><br><span class="line">         -&gt;  Hash (actual time&#x3D;2290.201..2290.202 rows&#x3D;3625845 loops&#x3D;1)</span><br><span class="line">               Buckets: 2097152  Batches: 4  Memory Usage: 48222kB</span><br><span class="line">               -&gt;  Bitmap Heap Scan on orders (actual time&#x3D;563.536..1548.176 rows&#x3D;3625845 loops&#x3D;1)</span><br><span class="line">                     Recheck Cond: (o_orderdate &lt; &#39;1995-03-09&#39;::date)</span><br><span class="line">                     Heap Blocks: exact&#x3D;138113</span><br><span class="line">                     -&gt;  Bitmap Index Scan on idx_orders_orderdate (actual time&#x3D;333.284..333.285 rows&#x3D;3625845 loops&#x3D;1)</span><br><span class="line">                           Index Cond: (o_orderdate &lt; &#39;1995-03-09&#39;::date)</span><br><span class="line"> Planning Time: 0.267 ms</span><br><span class="line"> Execution Time: 15727.275 ms</span><br></pre></td></tr></table></figure><h3 id="Gather-Merge-1"><a href="#Gather-Merge-1" class="headerlink" title="Gather Merge"></a>Gather Merge</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tpch&#x3D;# explain (analyze, costs off)</span><br><span class="line">select  l_orderkey from lineitem where l_suppkey &gt; 10000 order by l_suppkey ;</span><br><span class="line">                                          QUERY PLAN</span><br><span class="line">----------------------------------------------------------------------------------------------</span><br><span class="line"> Gather Merge (actual time&#x3D;3351.705..8310.367 rows&#x3D;23998124 loops&#x3D;1)</span><br><span class="line">   Workers Planned: 4</span><br><span class="line">   Workers Launched: 4</span><br><span class="line">   -&gt;  Sort (actual time&#x3D;3181.446..3896.115 rows&#x3D;4799625 loops&#x3D;5)</span><br><span class="line">         Sort Key: l_suppkey</span><br><span class="line">         Sort Method: external merge  Disk: 136216kB</span><br><span class="line">         Worker 0:  Sort Method: external merge  Disk: 120208kB</span><br><span class="line">         Worker 1:  Sort Method: external merge  Disk: 116392kB</span><br><span class="line">         Worker 2:  Sort Method: external merge  Disk: 123520kB</span><br><span class="line">         Worker 3:  Sort Method: external merge  Disk: 114264kB</span><br><span class="line">         -&gt;  Parallel Seq Scan on lineitem (actual time&#x3D;55.688..915.160 rows&#x3D;4799625 loops&#x3D;5)</span><br><span class="line">               Filter: (l_suppkey &gt; 10000)</span><br><span class="line">               Rows Removed by Filter: 1200334</span><br><span class="line"> Planning Time: 0.102 ms</span><br><span class="line"> Execution Time: 9654.078 ms</span><br></pre></td></tr></table></figure><p>Gather Merge is a modified version of the Gather plan. It basically parallelizes the sort. So Gather gets sorted output from the workers, and then it merges them and returns sorted output.</p><p>A sequential Sort took almost almost twice longer :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">                                   QUERY PLAN                                    </span><br><span class="line">---------------------------------------------------------------------------------</span><br><span class="line"> Sort (actual time&#x3D;14399.200..18068.514 rows&#x3D;23998124 loops&#x3D;1)</span><br><span class="line">   Sort Key: l_suppkey</span><br><span class="line">   Sort Method: external merge  Disk: 610560kB</span><br><span class="line">   -&gt;  Seq Scan on lineitem (actual time&#x3D;16.346..4320.823 rows&#x3D;23998124 loops&#x3D;1)</span><br><span class="line">         Filter: (l_suppkey &gt; 10000)</span><br><span class="line">         Rows Removed by Filter: 6001671</span><br><span class="line"> Planning Time: 0.086 ms</span><br><span class="line"> Execution Time: 20015.980 ms</span><br></pre></td></tr></table></figure><p>There are lot of other scenarios where parallelism can be observed, but probably I will take it up in a later blog ….</p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Amit Dattatray Khandekar&lt;br&gt;原文链接: &lt;a href=&quot;https://amitdkhan-pg.blogspot.com/2021/04/a-quick-sanity-testing-of-parallel.html&quot;&gt;https://amitdkhan-pg.blogspot.com/2021/04/a-quick-sanity-testing-of-parallel.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PG在很久之前就已经支持并行查询，但是对于Arm64平台它是否有完备性是需要考究的。有很大的可能是还没有人真正完备的测试过，下面我们通过测试结果来看看它在ARM上的表现，同时理解并行查询的真正含义。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Need for external compression methods in PostgreSQL</title>
    <link href="https://kunpengcompute.github.io/2021/07/07/need-for-external-compression-methods-in-postgresql/"/>
    <id>https://kunpengcompute.github.io/2021/07/07/need-for-external-compression-methods-in-postgresql/</id>
    <published>2021-07-07T08:18:06.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Amit Dattatray Khandekar<br>原文链接: <a href="https://amitdkhan-pg.blogspot.com/2020/08/need-for-external-compression-methods.html">https://amitdkhan-pg.blogspot.com/2020/08/need-for-external-compression-methods.html</a></p><p>数据库中的压缩算法对DBA和后续的维护成本至关重要，PG社区上游正在讨论如何为用户提供自定义native压缩算法，包括软硬件压缩，对于DB产业价值很高。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><h1 id="Need-for-external-compression-methods-in-PostgreSQL"><a href="#Need-for-external-compression-methods-in-PostgreSQL" class="headerlink" title="Need for external compression methods in PostgreSQL"></a>Need for external compression methods in PostgreSQL</h1><p>现下每个数据库系统都有一些方法压缩其数据，最直白的原因是尽可能缩小数据库存储空间，缩小存储成本。尤其是在当今数据量呈指数级增长的环境下，这一功能显得很重要。另有一个比较隐式的原因是提高查询性能；思路是：较小的数据大小意味着需要扫描的数据页较少，较少的磁盘I/O和更快的数据访问。因此，如果我们不能提高查询性能，至少需要保证数据解压缩都应该足够快，不应影响查询性能。</p><p>来看看压缩级别有：页压缩、行压缩、列压缩等。列存数据库的优点是其列的压缩比非常高，因为列中存在重复结构的连续数据。另一种情况是，在行存数据库中，列值区间和空间非常大，以至于压缩列的单个值是有意义的。如果这些值不适合在单个页中，我们甚至可以单独保留在其他地方，并且该行具有指向行外压缩数据的指针。在PostgreSQL中，这种技术被称为TOAST（超大属性存储技术），其中，对于可以包含可变长度数据的列，数据将透明压缩并存储在同一行中，如果数据仍然太大，它将以较小的块作为行存储在一个单独表中，我们称其为toast表，在那里这些块本身可以被压缩，也可以不被压缩。</p><p>压缩技术可被应用于不同的目的。它不限于仅数据压缩。例如，在DB replication环境中，redo日志从主到从的传输可能会成为一个巨大的网络瓶颈，因此许多RDBMS提供压缩redo日志功能。</p><p>然后是RDBMS使用或供用户选择的压缩算法。这些尤其适用于数据压缩。由于数据是用户数据，用户数据中的特定模式可能适合特定的压缩算法，而不同的模式可能适合另一种压缩算法。这意味着，如果RDBMS提供一个选项，从已知标准压缩库列表中选择特定列或特定用户自定义类型的压缩算法，如zlib、lz4、ztd，或者，库算法很可能是一个基于客户业务需求完全定制的算法。</p><p>其次，在特定平台的压缩算法优化方面，已经取得了许多进步，并为压缩、加密和SIMD提供硬件加速器，这些加速器与CPU内核紧密耦合，然后可以通过压缩或加密算法使用这些硬件加速器。其中一个例子是<a href="https://github.com/kunpengcompute/KAEzip">鲲鹏Zlib加速引擎</a>，它提供了一个硬件使能的基础架构，用于在“鲲鹏920”ARM64处理器上进行压缩。现在我还没有机会测试这种能力，但听起来确实很有希望。</p><p>此外，压缩/加密算法往往对数据执行重复任务，这自然适合利用SIMD矢量化技术。ARM64和英特尔都有在zlib、lz4等知名库中进行相关特定平台的增强。查看此<a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/neon-programmers-guide-for-armv8-a/neon-intrinsics-chromium-case-study/adler-32">NEON Inrinsics案例研究</a>使用NEON优化zlib内部的adler-32算法。</p><p>以上这些都直接表明，RDBMS服务迫切需要为用户提供特定表或特定列的native压缩算法/库的选择。在撰写本文时，PostgreSQL使用<a href="https://doxygen.postgresql.org/pg__lzcompress_8c_source.html">其自己的内置压缩算法</a>基于LZ的toast表压缩。想象一下，如果有一个接口来选择zlib而不是内置算法。用户可以选择zlib压缩级别。再进一步，添加一个接口，供用户创建相关扩展，该扩展使用对于特定平台的硬件加速算法。</p><p>正是有这样一个proposal正在酝酿中。查看在PostgreSQL hackers Maillist中的<a href="https://www.postgresql.org/message-id/flat/CAFiTN-uUpX3ck%3DK0mLEk-G_kUQY%3DSNOTeqdaNRR9FMdQrHKebw%40mail.gmail.com#81b25677aea9423d8ebb3feebcd1af46">讨论主题</a>。这可能还有很长的路要走（在撰写本文时），但我非常希望这个功能能够进入，因为应用场景足够有说服力，如上文所述，社区对这个功能没有根本的反对意见，并且相关的开发者正在为之奋斗。</p><p>我提前merge了这个特性到我的环境，并尝试玩玩。下面是界面的外观。如果所有patch完全实现后，界面可能会有所不同，但我认为它的本质或多或少会保持不变。下面是我的测试输出；注意，这只是为了通过示例强调这个功能是多么的酷和有用，助于理解我在本博客中上面解释的内容。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE zlibtab(t TEXT COMPRESSION zlib WITH (level &#39;4&#39;));</span><br><span class="line">CREATE TABLE lztab(t TEXT);                           </span><br><span class="line">ALTER TABLE lztab ALTER COLUMN t SET COMPRESSION pglz;              </span><br><span class="line">                                         </span><br><span class="line">pgg:s2:pg$ time psql -c &quot;\copy zlibtab from text.data&quot;              </span><br><span class="line">COPY 13050                                    </span><br><span class="line">                                         </span><br><span class="line">real  0m1.344s                                 </span><br><span class="line">user  0m0.031s                                 </span><br><span class="line">sys   0m0.026s                                 </span><br><span class="line"></span><br><span class="line">pgg:s2:pg$ time psql -c &quot;\copy lztab from text.data&quot;               </span><br><span class="line">COPY 13050                                    </span><br><span class="line">                                         </span><br><span class="line">real  0m2.088s                                 </span><br><span class="line">user  0m0.008s                                 </span><br><span class="line">sys   0m0.050s                                 </span><br><span class="line">                                         </span><br><span class="line">                                         </span><br><span class="line">pgg:s2:pg$ time psql -c &quot;select pg_table_size(&#39;zlibtab&#39;::regclass), pg_table_size(&#39;lztab&#39;::regclass)&quot;</span><br><span class="line"> pg_table_size | pg_table_size                          </span><br><span class="line">---------------+---------------                         </span><br><span class="line">    1261568 |    1687552                          </span><br><span class="line"></span><br><span class="line">pgg:s2:pg$ time psql -c &quot;select NULL from zlibtab where t like &#39;0000&#39;&quot; &gt; &#x2F;dev&#x2F;null</span><br><span class="line"></span><br><span class="line">real  0m0.127s</span><br><span class="line">user  0m0.000s</span><br><span class="line">sys   0m0.002s</span><br><span class="line"></span><br><span class="line">pgg:s2:pg$ time psql -c &quot;select NULL from lztab where t like &#39;0000&#39;&quot; &gt; &#x2F;dev&#x2F;null</span><br><span class="line"></span><br><span class="line">real  0m0.050s</span><br><span class="line">user  0m0.002s</span><br><span class="line">sys   0m0.000s</span><br></pre></td></tr></table></figure><p>注意两种不同的压缩算法在压缩数据大小以及插入数据（压缩）和选择数据（解压缩）的速度方面有何不同。</p><p>你甚至可以使用与创建新索引相同的方式创建新的压缩访问方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE ACCESS METHOD pglz1 TYPE COMPRESSION HANDLER my_compression_handler;</span><br></pre></td></tr></table></figure><p>where my_compression_handler should be a PostgreSQL C function that could be created using a PostgreSQL extension. This function assigns its own implementation functions for a set of pre-defined hooks that define everything that the PostgreSQL core needs to know to make use of the compression access method :</p><p>其中my_compress_handler是可以使用PostgreSQL extension创建的PostgreSQL C函数。此函数为一组预定义的钩子分配自己的实现函数，这些钩子定义了PostgreSQL核心需要知道的压缩访问方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Datum</span><br><span class="line">my_compression_handler(PG_FUNCTION_ARGS)</span><br><span class="line">&#123;</span><br><span class="line">    CompressionAmRoutine *routine &#x3D; makeNode(CompressionAmRoutine);</span><br><span class="line"></span><br><span class="line">​    routine-&gt;cmcheck &#x3D; my_cmcheck;</span><br><span class="line">​    routine-&gt;cminitstate &#x3D; my_cminitstate;</span><br><span class="line">​    routine-&gt;cmcompress &#x3D; my_cmcompress;</span><br><span class="line">​    routine-&gt;cmdecompress &#x3D; my_cmdecompress;</span><br><span class="line">​    routine-&gt;cmdecompress_slice &#x3D; NULL;</span><br><span class="line"></span><br><span class="line">​    PG_RETURN_POINTER(routine);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This is PostgreSQL’s way of being highly extensible : Allow user to use built-in methods, but also provide a way for the user to define his/her own methods for doing the same job. All the above functions would be inside an PostgreSQL extension, that could be created using:</p><p>这是PostgreSQL高度可扩展的方式：允许用户使用内置方法，但也为用户提供了一种方法，以自定义的方法来执行相同的工作。上述所有函数都将位于PostgreSQL扩展中，可以用下面的命令创建：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTENSION my_compression;</span><br></pre></td></tr></table></figure></div><div id="English" class="tab-content"><p> Every modern database system has some way to compress its data at some level. The obvious reason for this feature is to reduce the size of it’s database, especially in today’s world where the data is growing exponentially. The less obvious reason is to improve query performance; the idea is: smaller data size means less data pages to scan, which means lesser disk i/o and faster data access. So, in any case, data de-compression should be fast enough so as not to hamper the query performance, if not improve it.</p><p>Compression is offered at different levels : page compression, row compression, column compression, etc. Columnar databases have the advantage of a very high compression ratio of its column because of presence of a repetetive pattern of contiguous data in a column. Another case is when, in a row oriented database, the column values are so large that it makes sense to compress individual values of the column. Such values can even be kept separately if they do not fit in a single page. And the row has pointers to the out-of-line compressed data. In PostgreSQL, such technique is called TOAST (The Oversized-Attribute Storage Technique), where, for columns that can contain variable-length data, the data is transparently compressed and stored in the same row, or else if it is still too large, it is stored in smaller chunks as rows in a separate table called a toast table, where these chunks themselves may or may not be compressed.</p><p>Compression is offered for different purposes. It may not be restricted for only data compression. E.g. in a replication system, the transfer of redo logs from the master to slave can become a huge network bottleneck, so many RDBMS offer to compress redo logs.</p><p>And then comes the compression algorithms that the RDBMS uses or gives options to choose. This applies especially more to data compression. Since data is user’s data, a specific pattern in the user data might suit a particular compression algorithm, while a different pattern might be suitable for another compression algorithm. Moreover, this implies that it would be far more beneficial if the RDBMS gives an option to choose a specific compression algorithm for a specific column or a specific user-defined type out of a list of well-known standard compression libraries such as zlib, lz4, ztd, snappy, gzip, etc. Or, the library algorithm may very well be a completely customized one.</p><p>Secondly, there has been a lot of advancements to optimize compression algorithms for specific platforms, and provide hardware accelerators for Compression, Encryption and SIMD that are closely coupled to CPU cores, which can then be levergaed by compression or encryption algorithms. One such example is the <a href="https://github.com/kunpengcompute/KAEzip">Kunpeng Zlib Acceleration Engine</a>, which offers a hardware-enabled infrastructure for compression on a “Kunpeng 920” ARM64 processor. I haven’t got a chance to test this capability, but it does sound promising.</p><p>Furthermore, the compression/encryption algorithms inherently do repetitive tasks over the data, which is a natural fit for leveraging SIMD vectorization. There has been independent projects going on on both ARM64 and Intel to do such platform-specific enhancements in well known libraries like zlib, lz4 etc. Check out this <a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/neon-programmers-guide-for-armv8-a/neon-intrinsics-chromium-case-study/adler-32">NEON Intrinsics case study</a> that optimizes zlib’s adler-32 algorithm using NEON intrinsics.</p><p>All this directly points to an urgent need for RDBMS servers to give users a choice for specific native compression algorithms/libraries for specific tables or specific columns. As of this writing, PostgreSQL uses <a href="https://doxygen.postgresql.org/pg__lzcompress_8c_source.html">its own built-in compression algorithm</a> based on LZ for toast table compression. Imagine if there were an interface to select zlib instead of the built-in algorithm. Further, select the zlib compression level. Still further, add an interface for users to create an extension that uses a customized algorithm native to a specific platform that uses hardware acceleration.</p><p>Well, there is exactly such a proposed feature in the making. Check out this <a href="https://www.postgresql.org/message-id/flat/CAFiTN-uUpX3ck%3DK0mLEk-G_kUQY%3DSNOTeqdaNRR9FMdQrHKebw%40mail.gmail.com#81b25677aea9423d8ebb3feebcd1af46">discussion thread</a> in the PostgreSQL hackers community. It may be a long way to go (as of this writing), but I am very hopeful of this feature going in, because the use-cases are strong enough as shown above, there are no fundamental objections to this functionality, and there are work-in-progress patches submitted.</p><p>I went ahead and applied this patch, and played around it. Roughly, below is how the interface looks like. After the patch-set fully materializes, the interface might be different, but I think the essence of it would remain more or less the same. Below is the output of my tests; please note that it is just to emphasize with examples how cool and useful this feature would be, and to make sense of whatever I explained above in this blog.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE zlibtab(t TEXT COMPRESSION zlib WITH (level &#39;4&#39;));</span><br><span class="line">CREATE TABLE lztab(t TEXT);                           </span><br><span class="line">ALTER TABLE lztab ALTER COLUMN t SET COMPRESSION pglz;              </span><br><span class="line">                                         </span><br><span class="line">pgg:s2:pg$ time psql -c &quot;\copy zlibtab from text.data&quot;              </span><br><span class="line">COPY 13050                                    </span><br><span class="line">                                         </span><br><span class="line">real  0m1.344s                                 </span><br><span class="line">user  0m0.031s                                 </span><br><span class="line">sys   0m0.026s                                 </span><br><span class="line"></span><br><span class="line">pgg:s2:pg$ time psql -c &quot;\copy lztab from text.data&quot;               </span><br><span class="line">COPY 13050                                    </span><br><span class="line">                                         </span><br><span class="line">real  0m2.088s                                 </span><br><span class="line">user  0m0.008s                                 </span><br><span class="line">sys   0m0.050s                                 </span><br><span class="line">                                         </span><br><span class="line">                                         </span><br><span class="line">pgg:s2:pg$ time psql -c &quot;select pg_table_size(&#39;zlibtab&#39;::regclass), pg_table_size(&#39;lztab&#39;::regclass)&quot;</span><br><span class="line"> pg_table_size | pg_table_size                          </span><br><span class="line">---------------+---------------                         </span><br><span class="line">    1261568 |    1687552                          </span><br><span class="line"></span><br><span class="line">pgg:s2:pg$ time psql -c &quot;select NULL from zlibtab where t like &#39;0000&#39;&quot; &gt; &#x2F;dev&#x2F;null</span><br><span class="line"></span><br><span class="line">real  0m0.127s</span><br><span class="line">user  0m0.000s</span><br><span class="line">sys   0m0.002s</span><br><span class="line"></span><br><span class="line">pgg:s2:pg$ time psql -c &quot;select NULL from lztab where t like &#39;0000&#39;&quot; &gt; &#x2F;dev&#x2F;null</span><br><span class="line"></span><br><span class="line">real  0m0.050s</span><br><span class="line">user  0m0.002s</span><br><span class="line">sys   0m0.000s</span><br></pre></td></tr></table></figure><p>Notice how two different compression algorithms differ in the compressed size, and the speed of inserting data (compression) and selecting data (decompression).</p><p>You would even be able to create a new compression access method using the same way as we do for creating a new index :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE ACCESS METHOD pglz1 TYPE COMPRESSION HANDLER my_compression_handler;</span><br></pre></td></tr></table></figure><p>where my_compression_handler should be a PostgreSQL C function that could be created using a PostgreSQL extension. This function assigns its own implementation functions for a set of pre-defined hooks that define everything that the PostgreSQL core needs to know to make use of the compression access method :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Datum</span><br><span class="line">my_compression_handler(PG_FUNCTION_ARGS)</span><br><span class="line">&#123;</span><br><span class="line">    CompressionAmRoutine *routine &#x3D; makeNode(CompressionAmRoutine);</span><br><span class="line"></span><br><span class="line">​    routine-&gt;cmcheck &#x3D; my_cmcheck;</span><br><span class="line">​    routine-&gt;cminitstate &#x3D; my_cminitstate;</span><br><span class="line">​    routine-&gt;cmcompress &#x3D; my_cmcompress;</span><br><span class="line">​    routine-&gt;cmdecompress &#x3D; my_cmdecompress;</span><br><span class="line">​    routine-&gt;cmdecompress_slice &#x3D; NULL;</span><br><span class="line"></span><br><span class="line">​    PG_RETURN_POINTER(routine);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This is PostgreSQL’s way of being highly extensible : Allow user to use built-in methods, but also provide a way for the user to define his/her own methods for doing the same job. All the above functions would be inside an PostgreSQL extension, that could be created using:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTENSION my_compression;</span><br></pre></td></tr></table></figure></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Amit Dattatray Khandekar&lt;br&gt;原文链接: &lt;a href=&quot;https://amitdkhan-pg.blogspot.com/2020/08/need-for-external-compression-methods.html&quot;&gt;https://amitdkhan-pg.blogspot.com/2020/08/need-for-external-compression-methods.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;数据库中的压缩算法对DBA和后续的维护成本至关重要，PG社区上游正在讨论如何为用户提供自定义native压缩算法，包括软硬件压缩，对于DB产业价值很高。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>号外号外，MDB认可了Huawei在社区上游的性能优化</title>
    <link href="https://kunpengcompute.github.io/2020/12/23/hao-wai-hao-wai-mdb-ren-ke-liao-huawei-zai-she-qu-shang-you-de-xing-neng-you-hua/"/>
    <id>https://kunpengcompute.github.io/2020/12/23/hao-wai-hao-wai-mdb-ren-ke-liao-huawei-zai-she-qu-shang-you-de-xing-neng-you-hua/</id>
    <published>2020-12-23T12:12:17.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者: bzhaoopenstack</p><p>2020年是非常令人难忘的一年，2020年是我们包含的无数个首次，居家隔离，在家办公，千辛万苦从老家赶往单位上班，从艰难的上班路程一直到现在越来越趋于正常的环境。回想这一整年，我们从今年年后开始组建团队在社区上游进行鲲鹏优化相关的工作，一开始我们只有两个人，到现在的初具规模，伙伴们从一个ARM优化小白成长到能够独挡一面的先驱者，这一年的经历相信会在我们的人生当中留下浓重的一笔。</p><a id="more"></a><hr><p>就在昨天，通过组内的不懈努力，我们在MariaDB开源社区上游获得社区官方的认可，团队中的大佬<a href="https://github.com/MariaDB/server/pulls?q=is%3Apr+author%3Amysqlonarm+">Krunal Bauskar</a>榜上有名，同时还有来自ARM公司的<a href="https://github.com/MariaDB/server/pulls?q=is%3Apr+author%3Aguyuqi">Gu Yuqi</a>和AWS的<a href="https://github.com/MariaDB/server/pulls?q=is%3Apr+author%3Atsahee">Tsahi Zidenberg</a>。这是社区对我们的承认，感谢社区，期望我们今后能够为越来越多的幕后英雄鼓掌。</p><p>我们浏览一下来自MariaDB Foundation的<a href="https://mariadb.org/author/vicentiu/">Vicențiu Ciorbaru</a>在<a href="https://mariadb.org/arm-improvements-in-2020/">信中</a>描述的，原文参见<a href="https://mariadb.org/arm-improvements-in-2020/">这里</a>:</p><p>在2020年， 各行各业有很多ARM架构的声音。对于MariaDB来说，情况也是一样的。首先，在今年，我们已经扩展了<a href="https://buildbot.mariadb.org/">测试基础架构</a>，用以在ARM上的覆盖更多Linux发行版（Debian,Ubuntu,Fedora,CentOS,RedHat），我们正在为所有这些Linux发行版构建MariaDB软件包。除了现有的RPM和DEB包之外，下一个MariaDB发行版的tar包还将包括可在ARM上运行的二进制文件。</p><p>所有的这些都离不开<a href="https://buildbot.mariadb.org/#/sponsor">华为</a>的帮助，华为已经给我们捐赠几台ARM构建机器用于扩展测试基础架构来满足这项工作的需求。我们坚信，只有通过在尽可能多平台上进行测试，使用尽可能多的编译器，我们才能保证MariaDB的高性能和稳定性。旁注，社区仍然缺少SPARC硬件，如果您碰巧有可用的硬件，请告诉我们。</p><p>此外，我们还要感谢<a href="https://github.com/MariaDB/server/pulls?q=is%3Apr+author%3Amysqlonarm+">Krunal Bauskar</a>, <a href="https://github.com/MariaDB/server/pulls?q=is%3Apr+author%3Aguyuqi">Gu Yuqi</a> 和<a href="https://github.com/MariaDB/server/pulls?q=is%3Apr+author%3Atsahee">Tsahi Zidenberg</a>为我们增强了ARM的性能和相关错误的修复。</p><p>为了获得最佳性能，编写代码时应考虑到底层硬件。这就是为什么性能关键代码尽可能使用本机指令的原因。这种情况就是计算crc32c校验和，由Gu Yuqi通过<a href="https://github.com/MariaDB/server/pull/772">PR 772</a>贡献。Krunal在此工作上引入了<a href="https://github.com/MariaDB/server/pull/1558">PR 1558</a>，其中统一了crc32库，并尽可能使用SIMD（单指令，多数据）包装器来确保运行高效。此外，某些任务，如检测代码或计时相关的函数调用，需要非常精确的计时器。通过添加本机ARM代码，my_timer_cycles()引入了精确的硬件计时器。这是通过<a href="https://github.com/MariaDB/server/pull/1620">PR 1620</a>实现的，与此同时还能通过编译器标志来改进原子操作性能。</p><p>我们感谢社区参与确保MariaDB利用所有可用的硬件功能，我们期待获得更多！</p><p>谢谢！</p><p>相当振奋啊，不过我们的文风不应该是纯粹的吹捧，干货还是要的。下面我们分析一下，信中提及的MariaDB在ARM上的硬件优化。</p><p><a href="https://github.com/MariaDB/server/pull/772">PR 772</a>这个PR比较好的阐释了如何将一个底层硬件功能开放到上层软件，代码结构非常清晰。从crc32.cmake自适应底层架构进行分支选择，使用ARM v8加密指令优化crc32c计算密集型任务，放弃使用原本的线性crc指令，提升CPU的稳定性。引入了新的extension, 位置在<a href="https://github.com/MariaDB/server/pull/772/files#diff-ce0e7ad79f5f94a9900f6c5e712e3e53c5ed5aaafee265057bce1fbac0bc453d">extra/crc32_armv8_neon/crc32_armv8.c</a>，都是底层汇编的适配。x86使用的是crc32b和crc32q汇编指令完成CRC32C校验值计算功能，而Arm64平台使用crc32cb、crc32ch、crc32cw、crc32cx 4个汇编指令完成CRC32C校验值计算功能。</p><table><thead><tr><th>指令</th><th>输入数据位宽（bit）</th><th>备注</th></tr></thead><tbody><tr><td>crc32cb</td><td>8</td><td>适用输入数据位宽为8bit，可用于替换x86的crc32b汇编指令。</td></tr><tr><td>crc32ch</td><td>16</td><td>适用输入数据位宽为16bit。</td></tr><tr><td>crc32cw</td><td>32</td><td>适用输入数据位宽为32bit。</td></tr><tr><td>crc32cx</td><td>64</td><td>适用输入数据位宽为64bit，可用于替换x86的crc32q汇编指令。</td></tr></tbody></table><p>更多的请参考官方文档，或者<a href="https://support.huaweicloud.com/pinsrcase-kunpengprocs/kunpengprocessor_18_0006.html">这里</a></p><p><a href="https://github.com/MariaDB/server/pull/1558">PR 1558</a>这个PR是基于<a href="https://github.com/MariaDB/server/pull/772">PR 772</a>进行的改进，引入位于<a href="https://github.com/MariaDB/server/pull/1557/files#diff-649660bc396e7e68dd1f03cfc276d2ef6604e5e5b32f9d389e5c1683414d1eed">mysys/checksum.c</a>的上层接口，用于计算表和binlog校验和。并对powerpc，X86和ARM平台都进行了优化，X86使用的是PCLMUL指令，ARM使用ACLE调用底层指令替代了默认的zlib-crc32。同时对相关checksum使用的调用进行了规整，梳理出了对应所有硬件平台的内部上层接口，包含x86, ARM, POWERPC等等硬件平台，使其尽量使用底层硬件加速，避免回退到zlib crc32软实现。回看这里，对于ARM，亮点不但是使用了底层加速，还是基于SIMD思想的。那么，</p><p>What is SIMD？</p><p>通常我们进行多媒体处理的时候,很多的数据都是16位或者8位的,如果这些程序运行在32位的机器上,那么计算机有一部分的计算单元是没有工作的.所以这是一种浪费.为了更好的使用那些被浪费的资源.SIMD就应运而生了.SIMD这种技术就是使用一条指令，但对多个相同类型和尺寸的数据进行并行处理.就像我们现实生活中的好几个人都在做同一件事情那样,这样就可以将速度提升很多倍.</p><p>这个PR干了啥？看<a href="https://github.com/MariaDB/server/pull/1557/files#diff-ce0e7ad79f5f94a9900f6c5e712e3e53c5ed5aaafee265057bce1fbac0bc453d">extra/crc32_armv8_neon/crc32_armv8.c</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;* There are multiple approaches to calculate crc.</span><br><span class="line">Approach-1: Process 8 bytes then 4 bytes then 2 bytes and then 1 bytes</span><br><span class="line">Approach-2: Process 8 bytes and remaining workload using 1 bytes</span><br><span class="line">Apporach-3: Process 64 bytes at once by issuing 8 crc call and remaining</span><br><span class="line">            using 8&#x2F;1 combination.</span><br><span class="line">Based on micro-benchmark testing we found that Approach-2 works best especially</span><br><span class="line">given small chunk of variable data. *&#x2F;</span><br></pre></td></tr></table></figure><p>充分测试后，上述第二种的性能最高，特别是在小块数据的场景中，好底层啊，算法和原理参考<a href="http://www.baiheee.com/Documents/090107/090107122214.htm">这里</a>，同时测出来的数据就是这么表现的。看着其实就是运算时合理利用闲置的硬件资源，尽量保证使用更多的硬件，ARM的有效性体现的淋漓尽致，估计后面的优化工作会有大量相关的代码优化。</p><p>然后，我抱着试试看的态度，将这两个PR进行了测试，结果还是比较喜人的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">without patch</span><br><span class="line">mysql&gt; checksum table test.sbtest1;</span><br><span class="line">+--------------+------------+</span><br><span class="line">| Table | Checksum |</span><br><span class="line">+--------------+------------+</span><br><span class="line">| test.sbtest1 | 3664919850 |</span><br><span class="line">+--------------+------------+</span><br><span class="line">1 row in set (15.06 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; checksum table test.sbtest2;</span><br><span class="line">+--------------+------------+</span><br><span class="line">| Table | Checksum |</span><br><span class="line">+--------------+------------+</span><br><span class="line">| test.sbtest2 | 2870489758 |</span><br><span class="line">+--------------+------------+</span><br><span class="line">1 row in set (15.07 sec)</span><br><span class="line"></span><br><span class="line">with patch</span><br><span class="line">-----------------</span><br><span class="line">mysql&gt; checksum table test.sbtest1;</span><br><span class="line">+--------------+------------+</span><br><span class="line">| Table | Checksum |</span><br><span class="line">+--------------+------------+</span><br><span class="line">| test.sbtest1 | 3664919850 |</span><br><span class="line">+--------------+------------+</span><br><span class="line">1 row in set (7.35 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; checksum table test.sbtest2;</span><br><span class="line">+--------------+------------+</span><br><span class="line">| Table | Checksum |</span><br><span class="line">+--------------+------------+</span><br><span class="line">| test.sbtest2 | 2870489758 |</span><br><span class="line">+--------------+------------+</span><br><span class="line">1 row in set (7.41 sec)</span><br></pre></td></tr></table></figure><p>你发现了什么？我们发现了合入后checksum时间由15秒降到了7.4秒，近50%性能提升！</p><p><a href="https://github.com/MariaDB/server/pull/1620">PR 1620</a>引入了两个东西，一个是引入了gcc <code>-moutline-atomics</code>能让应用自定义原子操作，另外将硬件时钟引入到mysql/innodb自己的时钟周期，这样，让程序代码执行时间的衡量可以精确到CPU Cycle级别，是不是有种合二为一的感觉。其实这类具体的做法比较早的就有人发出来了，可以参考<a href="https://ilinuxkernel.com/?p=1755">这里</a>,这一点说明软件可以更贴近硬件来做优化。</p><p>总的来说，硬件适配及底层硬件功能应用在ARM上也才刚刚起步，尤其是在开源社区上游，我们要做的工作还有很多，但是目前来看，ARM的潮流已经来了，苹果的M1，以及最近微软宣称自己也要开始研发自己的ARM芯片，这说明ARM的圈子慢慢变大，从最开始的ARM，到ARM + 华为 + AWS，到现在广大的群体。其实作为云厂商的AWS给我们上了非常重要的一课，排老大到底是有原因的，作为未来厂商的发展，一定是软硬结合的这种结构，所以软件生态牵引硬件这种模式应该会持续很久。</p><p>好了，ARM是否能独闯一片天地，现在的趋势说明了一切，对于鲲鹏来说，我觉得是非常好的势头，让我们拭目以待吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: bzhaoopenstack&lt;/p&gt;
&lt;p&gt;2020年是非常令人难忘的一年，2020年是我们包含的无数个首次，居家隔离，在家办公，千辛万苦从老家赶往单位上班，从艰难的上班路程一直到现在越来越趋于正常的环境。回想这一整年，我们从今年年后开始组建团队在社区上游进行鲲鹏优化相关的工作，一开始我们只有两个人，到现在的初具规模，伙伴们从一个ARM优化小白成长到能够独挡一面的先驱者，这一年的经历相信会在我们的人生当中留下浓重的一笔。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Test MariaDB CONNECT storage engine on arm64 platform</title>
    <link href="https://kunpengcompute.github.io/2020/12/15/test-mariadb-connect-storage-engine-on-arm64-platform/"/>
    <id>https://kunpengcompute.github.io/2020/12/15/test-mariadb-connect-storage-engine-on-arm64-platform/</id>
    <published>2020-12-15T07:30:02.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>Author:zhaorenhai </p><p>MariaDB’s connect storage engine is an exciting storage engine.</p><p>The reason why it’s exciting is that the storage engine can do so much. Except for OLTP field, it is almost omnipotent. It can directly use SQL to query external files. It supports various common text files and log files. As long as you tell CONNECT the record format, you can use SQL to query and analyze. It also supports common file formats such as JSON, XML and INI. In addition to supporting local files, remote data can also be queried directly through the rest interface. For local files, in addition to query, it also supports insert, delete and other functions. It can also realize the dblink function in Oracle, that is, it can remotely query the tables of another database, even different types of databases, including Mysql, PostgreSQL, Oracle, etc., and even MongoDB of NoSQL type.  Of course, on this basis, it is easier to import external files into the database, and it is more powerful and flexible than mysqlimport, because it can realize various format conversion and filtering. The same is true for exporting data to an external file. It also supports querying compressed files (currently only in ZIP format). Even binary file queries and updates are supported. The engine also supports virtual table which functions similar to Oracle’s dual. It can also support a variety of functions, such as single row to multi row, row column conversion, multi table mapping to a table.</p><p>In a word, with the CONNECT storage engine, everything can be transformed into SQL, and SQL can be transformed into everything.</p><p>Next, let’s take a look at the performance of the connect storage engine on the arm64 platform.</p><a id="more"></a><p>(note that this article is not a detailed guide to the use of CONNECT. As CONNECT involves too many contents, please refer to the official guidance for detailed guidance, various parameters, and restriction of each usage <a href="https://mariadb.com/kb/en/connect/">https://mariadb.com/kb/en/connect/</a> )</p><p>Our test platform uses Kunpeng virtual machine of Huawei cloud. The OS is Ubuntu 18.04 version.</p><p>Execute the following command:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt search mariadb |grep connect</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mariadb-plugin-connect&#x2F;bionic-updates,bionic-security 1:10.1.47-0ubuntu0.18.04.1 arm64</span><br></pre></td></tr></table></figure><p>It can be seen that MariaDB has released the CONNECT plugin of arm64 platform, which shows that the basic functions of this plugin should be OK on arm64.</p><p>If you compile MariaDB code yourself (please refer to <a href="https://kunpengcompute.github.io/2020/12/04/test-mariadb-s-s3-storage-engine-on-arm64-platform/">this blog</a>  for specific compilation methods）, you can run the connect test case, you would  find that most of the test cases  are successfully  on arm64, except some skipped due to environmental reasons .</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line">mariadb@host1:~/build-mariadb-server/mysql-test$ ./mysql-test-run --suite=connect</span><br><span class="line">Logging: /home/mariadb/server/mysql-test/mysql-test-run.pl  --suite=connect</span><br><span class="line">vardir: /home/mariadb/build-mariadb-server/mysql-test/var</span><br><span class="line">Checking leftover processes...</span><br><span class="line">Removing old var directory...</span><br><span class="line">Creating var directory '/home/mariadb/build-mariadb-server/mysql-test/var'...</span><br><span class="line">Checking supported features...</span><br><span class="line">MariaDB Version 10.6.0-MariaDB</span><br><span class="line"></span><br><span class="line"> - SSL connections supported</span><br><span class="line">   Using suites: connect</span><br><span class="line">   Collecting tests...</span><br><span class="line">   Installing system database...</span><br><span class="line"></span><br><span class="line">==============================================================================</span><br><span class="line"></span><br><span class="line">TEST                                      RESULT   TIME (ms) or COMMENT</span><br><span class="line">--------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">worker[1] Using MTR_BUILD_THREAD 300, with reserved ports 16000..16019</span><br><span class="line">connect.grant2                           [ disabled ]  Until fixed</span><br><span class="line">connect.jdbc                             [ disabled ]  Variable settings depend on machine configuration</span><br><span class="line">connect.jdbc_new                         [ disabled ]  Variable settings depend on machine configuration</span><br><span class="line">connect.jdbc_oracle                      [ disabled ]  Variable settings depend on machine configuration</span><br><span class="line">connect.jdbc_postgresql                  [ disabled ]  Variable settings depend on machine configuration</span><br><span class="line">connect.json_java_2                      [ disabled ]  Need MongoDB running and its Java Driver installed</span><br><span class="line">connect.json_java_3                      [ disabled ]  Need MongoDB running and its Java Driver installed</span><br><span class="line">connect.json_mongo_c                     [ disabled ]  Need MongoDB running and its C Driver installed</span><br><span class="line">connect.mongo_c                          [ disabled ]  Need MongoDB running and its C Driver installed</span><br><span class="line">connect.mongo_java_2                     [ disabled ]  Need MongoDB running and its Java Driver installed</span><br><span class="line">connect.mongo_java_3                     [ disabled ]  Need MongoDB running and its Java Driver installed</span><br><span class="line">connect.tbl_thread                       [ disabled ]  Bug MDEV-9844,10179,14214 03/01/2018 OB Option THREAD removed</span><br><span class="line">connect.json                             [ pass ]     23</span><br><span class="line">connect.part_file                        [ pass ]     41</span><br><span class="line">connect.part_table                       [ pass ]     46</span><br><span class="line">connect.drop-open-error                  [ pass ]      7</span><br><span class="line">connect.secure_file_priv                 [ pass ]      5</span><br><span class="line">connect.alter                            [ pass ]     27</span><br><span class="line">connect.alter_xml                        [ skipped ]  Need windows</span><br><span class="line">connect.alter_xml2                       [ pass ]      7</span><br><span class="line">connect.bin                              [ pass ]      7</span><br><span class="line">connect.csv                              [ pass ]     16</span><br><span class="line">connect.datest                           [ pass ]      4</span><br><span class="line">connect.dbf                              [ pass ]     34</span><br><span class="line">connect.dir                              [ pass ]      3</span><br><span class="line">connect.endian                           [ pass ]      6</span><br><span class="line">connect.fix                              [ pass ]     18</span><br><span class="line">connect.fmt                              [ pass ]      5</span><br><span class="line">connect.general                          [ pass ]      3</span><br><span class="line">connect.grant                            [ pass ]     68</span><br><span class="line">connect.grant3                           [ pass ]      2</span><br><span class="line">connect.index                            [ pass ]    153</span><br><span class="line">connect.infoschema-9739                  [ skipped ]  Need windows</span><br><span class="line">connect.infoschema2-9739                 [ pass ]      3</span><br><span class="line">connect.ini                              [ pass ]     22</span><br><span class="line">connect.ini_grant                        [ pass ]      9</span><br><span class="line">connect.json_udf                         [ pass ]     31</span><br><span class="line">connect.json_udf_bin                     [ pass ]     45</span><br><span class="line">connect.mrr                              [ pass ]     14</span><br><span class="line">connect.mul                              [ pass ]      5</span><br><span class="line">connect.mul_new                          [ pass ]      7</span><br><span class="line">connect.mysql                            [ pass ]     51</span><br><span class="line">connect.mysql_discovery                  [ pass ]      9</span><br><span class="line">connect.mysql_exec                       [ pass ]      9</span><br><span class="line">connect.mysql_grant                      [ pass ]      7</span><br><span class="line">connect.mysql_index                      [ pass ]    173</span><br><span class="line">connect.mysql_new                        [ pass ]     24</span><br><span class="line">connect.null                             [ pass ]      9</span><br><span class="line">connect.occur                            [ pass ]     12</span><br><span class="line">connect.odbc                             [ skipped ]  No ODBC support</span><br><span class="line">connect.odbc_firebird                    [ skipped ]  No ODBC support</span><br><span class="line">connect.odbc_oracle                      [ skipped ]  No ODBC support</span><br><span class="line">connect.odbc_postgresql                  [ skipped ]  No ODBC support</span><br><span class="line">connect.odbc_sqlite3                     [ skipped ]  No ODBC support</span><br><span class="line">connect.odbc_sqlite3_grant               [ skipped ]  No ODBC support</span><br><span class="line">connect.odbc_xls                         [ skipped ]  No ODBC support</span><br><span class="line">connect.pivot                            [ pass ]     22</span><br><span class="line">connect.tbl                              [ pass ]     13</span><br><span class="line">connect.temporary                        [ pass ]</span><br><span class="line">connect.type_inet6                       [ pass ]      2</span><br><span class="line">connect.unsigned                         [ pass ]      6</span><br><span class="line">connect.upd                              [ pass ]    218</span><br><span class="line">connect.updelx                           [ pass ]    282</span><br><span class="line">connect.updelx2                          [ pass ]      5</span><br><span class="line">connect.vcol                             [ pass ]      2</span><br><span class="line">connect.vec                              [ pass ]     10</span><br><span class="line">connect.xcol                             [ pass ]      5</span><br><span class="line">connect.xml                              [ skipped ]  Need windows</span><br><span class="line">connect.xml2                             [ pass ]     22</span><br><span class="line">connect.xml2_grant                       [ pass ]     14</span><br><span class="line">connect.xml2_html                        [ pass ]      3</span><br><span class="line">connect.xml2_mdev5261                    [ pass ]      5</span><br><span class="line">connect.xml2_mult                        [ pass ]      9</span><br><span class="line">connect.xml2_zip                         [ pass ]      4</span><br><span class="line">connect.xml_grant                        [ skipped ]  Need windows</span><br><span class="line">connect.xml_html                         [ skipped ]  Need windows</span><br><span class="line">connect.xml_mdev5261                     [ skipped ]  Need windows</span><br><span class="line">connect.xml_mult                         [ skipped ]  Need windows</span><br><span class="line">connect.xml_zip                          [ skipped ]  Need windows</span><br><span class="line"></span><br><span class="line">connect.zip                              [ pass ]     13</span><br><span class="line">--------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">The servers were restarted 3 times</span><br><span class="line">Spent 1.540 of 11 seconds executing testcases</span><br><span class="line"></span><br><span class="line">Completed: All 53 tests were successful.</span><br><span class="line"></span><br><span class="line">15 tests were skipped, 15 by the test itself.</span><br></pre></td></tr></table></figure><p>Next, let’s test several table types that the CONNECT storage engine may involve in performance.</p><p>In the configuration file of MariaDB, add the following configuration:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plugin_dir &#x3D; &#x2F;home&#x2F;mariadb&#x2F;build-mariadb-server&#x2F;storage&#x2F;connect</span><br></pre></td></tr></table></figure><p>Restart the database.</p><p>Log in to MariaDB and execute the following statement to enable the connect storage engine:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">install</span> <span class="keyword">soname</span> <span class="string">'ha_connect'</span>;</span><br></pre></td></tr></table></figure><p><strong>DOS Table Type</strong></p><p>DOS table type actually corresponds to ordinary text file. The data of each column in the file is fixed length, only the last column is variable length.</p><p>We have a file in the following format, the size is 659M, and the number of lines is more than 13 million.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">root@mariadb-arm-test:~# ls -lh /tmp/data_dos.txt</span><br><span class="line">-rw-r--r-- 1 root root 659M Dec 11 16:56 /tmp/data_dos.txt</span><br><span class="line">root@mariadb-arm-test:~# wc -l /tmp/data_dos.txt</span><br><span class="line">13819904 /tmp/data_dos.txt</span><br><span class="line">root@mariadb-arm-test:~# tail /tmp/data_dos.txt</span><br><span class="line">53975 1352378138480 dxmpjtsqpmjhltthckyniaxdw</span><br><span class="line">53976 1364107020874 wyqnxsuexajbvekyqamxpzlcoy</span><br><span class="line">53977 1374147913184 ltmwgochhwfwpzbsquuttiglcls</span><br><span class="line">53978 1382141036405 gmedqtymkbkxntltajxbhlaeomda</span><br><span class="line">53979 1391311715118 kfnwwabonxwyfauqlbeguhstbrumj</span><br><span class="line">53980 1303473032800 padqsyhvhvltcpebmtkt</span><br><span class="line">53981 1313713811959 nwduknfpyaeplwpmpozwh</span><br><span class="line">53982 1321669920517 aglfvcespetmimkugoyygu</span><br><span class="line">53983 1333059736260 ltccjcvptqcrwklutmrlles</span><br><span class="line">53984 1342207932901 vqkkrkrqkmcxvkdmiwyfylxe</span><br></pre></td></tr></table></figure><p>Let’s use CONNECT storage engine to query it.</p><p>Log in to the database and create a table:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table userlist (</span><br><span class="line">  id char(5) not null,</span><br><span class="line">  phonenum char(13) not null flag&#x3D;6,</span><br><span class="line">  userid char(30) not null flag&#x3D;20</span><br><span class="line">  )</span><br><span class="line">engine&#x3D;CONNECT table_type&#x3D;DOS file_name&#x3D;&#39;&#x2F;tmp&#x2F;data_dos.txt&#39;;</span><br></pre></td></tr></table></figure><p>It’s very fast:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br></pre></td></tr></table></figure><p>Query the total number of records,  also fast,  it’s 13 seconds:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [test]&gt; select count(*) from userlist;</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">| 13819904 |</span><br><span class="line">+----------+</span><br><span class="line">1 row in set (13.350 sec)</span><br></pre></td></tr></table></figure><p>Try the conditional query , insert, delete:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">8f3941802b868b2bbb43390ab5a2f1c8  -</span><br><span class="line">256 rows in set (9.221 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">8f3941802b868b2bbb43390ab5a2f1c8  -</span><br><span class="line">256 rows in set (9.221 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist where userid like &#39;ltccjc%&#39;;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">2bde22161d7a5bad42c377b7290cdff4  -</span><br><span class="line">256 rows in set (19.196 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist where userid like &#39;ltccjc%&#39;;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">2bde22161d7a5bad42c377b7290cdff4  -</span><br><span class="line">256 rows in set (19.279 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; insert into userlist values(&#39;76245&#39;,&#39;1812345678901&#39;, &#39;swweyuoyslwtqwtoeurwio&#39;);</span><br><span class="line">Query OK, 1 row affected (0.000 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; delete from userlist where id&#x3D;76245 and phonenum &#x3D; &#39;1812345678901&#39;;</span><br><span class="line">Query OK, 1 row affected (9.719 sec)</span><br></pre></td></tr></table></figure><p>You can see the query is fast, with 256 records queried from over 13 million records taking only 9 seconds. However, like queries are slightly slower, taking 19 seconds. Insertion is also fast. Inserting is essentially inserting a record at the end of the original file.</p><p>Update operations on tables of this DOS type are not recommended and strange things may occur. Because Update does not update the original file by default, it updates a temporary file. There is one parameter<code>connect_Use_Tempfile</code> can be configured to support updating source files, but is not recommended for files with variable length records. If you are sure you want to use this feature, it is recommended that you test it yourself before using it.</p><p><strong>FIX Table Type</strong></p><p>The difference between FIX table type and DOS is whether the width of the last column is fixed or variable,  FIX if it is fixed, or DOS if it is variable.</p><p>Now let’s test the FIX type. We have the following file, 667M in size, 13.43 million lines, and the last column is the same length.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mariadb@mariadb-arm-test:/tmp$ ls -lhrt data_fix.txt</span><br><span class="line">-rw-rw-r-- 1 mariadb mariadb 667M Dec 13 16:29 data_fix.txt</span><br><span class="line">mariadb@mariadb-arm-test:/tmp$ wc -l data_fix.txt</span><br><span class="line">13432192 data_fix.txt</span><br><span class="line">mariadb@mariadb-arm-test:/tmp$ tail data_fix.txt</span><br><span class="line">104930 1302533021891 idmujolnoimfhafacordyrrzmmjjuf</span><br><span class="line">104931 1313195411180 bwuhbnyhxscqprsizgyjkotqfyndda</span><br><span class="line">104932 1323234034390 frynxnmkttbxzibvsdzokelmnqwkwc</span><br><span class="line">104933 1331271621869 sthxlnhbfqpnrpkiahgvqrlfhrcfxu</span><br><span class="line">104934 1342061419324 hheddnlwxslnqttygyalcamwdxujpp</span><br><span class="line">104935 1351001433696 ajlhunoosohztneqfxzotfbgfdegrj</span><br><span class="line">104936 1362857527592 qraermmsdzuxoogprmxsxrqmktfrwr</span><br><span class="line">104937 1372706024878 tkozushzmqhuxkavdbcxwnmksiceji</span><br><span class="line">104938 1382111414256 osllmzidwpmdbxzkzfkelsnwcdtstr</span><br><span class="line">104939 1393077713849 dzafytqywdyahhjvhvdosnsduwwpvw</span><br></pre></td></tr></table></figure><p>Logging in to the MariaDB database, create a FIX table:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table userlist_fix (</span><br><span class="line">  id char(6) not null,</span><br><span class="line">  phonenum char(13) not null flag&#x3D;7,</span><br><span class="line">  userid char(30) not null flag&#x3D;21</span><br><span class="line">  )</span><br><span class="line">engine&#x3D;CONNECT table_type&#x3D;FIX file_name&#x3D;&#39;&#x2F;tmp&#x2F;data_fix.txt&#39; lrecl&#x3D;52;</span><br></pre></td></tr></table></figure><p>Note that lrecl=52 is the length of the longest line of bytes and contains line breaks.</p><p>Let’s repeat the query, insertion operations:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [test]&gt; select count(*) from userlist_fix;</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">| 13432193 |</span><br><span class="line">+----------+</span><br><span class="line">1 row in set (0.000 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_fix where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">0871c0088df3f4dbf37b12e56611a0b0  -</span><br><span class="line">128 rows in set (3.535 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_fix where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">0871c0088df3f4dbf37b12e56611a0b0  -</span><br><span class="line">128 rows in set (3.535 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_fix where userid like &#39;ltccjc%&#39;;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">Empty set (9.344 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_fix where userid like &#39;ltccjc%&#39;;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">Empty set (9.325 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; insert into userlist_fix values(&#39;76245&#39;,&#39;1812345678901&#39;, &#39;swweyuoyslwtqwtoeurwio&#39;);</span><br><span class="line">Query OK, 1 row affected (0.000 sec)</span><br></pre></td></tr></table></figure><p>You can see that each type of FIX type operates much faster than the DOS type, indicating that MariaDB takes advantage of the fixed-length feature in the processing of fixed-length formats and has better performance. Last line, the data we inserted is not aligned, MariaDB will automatically complete for you when it inserts the database:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mariadb@mariadb-arm-test:/tmp$ tail data_fix.txt</span><br><span class="line">104930 1302533021891 idmujolnoimfhafacordyrrzmmjjuf</span><br><span class="line">104931 1313195411180 bwuhbnyhxscqprsizgyjkotqfyndda</span><br><span class="line">104932 1323234034390 frynxnmkttbxzibvsdzokelmnqwkwc</span><br><span class="line">104933 1331271621869 sthxlnhbfqpnrpkiahgvqrlfhrcfxu</span><br><span class="line">104934 1342061419324 hheddnlwxslnqttygyalcamwdxujpp</span><br><span class="line">104935 1351001433696 ajlhunoosohztneqfxzotfbgfdegrj</span><br><span class="line">104936 1362857527592 qraermmsdzuxoogprmxsxrqmktfrwr</span><br><span class="line">104937 1372706024878 tkozushzmqhuxkavdbcxwnmksiceji</span><br><span class="line">104938 1382111414256 osllmzidwpmdbxzkzfkelsnwcdtstr</span><br><span class="line">76245  1812345678901 swweyuoyslwtqwtoeurwio</span><br></pre></td></tr></table></figure><p>Fixed-length types are also friendly to update and delete operations, are fast, and operate directly on the original file.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [test]&gt; update userlist_fix set phonenum = '1811111111111' where id= 104938;</span><br><span class="line">Query OK, 128 rows affected (3.513 sec)</span><br><span class="line">Rows matched: 128  Changed: 128  Warnings: 0</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; delete from userlist_fix where phonenum = '1811111111111';</span><br><span class="line">Query OK, 128 rows affected (3.672 sec)</span><br></pre></td></tr></table></figure><p>So when we build a table from the original text file, if it is possible to process it as FIX type, try to process it as FIX type.</p><p><strong>About connect_work_size</strong></p><p>Next let’s modify connect_work_size  parameter, which should be the memory area used by the CONNECT engine from the description of the Knowledge Base website. We enlarge it to see if it works.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [test]&gt; select @@session.connect_work_size;</span><br><span class="line">+-----------------------------+</span><br><span class="line">| @@session.connect_work_size |</span><br><span class="line">+-----------------------------+</span><br><span class="line">|                    67108864 |</span><br><span class="line">+-----------------------------+</span><br><span class="line">1 row in set (0.000 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; set session connect_work_size &#x3D; 1073741824;</span><br><span class="line">Query OK, 0 rows affected (0.000 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; select count(*) from userlist;</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">| 13819650 |</span><br><span class="line">+----------+</span><br><span class="line">1 row in set (13.576 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">8f3941802b868b2bbb43390ab5a2f1c8  -</span><br><span class="line">256 rows in set (9.306 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">8f3941802b868b2bbb43390ab5a2f1c8  -</span><br><span class="line">256 rows in set (9.305 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_fix where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">6daf8a0815666570cee0011046e3b568  -</span><br><span class="line">128 rows in set (3.526 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_fix where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">6daf8a0815666570cee0011046e3b568  -</span><br><span class="line">128 rows in set (3.526 sec)</span><br></pre></td></tr></table></figure><p>As you can see from the above output, it has no effect. When the CONNECT storage engine processes files, it does not cache the entire file, increasing the memory space, and is not useful. The reason we’re so fast is also due to the fact that our Huawei Cloud virtual machine uses SSD storage, which is faster.</p><p><strong>CSV Table Type</strong></p><p>The CSV table type corresponds to ordinary CSV text files, but the delimiter here can be specified, either as a comma or as other specified characters.</p><p>Let’s test the performance of the CSV table type.</p><p>We create a CSV file in following format, 603M in size, with over 13.9 million lines. Columns are separated by |.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mariadb@mariadb-arm-test:/tmp$ wc -l data.csv</span><br><span class="line">13909120 data.csv</span><br><span class="line">mariadb@mariadb-arm-test:/tmp$ ls -lhrt data.csv</span><br><span class="line">-rw-rw-r-- 1 mariadb mariadb 603M Dec 14 12:01 data.csv</span><br><span class="line">mariadb@mariadb-arm-test:/tmp$ tail -10 data.csv</span><br><span class="line">1411|1311008423658|hudulfqgofqahfxwtqihs</span><br><span class="line">1412|1321119524436|atacjjbibtzqwohjnrtebc</span><br><span class="line">1413|1334167818661|tnhdlrpbpaxzfyiymholbfy</span><br><span class="line">1414|1344170515956|xlwsuiotrfhmoltiscorhdec</span><br><span class="line">1415|1351643025744|httzsnqpjydxeinvwcaekgydt</span><br><span class="line">1416|1363624529884|tyyttcbsqjiwufvmpuveototmq</span><br><span class="line">1417|1371750537023|diwtmqdriozmoscypldbcdrqvfd</span><br><span class="line">1418|1383308741505|gqchktgbsstyixidjztgtpgutzeb</span><br><span class="line">1419|1393553934078|culiovhkrdovnbcdestvnqqubwdzl</span><br><span class="line">1420|1302340336368|bfjdepbcrefcfvbhznbd</span><br></pre></td></tr></table></figure><p>Create a CSV-type table using the following SQL:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table userlist_csv (</span><br><span class="line">  id int not null,</span><br><span class="line">  phonenum char(13) not null,</span><br><span class="line">  userid char(29) not null)</span><br><span class="line">engine&#x3D;CONNECT table_type&#x3D;CSV file_name&#x3D;&#39;&#x2F;tmp&#x2F;data.csv&#39;</span><br><span class="line">header&#x3D;0 sep_char&#x3D;&#39;|&#39; quoted&#x3D;0;</span><br></pre></td></tr></table></figure><p>Then we do some query ,insertion and deletion operations:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [test]&gt; select count(*) from userlist_csv;</span><br><span class="line">9229c8871853852300fca7ce0bb33598  -</span><br><span class="line">1 row in set (17.835 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; select count(*) from userlist_csv;</span><br><span class="line">9229c8871853852300fca7ce0bb33598  -</span><br><span class="line">1 row in set (17.712 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_csv where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">9a02ca9b5c3044a73f4441c10945ff91  -</span><br><span class="line">140 rows in set (9.356 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_csv where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">9a02ca9b5c3044a73f4441c10945ff91  -</span><br><span class="line">140 rows in set (9.355 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_csv where userid like &#39;ltccjc%&#39;;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">Empty set (21.788 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_csv where userid like &#39;ltccjc%&#39;;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">Empty set (21.781 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; insert into userlist_csv values(&#39;76245&#39;,&#39;1812345678901&#39;, &#39;swweyuoyslwtqwtoeurwio&#39;);</span><br><span class="line">Query OK, 1 row affected (0.000 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; update userlist_csv set phonenum&#x3D;&#39;1811111111111&#39; where id&#x3D;76245;</span><br><span class="line">Query OK, 141 rows affected (6.803 sec)</span><br><span class="line">Rows matched: 141  Changed: 141  Warnings: 0</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; delete from  userlist_csv  where id&#x3D;76245 and phonenum&#x3D;&#39;1811111111111&#39;;</span><br><span class="line">Query OK, 141 rows affected (6.674 sec)</span><br></pre></td></tr></table></figure><p>You can see that the performance is still good, count query and like query are slightly slower than the DOS type,  both query and insert operations are similar to the DOS type, and delete operation is faster than the DOS type.</p><p><strong>FMT Table Type</strong></p><p>The FMT table type is more flexible than CSV, and you can specify the format and length of each field by format matching. FMT is useful when analyzing various types of logs.</p><p>We use it to analyze an operating system auth log.</p><p>This auth log has over 600 M, more than 5 million rows.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@mariadb-arm-test:/var/log# ls -lhrt auth.log.1</span><br><span class="line">-rw-r----- 1 syslog adm 625M Dec 14 15:11 auth.log.1</span><br><span class="line">root@mariadb-arm-test:/var/log# wc -l auth.log.1</span><br><span class="line">5890048 auth.log.1</span><br></pre></td></tr></table></figure><p>Intercept part of the file content:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Dec 12 16:39:23 localhost groupadd[32556]: group added to &#x2F;etc&#x2F;group: name&#x3D;mariadb, GID&#x3D;1000</span><br><span class="line">Dec 12 16:39:23 localhost groupadd[32556]: group added to &#x2F;etc&#x2F;gshadow: name&#x3D;mariadb</span><br><span class="line">Dec 12 16:39:23 localhost groupadd[32556]: new group: name&#x3D;mariadb, GID&#x3D;1000</span><br><span class="line">Dec 12 16:39:23 localhost useradd[32560]: new user: name&#x3D;mariadb, UID&#x3D;1000, GID&#x3D;1000, home&#x3D;&#x2F;home&#x2F;mariadb, shell&#x3D;&#x2F;bin&#x2F;bash</span><br><span class="line">Dec 12 16:39:43 localhost passwd[32569]: pam_unix(passwd:chauthtok): password changed for mariadb</span><br><span class="line">Dec 12 16:39:46 localhost chfn[32570]: changed user &#39;mariadb&#39; information</span><br><span class="line">Dec 12 16:45:36 localhost su[1805]: Successful su for mariadb by root</span><br><span class="line">Dec 12 16:45:36 localhost su[1805]: + &#x2F;dev&#x2F;pts&#x2F;0 root:mariadb</span><br><span class="line">Dec 12 16:45:36 localhost su[1805]: pam_unix(su:session): session opened for user mariadb by root(uid&#x3D;0)</span><br><span class="line">Dec 12 16:45:36 localhost su[1805]: pam_systemd(su:session): Cannot create session: Already running in a session</span><br><span class="line">Dec 12 16:50:01 localhost sudo:  mariadb : TTY&#x3D;pts&#x2F;0 ; PWD&#x3D;&#x2F;home&#x2F;mariadb ; USER&#x3D;root ; COMMAND&#x3D;&#x2F;usr&#x2F;bin&#x2F;apt-get install build-essential libncurses5-dev gnutls-dev libcurl4-gnutls-dev</span><br><span class="line">Dec 12 16:50:01 localhost sudo: pam_unix(sudo:session): session opened for user root by root(uid&#x3D;0)</span><br><span class="line">Dec 12 16:50:04 localhost sudo: pam_unix(sudo:session): session closed for user root</span><br><span class="line">Dec 12 16:50:14 localhost sudo:  mariadb : TTY&#x3D;pts&#x2F;0 ; PWD&#x3D;&#x2F;home&#x2F;mariadb ; USER&#x3D;root ; COMMAND&#x3D;&#x2F;usr&#x2F;bin&#x2F;apt-get install build-essential libncurses5-dev gnutls-dev libcurl4-gnutls-dev</span><br><span class="line">Dec 12 16:50:14 localhost sudo: pam_unix(sudo:session): session opened for user root by root(uid&#x3D;0)</span><br><span class="line">Dec 12 16:50:25 localhost sudo: pam_unix(sudo:session): session closed for user root</span><br><span class="line">Dec 12 16:50:49 localhost sudo:  mariadb : TTY&#x3D;pts&#x2F;0 ; PWD&#x3D;&#x2F;home&#x2F;mariadb ; USER&#x3D;root ; COMMAND&#x3D;&#x2F;usr&#x2F;bin&#x2F;apt-get install zlib1g-dev ccache libnuma-dev libxml2-dev cmake bison</span><br><span class="line">Dec 12 16:50:49 localhost sudo: pam_unix(sudo:session): session opened for user root by root(uid&#x3D;0)</span><br><span class="line">Dec 12 16:51:08 localhost sudo: pam_unix(sudo:session): session closed for user root</span><br><span class="line">Dec 12 16:56:01 localhost CRON[7999]: pam_unix(cron:session): session opened for user root by (uid&#x3D;0)</span><br><span class="line">Dec 12 16:56:01 localhost CRON[7999]: pam_unix(cron:session): session closed for user root</span><br><span class="line">Dec 12 17:17:01 localhost CRON[16493]: pam_unix(cron:session): session opened for user root by (uid&#x3D;0)</span><br><span class="line">Dec 12 17:17:01 localhost CRON[16493]: pam_unix(cron:session): session closed for user root</span><br></pre></td></tr></table></figure><p>Create the table:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">create table authlog (</span><br><span class="line">  month char(3) not null field_format&#x3D;&#39;%n%s%n&#39;,</span><br><span class="line">  day char(2) not null field_format&#x3D;&#39; %n%s%n&#39;,</span><br><span class="line">  time char(8) not null field_format&#x3D;&#39; %n%s%n&#39;,</span><br><span class="line">  hostname char(9) not null field_format&#x3D;&#39; %n%s%n&#39;,</span><br><span class="line">  module char(20) not null field_format&#x3D;&#39; %n%s%n&#39;,</span><br><span class="line">  message char(255) not null field_format&#x3D;&#39; %n%255[^\n]%n&#39;)</span><br><span class="line">engine&#x3D;CONNECT table_type&#x3D;FMT file_name&#x3D;&#39;&#x2F;var&#x2F;log&#x2F;auth.log.1&#39; ;</span><br></pre></td></tr></table></figure><p>Query it , the format looks better now:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [test]&gt;  select * from authlog limit 10;</span><br><span class="line">+-------+-----+----------+-----------+----------------------+-----------------------------------------------------------------------+</span><br><span class="line">| month | day | time     | hostname  | module               | message                                                               |</span><br><span class="line">+-------+-----+----------+-----------+----------------------+-----------------------------------------------------------------------+</span><br><span class="line">| Dec   | 2   | 20:47:26 | localhost | systemd-logind[721]: | Power key pressed.                                                    |</span><br><span class="line">| Dec   | 2   | 20:47:26 | localhost | systemd-logind[721]: | Powering Off...                                                       |</span><br><span class="line">| Dec   | 2   | 20:47:26 | localhost | systemd-logind[721]: | System is powering down.                                              |</span><br><span class="line">| Dec   | 2   | 20:47:27 | localhost | systemd:             | pam_unix(systemd-user:session): session closed for user root          |</span><br><span class="line">| Dec   | 11  | 10:07:26 | localhost | systemd-logind[960]: | New seat seat0.                                                       |</span><br><span class="line">| Dec   | 11  | 10:07:26 | localhost | systemd-logind[960]: | Watching system buttons on &#x2F;dev&#x2F;input&#x2F;event0 (Power Button)           |</span><br><span class="line">| Dec   | 11  | 10:07:26 | localhost | systemd-logind[960]: | Watching system buttons on &#x2F;dev&#x2F;input&#x2F;event2 (QEMU QEMU USB Keyboard) |</span><br><span class="line">| Dec   | 11  | 10:07:50 | localhost | sshd[2384]:          | Server listening on 0.0.0.0 port 22.                                  |</span><br><span class="line">| Dec   | 11  | 10:07:50 | localhost | sshd[2384]:          | Server listening on :: port 22.                                       |</span><br><span class="line">| Dec   | 11  | 10:07:51 | localhost | sshd[2384]:          | Received signal 15; terminating.                                      |</span><br><span class="line">+-------+-----+----------+-----------+----------------------+-----------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><p>Let’s do some queries:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [test]&gt; select count(*) from authlog where month&#x3D;&#39;Dec&#39; and day&#x3D;&#39;12&#39; and module like &#39;sshd%&#39;;</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">|  3170304 |</span><br><span class="line">+----------+</span><br><span class="line">1 row in set (15.609 sec)</span><br></pre></td></tr></table></figure><p>The speed is acceptable. </p><p>Of course, if you use the grep command to filter in the following way, it will be faster:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@mariadb-arm-test:/var/log# echo 3 &gt; /proc/sys/vm/drop_caches</span><br><span class="line">root@mariadb-arm-test:/var/log# time grep 'Dec 12' auth.log.1 |grep sshd  |wc -l</span><br><span class="line">3170304</span><br><span class="line"></span><br><span class="line">real    0m3.610s</span><br><span class="line">user    0m1.745s</span><br><span class="line">sys     0m1.170s</span><br></pre></td></tr></table></figure><p>However, with SQL, there are several advantages:</p><ol><li><p>Data is separated by fields and the results are more accurate.</p></li><li><p>If you want to import the log into the database, it’s very convenient. Just `create table tablename as select * from authlog’.</p></li><li><p>Sometimes it is convenient to create a table when you want to join the contents of the log and the contents of some tables.</p></li></ol><p><strong>Summary</strong></p><p>CONNECT has many functions, such as parsing xml, json, remote querying other database functions, row and column conversion, virtual columns and so on, which we have not demonstrated. However, these features can be found to be supported on arm64 by the test cases previously run. And there are not many scenarios where these functions involve performance. Like XML and Json, you hardly can find  some files which are several hundreds megabytes. Remote queries on remote databases actually rely more on the performance of remote databases themselves. For specific usage of these features,  you can refer to MariaDB’s official Knowledge Base guide, which is detailed.</p><p>This article mainly tests several CONNECT storage engine scenarios may be related to performance, need to parse large files, found that the performance on arm64 is also good. In addition, the query scenarios tested in this paper are all unindexed scenes. If the data’s selectivity is good, you can choose to index and query again, then the result can be returned instantly. Of course, the index needs to take up a certain amount of disk space, and it also requires write permission to the disk. Please refer to the official guidance of MariaDB for specific usage.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Author:zhaorenhai &lt;/p&gt;
&lt;p&gt;MariaDB’s connect storage engine is an exciting storage engine.&lt;/p&gt;
&lt;p&gt;The reason why it’s exciting is that the storage engine can do so much. Except for OLTP field, it is almost omnipotent. It can directly use SQL to query external files. It supports various common text files and log files. As long as you tell CONNECT the record format, you can use SQL to query and analyze. It also supports common file formats such as JSON, XML and INI. In addition to supporting local files, remote data can also be queried directly through the rest interface. For local files, in addition to query, it also supports insert, delete and other functions. It can also realize the dblink function in Oracle, that is, it can remotely query the tables of another database, even different types of databases, including Mysql, PostgreSQL, Oracle, etc., and even MongoDB of NoSQL type.  Of course, on this basis, it is easier to import external files into the database, and it is more powerful and flexible than mysqlimport, because it can realize various format conversion and filtering. The same is true for exporting data to an external file. It also supports querying compressed files (currently only in ZIP format). Even binary file queries and updates are supported. The engine also supports virtual table which functions similar to Oracle’s dual. It can also support a variety of functions, such as single row to multi row, row column conversion, multi table mapping to a table.&lt;/p&gt;
&lt;p&gt;In a word, with the CONNECT storage engine, everything can be transformed into SQL, and SQL can be transformed into everything.&lt;/p&gt;
&lt;p&gt;Next, let’s take a look at the performance of the connect storage engine on the arm64 platform.&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>MariaDB的CONNECT存储引擎在arm64平台上的表现</title>
    <link href="https://kunpengcompute.github.io/2020/12/15/mariadb-de-connect-cun-chu-yin-qing-zai-arm64-ping-tai-shang-de-biao-xian/"/>
    <id>https://kunpengcompute.github.io/2020/12/15/mariadb-de-connect-cun-chu-yin-qing-zai-arm64-ping-tai-shang-de-biao-xian/</id>
    <published>2020-12-15T07:28:21.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者: zhaorenhai</p><p>MariaDB的CONNECT存储引擎是一个让人感到兴奋的存储引擎。</p><p>之所以说让人感到兴奋，是因为这个存储引擎能做的事实在是太多了。除了不适合用于OLTP领域之外，几乎是上天入地，无所不能。它可以直接用SQL去查询外部文件，支持各种普通的文本文件，日志文件，只要告诉CONNECT固定的文件格式，就可以用SQL去查询，去分析。它还支持JSON，XML，INI这些常见的文件格式。除了支持本地的文件，还可以通过REST接口直接查询远程的数据。对于本地的文件，除了查询，还支持插入，删除等功能。它还可以实现Oracle里面的dblink功能，也就是可以远程查询另外一个数据库的表，甚至是不同类型的数据库，包括MySQL，PostgreSQL，Oracle等等，甚至可以是NoSQL类型的MongoDB。当然所谓的跨源查询，多库查询，更是不在话下。当然在此基础上，将外部文件导入到数据库就更简单了，而且比mysqlimport更强大，更灵活，因为可以实现各种各样的格式转换和过滤。将数据导出到外部文件也是一样。它还支持对压缩文件的查询(目前只有zip格式)。甚至对于二进制格式的文件查询和更新都支持。。这个引擎还支持类似于Oracle的dual的虚拟表功能。还可以支持单行转多行，行列转换，多表映射成一个表等各种各样的功能。</p><p>总之有了这个CONNECT存储引擎，可以说，一切都可以转化成SQL，SQL也可以转化成一切。</p><p>接下来我们就看看CONNECT存储引擎在arm64平台上的表现。</p><a id="more"></a><p>（注意本文不是CONNECT的详细使用指导，由于CONNECT涉及内容太广，具体每种用法的详细指导、各种参数、以及限制说明等等请参考官方指导<a href="https://mariadb.com/kb/en/connect/）">https://mariadb.com/kb/en/connect/）</a></p><p>我们的测试平台使用的是华为云的鲲鹏虚拟机。OS采用Ubuntu18.04版本。</p><p>用如下命令查询</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt search mariadb |grep connect</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mariadb-plugin-connect&#x2F;bionic-updates,bionic-security 1:10.1.47-0ubuntu0.18.04.1 arm64</span><br></pre></td></tr></table></figure><p>可以看出来当前MariaDB已经发布了arm64平台的CONNECT插件，说明这个插件的基本功能在arm64上是没问题的。</p><p>如果你自己编译了MariaDB的代码(具体的编译方法请参考<a href="https://kunpengcompute.github.io/2020/12/04/mariadb-de-s3-cun-chu-yin-qing-zai-arm64-ping-tai-shang-de-biao-xian/">这篇博客</a>里的内容)，然后去跑CONNECT的测试用例的话，也可以发现，除了一些测试用力由于环境原因被skip以外，大部分测试用例都是可以成功通过的，如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line">mariadb@host1:~/build-mariadb-server/mysql-test$ ./mysql-test-run --suite=connect</span><br><span class="line">Logging: /home/mariadb/server/mysql-test/mysql-test-run.pl  --suite=connect</span><br><span class="line">vardir: /home/mariadb/build-mariadb-server/mysql-test/var</span><br><span class="line">Checking leftover processes...</span><br><span class="line">Removing old var directory...</span><br><span class="line">Creating var directory '/home/mariadb/build-mariadb-server/mysql-test/var'...</span><br><span class="line">Checking supported features...</span><br><span class="line">MariaDB Version 10.6.0-MariaDB</span><br><span class="line"></span><br><span class="line"> - SSL connections supported</span><br><span class="line">   Using suites: connect</span><br><span class="line">   Collecting tests...</span><br><span class="line">   Installing system database...</span><br><span class="line"></span><br><span class="line">==============================================================================</span><br><span class="line"></span><br><span class="line">TEST                                      RESULT   TIME (ms) or COMMENT</span><br><span class="line">--------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">worker[1] Using MTR_BUILD_THREAD 300, with reserved ports 16000..16019</span><br><span class="line">connect.grant2                           [ disabled ]  Until fixed</span><br><span class="line">connect.jdbc                             [ disabled ]  Variable settings depend on machine configuration</span><br><span class="line">connect.jdbc_new                         [ disabled ]  Variable settings depend on machine configuration</span><br><span class="line">connect.jdbc_oracle                      [ disabled ]  Variable settings depend on machine configuration</span><br><span class="line">connect.jdbc_postgresql                  [ disabled ]  Variable settings depend on machine configuration</span><br><span class="line">connect.json_java_2                      [ disabled ]  Need MongoDB running and its Java Driver installed</span><br><span class="line">connect.json_java_3                      [ disabled ]  Need MongoDB running and its Java Driver installed</span><br><span class="line">connect.json_mongo_c                     [ disabled ]  Need MongoDB running and its C Driver installed</span><br><span class="line">connect.mongo_c                          [ disabled ]  Need MongoDB running and its C Driver installed</span><br><span class="line">connect.mongo_java_2                     [ disabled ]  Need MongoDB running and its Java Driver installed</span><br><span class="line">connect.mongo_java_3                     [ disabled ]  Need MongoDB running and its Java Driver installed</span><br><span class="line">connect.tbl_thread                       [ disabled ]  Bug MDEV-9844,10179,14214 03/01/2018 OB Option THREAD removed</span><br><span class="line">connect.json                             [ pass ]     23</span><br><span class="line">connect.part_file                        [ pass ]     41</span><br><span class="line">connect.part_table                       [ pass ]     46</span><br><span class="line">connect.drop-open-error                  [ pass ]      7</span><br><span class="line">connect.secure_file_priv                 [ pass ]      5</span><br><span class="line">connect.alter                            [ pass ]     27</span><br><span class="line">connect.alter_xml                        [ skipped ]  Need windows</span><br><span class="line">connect.alter_xml2                       [ pass ]      7</span><br><span class="line">connect.bin                              [ pass ]      7</span><br><span class="line">connect.csv                              [ pass ]     16</span><br><span class="line">connect.datest                           [ pass ]      4</span><br><span class="line">connect.dbf                              [ pass ]     34</span><br><span class="line">connect.dir                              [ pass ]      3</span><br><span class="line">connect.endian                           [ pass ]      6</span><br><span class="line">connect.fix                              [ pass ]     18</span><br><span class="line">connect.fmt                              [ pass ]      5</span><br><span class="line">connect.general                          [ pass ]      3</span><br><span class="line">connect.grant                            [ pass ]     68</span><br><span class="line">connect.grant3                           [ pass ]      2</span><br><span class="line">connect.index                            [ pass ]    153</span><br><span class="line">connect.infoschema-9739                  [ skipped ]  Need windows</span><br><span class="line">connect.infoschema2-9739                 [ pass ]      3</span><br><span class="line">connect.ini                              [ pass ]     22</span><br><span class="line">connect.ini_grant                        [ pass ]      9</span><br><span class="line">connect.json_udf                         [ pass ]     31</span><br><span class="line">connect.json_udf_bin                     [ pass ]     45</span><br><span class="line">connect.mrr                              [ pass ]     14</span><br><span class="line">connect.mul                              [ pass ]      5</span><br><span class="line">connect.mul_new                          [ pass ]      7</span><br><span class="line">connect.mysql                            [ pass ]     51</span><br><span class="line">connect.mysql_discovery                  [ pass ]      9</span><br><span class="line">connect.mysql_exec                       [ pass ]      9</span><br><span class="line">connect.mysql_grant                      [ pass ]      7</span><br><span class="line">connect.mysql_index                      [ pass ]    173</span><br><span class="line">connect.mysql_new                        [ pass ]     24</span><br><span class="line">connect.null                             [ pass ]      9</span><br><span class="line">connect.occur                            [ pass ]     12</span><br><span class="line">connect.odbc                             [ skipped ]  No ODBC support</span><br><span class="line">connect.odbc_firebird                    [ skipped ]  No ODBC support</span><br><span class="line">connect.odbc_oracle                      [ skipped ]  No ODBC support</span><br><span class="line">connect.odbc_postgresql                  [ skipped ]  No ODBC support</span><br><span class="line">connect.odbc_sqlite3                     [ skipped ]  No ODBC support</span><br><span class="line">connect.odbc_sqlite3_grant               [ skipped ]  No ODBC support</span><br><span class="line">connect.odbc_xls                         [ skipped ]  No ODBC support</span><br><span class="line">connect.pivot                            [ pass ]     22</span><br><span class="line">connect.tbl                              [ pass ]     13</span><br><span class="line">connect.temporary                        [ pass ]</span><br><span class="line">connect.type_inet6                       [ pass ]      2</span><br><span class="line">connect.unsigned                         [ pass ]      6</span><br><span class="line">connect.upd                              [ pass ]    218</span><br><span class="line">connect.updelx                           [ pass ]    282</span><br><span class="line">connect.updelx2                          [ pass ]      5</span><br><span class="line">connect.vcol                             [ pass ]      2</span><br><span class="line">connect.vec                              [ pass ]     10</span><br><span class="line">connect.xcol                             [ pass ]      5</span><br><span class="line">connect.xml                              [ skipped ]  Need windows</span><br><span class="line">connect.xml2                             [ pass ]     22</span><br><span class="line">connect.xml2_grant                       [ pass ]     14</span><br><span class="line">connect.xml2_html                        [ pass ]      3</span><br><span class="line">connect.xml2_mdev5261                    [ pass ]      5</span><br><span class="line">connect.xml2_mult                        [ pass ]      9</span><br><span class="line">connect.xml2_zip                         [ pass ]      4</span><br><span class="line">connect.xml_grant                        [ skipped ]  Need windows</span><br><span class="line">connect.xml_html                         [ skipped ]  Need windows</span><br><span class="line">connect.xml_mdev5261                     [ skipped ]  Need windows</span><br><span class="line">connect.xml_mult                         [ skipped ]  Need windows</span><br><span class="line">connect.xml_zip                          [ skipped ]  Need windows</span><br><span class="line"></span><br><span class="line">connect.zip                              [ pass ]     13</span><br><span class="line">--------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">The servers were restarted 3 times</span><br><span class="line">Spent 1.540 of 11 seconds executing testcases</span><br><span class="line"></span><br><span class="line">Completed: All 53 tests were successful.</span><br><span class="line"></span><br><span class="line">15 tests were skipped, 15 by the test itself.</span><br></pre></td></tr></table></figure><p>接下来我们测试一下CONNECT存储引擎可能会涉及到性能的几个表类型，看看性能情况怎么样。</p><p>在mariadb的配置文件里，添加如下配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plugin_dir &#x3D; &#x2F;home&#x2F;mariadb&#x2F;build-mariadb-server&#x2F;storage&#x2F;connect</span><br></pre></td></tr></table></figure><p>重启数据库。</p><p>登陆到mariadb，执行如下语句，启用CONNECT存储引擎：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">install</span> <span class="keyword">soname</span> <span class="string">'ha_connect'</span>;</span><br></pre></td></tr></table></figure><p><strong>DOS表类型</strong></p><p>DOS表类型实际上对应的就是普通的文本文件，文件中每一列的数据是定长的，只有最后一列是变长的。</p><p>我们有如下格式的一个文件，大小在659M，行数为1300多万行。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">root@mariadb-arm-test:~# ls -lh /tmp/data_dos.txt</span><br><span class="line">-rw-r--r-- 1 root root 659M Dec 11 16:56 /tmp/data_dos.txt</span><br><span class="line">root@mariadb-arm-test:~# wc -l /tmp/data_dos.txt</span><br><span class="line">13819904 /tmp/data_dos.txt</span><br><span class="line">root@mariadb-arm-test:~# tail /tmp/data_dos.txt</span><br><span class="line">53975 1352378138480 dxmpjtsqpmjhltthckyniaxdw</span><br><span class="line">53976 1364107020874 wyqnxsuexajbvekyqamxpzlcoy</span><br><span class="line">53977 1374147913184 ltmwgochhwfwpzbsquuttiglcls</span><br><span class="line">53978 1382141036405 gmedqtymkbkxntltajxbhlaeomda</span><br><span class="line">53979 1391311715118 kfnwwabonxwyfauqlbeguhstbrumj</span><br><span class="line">53980 1303473032800 padqsyhvhvltcpebmtkt</span><br><span class="line">53981 1313713811959 nwduknfpyaeplwpmpozwh</span><br><span class="line">53982 1321669920517 aglfvcespetmimkugoyygu</span><br><span class="line">53983 1333059736260 ltccjcvptqcrwklutmrlles</span><br><span class="line">53984 1342207932901 vqkkrkrqkmcxvkdmiwyfylxe</span><br></pre></td></tr></table></figure><p>我们用CONNECT存储引擎来查询一下试试。</p><p>登陆到数据库，创建表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table userlist (</span><br><span class="line">  id char(5) not null,</span><br><span class="line">  phonenum char(13) not null flag&#x3D;6,</span><br><span class="line">  userid char(30) not null flag&#x3D;20</span><br><span class="line">  )</span><br><span class="line">engine&#x3D;CONNECT table_type&#x3D;DOS file_name&#x3D;&#39;&#x2F;tmp&#x2F;data_dos.txt&#39;;</span><br></pre></td></tr></table></figure><p>建表速度很快</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br></pre></td></tr></table></figure><p>查询一下总条数，速度也很快，只要13秒</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [test]&gt; select count(*) from userlist;</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">| 13819904 |</span><br><span class="line">+----------+</span><br><span class="line">1 row in set (13.350 sec)</span><br></pre></td></tr></table></figure><p>再试试带条件查询和插入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">8f3941802b868b2bbb43390ab5a2f1c8  -</span><br><span class="line">256 rows in set (9.221 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">8f3941802b868b2bbb43390ab5a2f1c8  -</span><br><span class="line">256 rows in set (9.221 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist where userid like &#39;ltccjc%&#39;;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">2bde22161d7a5bad42c377b7290cdff4  -</span><br><span class="line">256 rows in set (19.196 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist where userid like &#39;ltccjc%&#39;;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">2bde22161d7a5bad42c377b7290cdff4  -</span><br><span class="line">256 rows in set (19.279 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; insert into userlist values(&#39;76245&#39;,&#39;1812345678901&#39;, &#39;swweyuoyslwtqwtoeurwio&#39;);</span><br><span class="line">Query OK, 1 row affected (0.000 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; delete from userlist where id&#x3D;76245 and phonenum &#x3D; &#39;1812345678901&#39;;</span><br><span class="line">Query OK, 1 row affected (9.719 sec)</span><br></pre></td></tr></table></figure><p>可以看到查询速度还是比较快的，从1300多万条记录查询出256条记录只要9秒. 不过like查询稍微慢一点，要19秒。<br>插入也很快,插入实际上就是在原文件的最后插入一条记录。</p><p>不建议在这种DOS类型的表上面进行Update操作，可能会有奇怪的事情发生。因为Update的时候默认不是更新原文件，而是更新一个临时文件。有一个参数<code>connect_use_tempfile</code>可以配置支持更新源文件，但是在变长记录的文件里面也不建议使用。如果确定要用这个功能，建议自己测试以后再使用。</p><p><strong>FIX表类型</strong></p><p>FIX表类型和DOS的区别就在于最后一列的宽度是定长的，还是变长的，如果是定长的，就是FIX类型，如果是变长的，就是DOS类型。</p><p>现在我们来测试一下FIX类型，我们有如下文件，大小为667M，1343万行，最后一列长度都是一样的。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mariadb@mariadb-arm-test:/tmp$ ls -lhrt data_fix.txt</span><br><span class="line">-rw-rw-r-- 1 mariadb mariadb 667M Dec 13 16:29 data_fix.txt</span><br><span class="line">mariadb@mariadb-arm-test:/tmp$ wc -l data_fix.txt</span><br><span class="line">13432192 data_fix.txt</span><br><span class="line">mariadb@mariadb-arm-test:/tmp$ tail data_fix.txt</span><br><span class="line">104930 1302533021891 idmujolnoimfhafacordyrrzmmjjuf</span><br><span class="line">104931 1313195411180 bwuhbnyhxscqprsizgyjkotqfyndda</span><br><span class="line">104932 1323234034390 frynxnmkttbxzibvsdzokelmnqwkwc</span><br><span class="line">104933 1331271621869 sthxlnhbfqpnrpkiahgvqrlfhrcfxu</span><br><span class="line">104934 1342061419324 hheddnlwxslnqttygyalcamwdxujpp</span><br><span class="line">104935 1351001433696 ajlhunoosohztneqfxzotfbgfdegrj</span><br><span class="line">104936 1362857527592 qraermmsdzuxoogprmxsxrqmktfrwr</span><br><span class="line">104937 1372706024878 tkozushzmqhuxkavdbcxwnmksiceji</span><br><span class="line">104938 1382111414256 osllmzidwpmdbxzkzfkelsnwcdtstr</span><br><span class="line">104939 1393077713849 dzafytqywdyahhjvhvdosnsduwwpvw</span><br></pre></td></tr></table></figure><p>登陆到MariaDB数据库，我们创建一个FIX表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table userlist_fix (</span><br><span class="line">  id char(6) not null,</span><br><span class="line">  phonenum char(13) not null flag&#x3D;7,</span><br><span class="line">  userid char(30) not null flag&#x3D;21</span><br><span class="line">  )</span><br><span class="line">engine&#x3D;CONNECT table_type&#x3D;FIX file_name&#x3D;&#39;&#x2F;tmp&#x2F;data_fix.txt&#39; lrecl&#x3D;52;</span><br></pre></td></tr></table></figure><p>注意那个lrecl=52是最长行的字节数长度，包含换行符。</p><p>我们把上面的操作再来一遍：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [test]&gt; select count(*) from userlist_fix;</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">| 13432193 |</span><br><span class="line">+----------+</span><br><span class="line">1 row in set (0.000 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_fix where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">0871c0088df3f4dbf37b12e56611a0b0  -</span><br><span class="line">128 rows in set (3.535 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_fix where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">0871c0088df3f4dbf37b12e56611a0b0  -</span><br><span class="line">128 rows in set (3.535 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_fix where userid like &#39;ltccjc%&#39;;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">Empty set (9.344 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_fix where userid like &#39;ltccjc%&#39;;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">Empty set (9.325 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; insert into userlist_fix values(&#39;76245&#39;,&#39;1812345678901&#39;, &#39;swweyuoyslwtqwtoeurwio&#39;);</span><br><span class="line">Query OK, 1 row affected (0.000 sec)</span><br></pre></td></tr></table></figure><p>可以看出来，FIX格式每一种类型的操作都比DOS格式快了很多，说明MariaDB在定长格式的处理里面，利用了定长的特性，性能会更好。 最后一行，我们插入的数据不是对齐的，MariaDB插入数据库的时候，会自动给你补齐：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mariadb@mariadb-arm-test:/tmp$ tail data_fix.txt</span><br><span class="line">104930 1302533021891 idmujolnoimfhafacordyrrzmmjjuf</span><br><span class="line">104931 1313195411180 bwuhbnyhxscqprsizgyjkotqfyndda</span><br><span class="line">104932 1323234034390 frynxnmkttbxzibvsdzokelmnqwkwc</span><br><span class="line">104933 1331271621869 sthxlnhbfqpnrpkiahgvqrlfhrcfxu</span><br><span class="line">104934 1342061419324 hheddnlwxslnqttygyalcamwdxujpp</span><br><span class="line">104935 1351001433696 ajlhunoosohztneqfxzotfbgfdegrj</span><br><span class="line">104936 1362857527592 qraermmsdzuxoogprmxsxrqmktfrwr</span><br><span class="line">104937 1372706024878 tkozushzmqhuxkavdbcxwnmksiceji</span><br><span class="line">104938 1382111414256 osllmzidwpmdbxzkzfkelsnwcdtstr</span><br><span class="line">76245  1812345678901 swweyuoyslwtqwtoeurwio</span><br></pre></td></tr></table></figure><p>定长的类型，对于update和delete操作也是比较友好的，速度也很快，而且都是在原文件上直接操作的。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [test]&gt; update userlist_fix set phonenum = '1811111111111' where id= 104938;</span><br><span class="line">Query OK, 128 rows affected (3.513 sec)</span><br><span class="line">Rows matched: 128  Changed: 128  Warnings: 0</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; delete from userlist_fix where phonenum = '1811111111111';</span><br><span class="line">Query OK, 128 rows affected (3.672 sec)</span><br></pre></td></tr></table></figure><p>所以我们在根据原始文本文件建表的时候，如果有可能处理成FIX类型的，尽量处理成FIX类型的。</p><p><strong>关于connect_work_size参数</strong></p><p>接下来我们修改一下connect_work_size参数，这个参数从官网描述来看，应该是CONNECT引擎使用的内存区域，我们来增大一下看看有没有效果。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [test]&gt; select @@session.connect_work_size;</span><br><span class="line">+-----------------------------+</span><br><span class="line">| @@session.connect_work_size |</span><br><span class="line">+-----------------------------+</span><br><span class="line">|                    67108864 |</span><br><span class="line">+-----------------------------+</span><br><span class="line">1 row in set (0.000 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; set session connect_work_size &#x3D; 1073741824;</span><br><span class="line">Query OK, 0 rows affected (0.000 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; select count(*) from userlist;</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">| 13819650 |</span><br><span class="line">+----------+</span><br><span class="line">1 row in set (13.576 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">8f3941802b868b2bbb43390ab5a2f1c8  -</span><br><span class="line">256 rows in set (9.306 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">8f3941802b868b2bbb43390ab5a2f1c8  -</span><br><span class="line">256 rows in set (9.305 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_fix where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">6daf8a0815666570cee0011046e3b568  -</span><br><span class="line">128 rows in set (3.526 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_fix where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">6daf8a0815666570cee0011046e3b568  -</span><br><span class="line">128 rows in set (3.526 sec)</span><br></pre></td></tr></table></figure><p>根据上面输出可以看出来，并没有效果。说明CONNECT存储引擎处理文件时，并没有把整个文件缓存起来，增大内存空间，并没有用。我们速度之所以这么快，也得益于我们用的华为云的虚拟机采用的存储是SSD，速度比较快。</p><p><strong>CSV表类型</strong></p><p>CSV表类型对应的就是普通的CSV格式的文本文件，不过这里的分隔符可以指定，就是不仅仅可以是逗号，也可以是指定的其他字符。</p><p>我们来测试下CSV表类型的性能。</p><p>我们创建一个如下格式的CSV文件，大小为603M，1390多万行。列与列之间用竖线分隔。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mariadb@mariadb-arm-test:/tmp$ wc -l data.csv</span><br><span class="line">13909120 data.csv</span><br><span class="line">mariadb@mariadb-arm-test:/tmp$ ls -lhrt data.csv</span><br><span class="line">-rw-rw-r-- 1 mariadb mariadb 603M Dec 14 12:01 data.csv</span><br><span class="line">mariadb@mariadb-arm-test:/tmp$ tail -10 data.csv</span><br><span class="line">1411|1311008423658|hudulfqgofqahfxwtqihs</span><br><span class="line">1412|1321119524436|atacjjbibtzqwohjnrtebc</span><br><span class="line">1413|1334167818661|tnhdlrpbpaxzfyiymholbfy</span><br><span class="line">1414|1344170515956|xlwsuiotrfhmoltiscorhdec</span><br><span class="line">1415|1351643025744|httzsnqpjydxeinvwcaekgydt</span><br><span class="line">1416|1363624529884|tyyttcbsqjiwufvmpuveototmq</span><br><span class="line">1417|1371750537023|diwtmqdriozmoscypldbcdrqvfd</span><br><span class="line">1418|1383308741505|gqchktgbsstyixidjztgtpgutzeb</span><br><span class="line">1419|1393553934078|culiovhkrdovnbcdestvnqqubwdzl</span><br><span class="line">1420|1302340336368|bfjdepbcrefcfvbhznbd</span><br></pre></td></tr></table></figure><p>使用如下sql创建CSV类型的表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table userlist_csv (</span><br><span class="line">  id int not null,</span><br><span class="line">  phonenum char(13) not null,</span><br><span class="line">  userid char(29) not null)</span><br><span class="line">engine&#x3D;CONNECT table_type&#x3D;CSV file_name&#x3D;&#39;&#x2F;tmp&#x2F;data.csv&#39;</span><br><span class="line">header&#x3D;0 sep_char&#x3D;&#39;|&#39; quoted&#x3D;0;</span><br></pre></td></tr></table></figure><p>然后我们进行一些增删改查操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [test]&gt; select count(*) from userlist_csv;</span><br><span class="line">9229c8871853852300fca7ce0bb33598  -</span><br><span class="line">1 row in set (17.835 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; select count(*) from userlist_csv;</span><br><span class="line">9229c8871853852300fca7ce0bb33598  -</span><br><span class="line">1 row in set (17.712 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_csv where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">9a02ca9b5c3044a73f4441c10945ff91  -</span><br><span class="line">140 rows in set (9.356 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_csv where id &#x3D; 23456;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">9a02ca9b5c3044a73f4441c10945ff91  -</span><br><span class="line">140 rows in set (9.355 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_csv where userid like &#39;ltccjc%&#39;;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">Empty set (21.788 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; pager md5sum;select * from userlist_csv where userid like &#39;ltccjc%&#39;;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">Empty set (21.781 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; insert into userlist_csv values(&#39;76245&#39;,&#39;1812345678901&#39;, &#39;swweyuoyslwtqwtoeurwio&#39;);</span><br><span class="line">Query OK, 1 row affected (0.000 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; update userlist_csv set phonenum&#x3D;&#39;1811111111111&#39; where id&#x3D;76245;</span><br><span class="line">Query OK, 141 rows affected (6.803 sec)</span><br><span class="line">Rows matched: 141  Changed: 141  Warnings: 0</span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; delete from  userlist_csv  where id&#x3D;76245 and phonenum&#x3D;&#39;1811111111111&#39;;</span><br><span class="line">Query OK, 141 rows affected (6.674 sec)</span><br></pre></td></tr></table></figure><p>可以看出来，性能还是不错的，count的查询和like查询比DOS类型稍微慢一点，等于查询和插入操作都和DOS类型差不多，删除操作比DOS类型还要快一点。</p><p><strong>FMT表类型</strong></p><p>FMT表类型相对于CSV更灵活点，可以通过格式匹配来指定每一个字段的格式，长度。FMT在分析各种类型的日志的时候很有用。</p><p>我们用它来分析一个操作系统auth日志。</p><p>这个auth日志有600多M，500多万行。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@mariadb-arm-test:/var/log# ls -lhrt auth.log.1</span><br><span class="line">-rw-r----- 1 syslog adm 625M Dec 14 15:11 auth.log.1</span><br><span class="line">root@mariadb-arm-test:/var/log# wc -l auth.log.1</span><br><span class="line">5890048 auth.log.1</span><br></pre></td></tr></table></figure><p>截取部分文件内容，格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Dec 12 16:39:23 localhost groupadd[32556]: group added to &#x2F;etc&#x2F;group: name&#x3D;mariadb, GID&#x3D;1000</span><br><span class="line">Dec 12 16:39:23 localhost groupadd[32556]: group added to &#x2F;etc&#x2F;gshadow: name&#x3D;mariadb</span><br><span class="line">Dec 12 16:39:23 localhost groupadd[32556]: new group: name&#x3D;mariadb, GID&#x3D;1000</span><br><span class="line">Dec 12 16:39:23 localhost useradd[32560]: new user: name&#x3D;mariadb, UID&#x3D;1000, GID&#x3D;1000, home&#x3D;&#x2F;home&#x2F;mariadb, shell&#x3D;&#x2F;bin&#x2F;bash</span><br><span class="line">Dec 12 16:39:43 localhost passwd[32569]: pam_unix(passwd:chauthtok): password changed for mariadb</span><br><span class="line">Dec 12 16:39:46 localhost chfn[32570]: changed user &#39;mariadb&#39; information</span><br><span class="line">Dec 12 16:45:36 localhost su[1805]: Successful su for mariadb by root</span><br><span class="line">Dec 12 16:45:36 localhost su[1805]: + &#x2F;dev&#x2F;pts&#x2F;0 root:mariadb</span><br><span class="line">Dec 12 16:45:36 localhost su[1805]: pam_unix(su:session): session opened for user mariadb by root(uid&#x3D;0)</span><br><span class="line">Dec 12 16:45:36 localhost su[1805]: pam_systemd(su:session): Cannot create session: Already running in a session</span><br><span class="line">Dec 12 16:50:01 localhost sudo:  mariadb : TTY&#x3D;pts&#x2F;0 ; PWD&#x3D;&#x2F;home&#x2F;mariadb ; USER&#x3D;root ; COMMAND&#x3D;&#x2F;usr&#x2F;bin&#x2F;apt-get install build-essential libncurses5-dev gnutls-dev libcurl4-gnutls-dev</span><br><span class="line">Dec 12 16:50:01 localhost sudo: pam_unix(sudo:session): session opened for user root by root(uid&#x3D;0)</span><br><span class="line">Dec 12 16:50:04 localhost sudo: pam_unix(sudo:session): session closed for user root</span><br><span class="line">Dec 12 16:50:14 localhost sudo:  mariadb : TTY&#x3D;pts&#x2F;0 ; PWD&#x3D;&#x2F;home&#x2F;mariadb ; USER&#x3D;root ; COMMAND&#x3D;&#x2F;usr&#x2F;bin&#x2F;apt-get install build-essential libncurses5-dev gnutls-dev libcurl4-gnutls-dev</span><br><span class="line">Dec 12 16:50:14 localhost sudo: pam_unix(sudo:session): session opened for user root by root(uid&#x3D;0)</span><br><span class="line">Dec 12 16:50:25 localhost sudo: pam_unix(sudo:session): session closed for user root</span><br><span class="line">Dec 12 16:50:49 localhost sudo:  mariadb : TTY&#x3D;pts&#x2F;0 ; PWD&#x3D;&#x2F;home&#x2F;mariadb ; USER&#x3D;root ; COMMAND&#x3D;&#x2F;usr&#x2F;bin&#x2F;apt-get install zlib1g-dev ccache libnuma-dev libxml2-dev cmake bison</span><br><span class="line">Dec 12 16:50:49 localhost sudo: pam_unix(sudo:session): session opened for user root by root(uid&#x3D;0)</span><br><span class="line">Dec 12 16:51:08 localhost sudo: pam_unix(sudo:session): session closed for user root</span><br><span class="line">Dec 12 16:56:01 localhost CRON[7999]: pam_unix(cron:session): session opened for user root by (uid&#x3D;0)</span><br><span class="line">Dec 12 16:56:01 localhost CRON[7999]: pam_unix(cron:session): session closed for user root</span><br><span class="line">Dec 12 17:17:01 localhost CRON[16493]: pam_unix(cron:session): session opened for user root by (uid&#x3D;0)</span><br><span class="line">Dec 12 17:17:01 localhost CRON[16493]: pam_unix(cron:session): session closed for user root</span><br></pre></td></tr></table></figure><p>创建表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">create table authlog (</span><br><span class="line">  month char(3) not null field_format&#x3D;&#39;%n%s%n&#39;,</span><br><span class="line">  day char(2) not null field_format&#x3D;&#39; %n%s%n&#39;,</span><br><span class="line">  time char(8) not null field_format&#x3D;&#39; %n%s%n&#39;,</span><br><span class="line">  hostname char(9) not null field_format&#x3D;&#39; %n%s%n&#39;,</span><br><span class="line">  module char(20) not null field_format&#x3D;&#39; %n%s%n&#39;,</span><br><span class="line">  message char(255) not null field_format&#x3D;&#39; %n%255[^\n]%n&#39;)</span><br><span class="line">engine&#x3D;CONNECT table_type&#x3D;FMT file_name&#x3D;&#39;&#x2F;var&#x2F;log&#x2F;auth.log.1&#39; ;</span><br></pre></td></tr></table></figure><p>查询一下，现在看格式是不是好看多了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [test]&gt;  select * from authlog limit 10;</span><br><span class="line">+-------+-----+----------+-----------+----------------------+-----------------------------------------------------------------------+</span><br><span class="line">| month | day | time     | hostname  | module               | message                                                               |</span><br><span class="line">+-------+-----+----------+-----------+----------------------+-----------------------------------------------------------------------+</span><br><span class="line">| Dec   | 2   | 20:47:26 | localhost | systemd-logind[721]: | Power key pressed.                                                    |</span><br><span class="line">| Dec   | 2   | 20:47:26 | localhost | systemd-logind[721]: | Powering Off...                                                       |</span><br><span class="line">| Dec   | 2   | 20:47:26 | localhost | systemd-logind[721]: | System is powering down.                                              |</span><br><span class="line">| Dec   | 2   | 20:47:27 | localhost | systemd:             | pam_unix(systemd-user:session): session closed for user root          |</span><br><span class="line">| Dec   | 11  | 10:07:26 | localhost | systemd-logind[960]: | New seat seat0.                                                       |</span><br><span class="line">| Dec   | 11  | 10:07:26 | localhost | systemd-logind[960]: | Watching system buttons on &#x2F;dev&#x2F;input&#x2F;event0 (Power Button)           |</span><br><span class="line">| Dec   | 11  | 10:07:26 | localhost | systemd-logind[960]: | Watching system buttons on &#x2F;dev&#x2F;input&#x2F;event2 (QEMU QEMU USB Keyboard) |</span><br><span class="line">| Dec   | 11  | 10:07:50 | localhost | sshd[2384]:          | Server listening on 0.0.0.0 port 22.                                  |</span><br><span class="line">| Dec   | 11  | 10:07:50 | localhost | sshd[2384]:          | Server listening on :: port 22.                                       |</span><br><span class="line">| Dec   | 11  | 10:07:51 | localhost | sshd[2384]:          | Received signal 15; terminating.                                      |</span><br><span class="line">+-------+-----+----------+-----------+----------------------+-----------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><p>我们来进行一些查询：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [test]&gt; select count(*) from authlog where month&#x3D;&#39;Dec&#39; and day&#x3D;&#39;12&#39; and module like &#39;sshd%&#39;;</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">|  3170304 |</span><br><span class="line">+----------+</span><br><span class="line">1 row in set (15.609 sec)</span><br></pre></td></tr></table></figure><p>速度还可以接受。 当然你如果用如下的方式，用grep命令去过滤，速度会更快一点：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@mariadb-arm-test:/var/log# echo 3 &gt; /proc/sys/vm/drop_caches</span><br><span class="line">root@mariadb-arm-test:/var/log# time grep 'Dec 12' auth.log.1 |grep sshd  |wc -l</span><br><span class="line">3170304</span><br><span class="line"></span><br><span class="line">real    0m3.610s</span><br><span class="line">user    0m1.745s</span><br><span class="line">sys     0m1.170s</span><br></pre></td></tr></table></figure><p>但是用SQL，有如下几个优点：</p><p>1、数据是按字段分隔开的，结果会更精确。</p><p>2、你如果想把日志导入到数据库里面，就很方便了，直接<code>create table tablename as select * from authlog</code>就搞定了。</p><p>3、有时候会需要把日志里面的内容和表里面的内容进行关联查询，创建成表，就很方便了。</p><p><strong>总结</strong></p><p>CONNECT的功能很多，像解析xml，json，远程查询其他数据库功能，行列转换，虚拟列等等，我们都没有演示。不过这些功能通过之前跑的测试用例可以发现在arm64上都是支持的。而且这些功能涉及性能的场景不太多，像一般xml和json不会有太大的文件，远程查询其实更依赖的是远程数据库的性能。对于这些功能具体的用法，感兴趣的小伙伴可以参考MariaDB官方Knowledge Base指导，写的都是很详细的。</p><p>本文主要测试了几个CONNECT可能会涉及到性能相关，需要解析大文件的表类型，发现在arm64上性能其实也是很不错的。另外，本文测试的查询场景，都是未建索引的场景，如果数据的选择性很好的话，可以选择建立索引，再去查询，几乎就是瞬间就可以返回了。当然索引需要占用一定的空间，另外也要求对磁盘有写权限。具体用法请参考MariaDB官方指导。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: zhaorenhai&lt;/p&gt;
&lt;p&gt;MariaDB的CONNECT存储引擎是一个让人感到兴奋的存储引擎。&lt;/p&gt;
&lt;p&gt;之所以说让人感到兴奋，是因为这个存储引擎能做的事实在是太多了。除了不适合用于OLTP领域之外，几乎是上天入地，无所不能。它可以直接用SQL去查询外部文件，支持各种普通的文本文件，日志文件，只要告诉CONNECT固定的文件格式，就可以用SQL去查询，去分析。它还支持JSON，XML，INI这些常见的文件格式。除了支持本地的文件，还可以通过REST接口直接查询远程的数据。对于本地的文件，除了查询，还支持插入，删除等功能。它还可以实现Oracle里面的dblink功能，也就是可以远程查询另外一个数据库的表，甚至是不同类型的数据库，包括MySQL，PostgreSQL，Oracle等等，甚至可以是NoSQL类型的MongoDB。当然所谓的跨源查询，多库查询，更是不在话下。当然在此基础上，将外部文件导入到数据库就更简单了，而且比mysqlimport更强大，更灵活，因为可以实现各种各样的格式转换和过滤。将数据导出到外部文件也是一样。它还支持对压缩文件的查询(目前只有zip格式)。甚至对于二进制格式的文件查询和更新都支持。。这个引擎还支持类似于Oracle的dual的虚拟表功能。还可以支持单行转多行，行列转换，多表映射成一个表等各种各样的功能。&lt;/p&gt;
&lt;p&gt;总之有了这个CONNECT存储引擎，可以说，一切都可以转化成SQL，SQL也可以转化成一切。&lt;/p&gt;
&lt;p&gt;接下来我们就看看CONNECT存储引擎在arm64平台上的表现。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Monitor MariaDB database on the arm64 platform</title>
    <link href="https://kunpengcompute.github.io/2020/12/08/monitor-mariadb-database-on-the-arm64-platform/"/>
    <id>https://kunpengcompute.github.io/2020/12/08/monitor-mariadb-database-on-the-arm64-platform/</id>
    <published>2020-12-08T13:15:39.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>Author: zhaorenhai</p><p>This article attempts to explore some useful performance monitoring tools on the arm64 platform that can be used to monitor MariaDB databases.</p><a id="more"></a><p>The test platform still uses Huawei Cloud’s Kunpeng virtual machine, and the OS uses Ubuntu 18.04. A pair of MariaDB primary-replica databases have been deployed on other virtual machines in the same intranet in advance, and database user names and passwords for remote connections have been established. MariaDB uses the 10.1 version that comes with Ubuntu.</p><p>This article is going to test four free and open source tools including two command line tools, and two large graphical operation and maintenance tools.</p><p>The two command line tools are innotop and mytop.</p><p>Graphical operation and maintenance tools are Zabbix and Prometheus.</p><p><strong>innotop</strong></p><p>Innotop is a monitoring tool written in Perl. The source code is here <a href="https://github.com/innotop/innotop">https://github.com/innotop/innotop</a> .</p><p>We directly download the latest code.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;innotop&#x2F;innotop</span><br></pre></td></tr></table></figure><p>Compile:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd innotop </span><br><span class="line">perl Makefile.PL</span><br></pre></td></tr></table></figure><p>installation:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo make install</span><br></pre></td></tr></table></figure><p>Run (-w parameter represents the persistence mode, the configuration will be persisted to the file):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">innotop -w</span><br></pre></td></tr></table></figure><p>Enter ? , you can see the instructions. As follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Query List (? for help)                                                                                                                                                                                                 Servers: server</span><br><span class="line"></span><br><span class="line">Switch to a different mode:</span><br><span class="line">   A  Dashboard         I  InnoDB I&#x2F;O Info     Q  Query List</span><br><span class="line">   B  InnoDB Buffers    K  InnoDB Lock Waits   R  InnoDB Row Ops</span><br><span class="line">   C  Command Summary   L  Locks               S  Variables &amp; Status</span><br><span class="line">   D  InnoDB Deadlocks  M  Replication Status  T  InnoDB Txns</span><br><span class="line">   F  InnoDB FK Err     O  Open Tables         U  User Statistics</span><br><span class="line"></span><br><span class="line">Actions:</span><br><span class="line">   a  Toggle the innotop process    n  Switch to the next connection</span><br><span class="line">   c  Choose visible columns        p  Pause innotop</span><br><span class="line">   d  Change refresh interval       q  Quit innotop</span><br><span class="line">   e  Explain a thread&#39;s query      r  Reverse sort order</span><br><span class="line">   f  Show a thread&#39;s full query    s  Change the display&#39;s sort column</span><br><span class="line">   h  Toggle the header on and off  t  Toggle slave processes</span><br><span class="line">   i  Toggle idle processes         x  Kill a query</span><br><span class="line">   k  Kill a query&#39;s connection</span><br><span class="line"></span><br><span class="line">Other:</span><br><span class="line"> TAB  Switch to the next server group   &#x2F;  Quickly filter what you see</span><br><span class="line">   !  Show license and warranty         &#x3D;  Toggle aggregation</span><br><span class="line"></span><br><span class="line">   #  Select&#x2F;create server groups       @  Select&#x2F;create server connections</span><br><span class="line"></span><br><span class="line">   $  Edit configuration settings       \  Clear quick-filters</span><br><span class="line">Press any key to continue</span><br></pre></td></tr></table></figure><p>We enter an @ and press Enter to configure a database and to see the effect.</p><p>This is the first time running, there is no connection. You can input a name, name the new connection, and press Enter.</p><p>Then follow the prompts to enter the database address, port, configuration database address and port format as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DBI:mysql:;host&#x3D;192.168.0.204;port&#x3D;3306</span><br></pre></td></tr></table></figure><p>Then press Enter, you will be prompted for your username, password, etc. The prompts are  very friendly.</p><p>After configuration, it can be used normally.</p><p>We can enter uppercase A, B, Q, I and other commands to switch between different interfaces to see the database cache information, IO information, TPS and so on.</p><p>In order to display multiple databases on the same interface, we can enter @ to configure multiple database links.</p><p>Then enter a # to create a server group, enter multiple connection names that have been established, separated by spaces, and that’s it.</p><p>When we select a server group during operation, multiple servers can be displayed on the same interface.</p><p>For example, we configured a primary database and a replica database and placed them in the server group.</p><p>The following is the IO interface of the two databases displayed, the effect is still very good:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">InnoDB I&#x2F;O Info (? for help)                                                                                                                                                                                            Servers: server</span><br><span class="line"></span><br><span class="line">_____________________________ I&#x2F;O Threads ______________________________</span><br><span class="line">CXN     Thread  Purpose               Thread Status</span><br><span class="line">master       0  insert buffer thread  waiting for completed aio requests</span><br><span class="line">master       1  log thread            waiting for completed aio requests</span><br><span class="line">master       2  read thread           waiting for completed aio requests</span><br><span class="line">master       3  read thread           waiting for completed aio requests</span><br><span class="line">master       4  read thread           waiting for completed aio requests</span><br><span class="line">master       5  read thread           waiting for completed aio requests</span><br><span class="line">master       6  write thread          waiting for completed aio requests</span><br><span class="line">master       7  write thread          waiting for completed aio requests</span><br><span class="line">master       8  write thread          waiting for completed aio requests</span><br><span class="line">master       9  write thread          waiting for completed aio requests</span><br><span class="line">slave        0  insert buffer thread  waiting for completed aio requests</span><br><span class="line">slave        1  log thread            waiting for completed aio requests</span><br><span class="line">slave        2  read thread           waiting for completed aio requests</span><br><span class="line">slave        3  read thread           waiting for completed aio requests</span><br><span class="line">slave        4  read thread           waiting for completed aio requests</span><br><span class="line">slave        5  read thread           waiting for completed aio requests</span><br><span class="line">slave        6  write thread          waiting for completed aio requests</span><br><span class="line">slave        7  write thread          waiting for completed aio requests</span><br><span class="line">slave        8  write thread          waiting for completed aio requests</span><br><span class="line">slave        9  write thread          waiting for completed aio requests</span><br><span class="line"></span><br><span class="line">________________________________ Pending I&#x2F;O _________________________________</span><br><span class="line">CXN     Async Rds  Async Wrt  IBuf Async Rds  Sync I&#x2F;Os  Log Flushes  Log I&#x2F;Os</span><br><span class="line">master                                     0          0            0         0</span><br><span class="line">slave                                      0          0            0         0</span><br><span class="line"></span><br><span class="line">____________________________ File I&#x2F;O Misc _____________________________</span><br><span class="line">CXN     OS Reads  OS Writes  OS fsyncs  Reads&#x2F;Sec  Writes&#x2F;Sec  Bytes&#x2F;Sec</span><br><span class="line">master       178        984        372       0.00        0.00          0</span><br><span class="line">slave        182       1529        584       0.00        0.00          0</span><br><span class="line"></span><br><span class="line">_________________________ Log Statistics _________________________</span><br><span class="line">CXN     Sequence No.  Flushed To  Last Checkpoint  IO Done  IO&#x2F;Sec</span><br><span class="line">master  1818551       1818551     1818551              207    0.00</span><br><span class="line">slave   1939016       1939016     1939016              306    0.00</span><br></pre></td></tr></table></figure><p>This article is not an introduction document for innotop, just to test whether innotop can run normally on the arm64 platform, and whether it can monitor the MariaDB database normally. Therefore, the introduction of other usage and interface of inntop is no longer wordy. Interested friends can try it. There are also a lot of innotop information on the Internet. You can also see the instructions by using command  <code>man innotop</code> .</p><p><strong>mytop</strong></p><p>mytop is also a small tool written in Perl, which is widely used. However, the disadvantage is that it can only connect to one database server, and it is relatively old, its functions are relatively simple, and the information obtained is relatively limited.</p><p>mytop can be installed directly from the apt command:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install mytop</span><br></pre></td></tr></table></figure><p>To configure, edit the following files:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi ~&#x2F;.mytop</span><br></pre></td></tr></table></figure><p>Enter the following:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">user&#x3D;proxysql</span><br><span class="line">pass&#x3D;proxypassword</span><br><span class="line">host&#x3D;192.168.0.204</span><br><span class="line">db&#x3D;testdb</span><br><span class="line">delay&#x3D;5</span><br><span class="line">port&#x3D;3306</span><br><span class="line">socket&#x3D;</span><br><span class="line">batchmode&#x3D;0</span><br><span class="line">header&#x3D;1</span><br><span class="line">color&#x3D;1</span><br><span class="line">idle&#x3D;1</span><br></pre></td></tr></table></figure><p>Execute:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mytop</span><br></pre></td></tr></table></figure><p>The interface is as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">MySQL on 192.168.0.204 (10.1.47)                                                                                                                                             load 0.00 0.00 0.00 1&#x2F;1397 32133 up 16+23:58:33 [17:45:26]</span><br><span class="line"> Queries: 50.0     qps:    0 Slow:     0.0         Se&#x2F;In&#x2F;Up&#x2F;De(%):    12&#x2F;00&#x2F;00&#x2F;00</span><br><span class="line"> Sorts:      0 qps now:    1 Slow qps: 0.0  Threads:    5 (   2&#x2F;   7) 00&#x2F;00&#x2F;00&#x2F;00</span><br><span class="line"> Key Efficiency: 84.0%  Bps in&#x2F;out:   0.0&#x2F;  0.2   Now in&#x2F;out:  21.3&#x2F; 3.6k</span><br><span class="line"></span><br><span class="line">       Id      User         Host&#x2F;IP         DB       Time    Cmd    State Query</span><br><span class="line">       --      ----         -------         --       ----    ---    ----- ----------</span><br><span class="line">       46 replicati hadoop-arm-kae-               1466725 Binlog Master h</span><br><span class="line">       63  proxysql hadoop-arm-kae-       test          4  Sleep</span><br><span class="line">       57   monitor hadoop-arm-kae-                     3  Sleep</span><br><span class="line">      838   monitor hadoop-arm-kae-                     0  Sleep</span><br><span class="line">    24644  proxysql hadoop-arm-kae-     testdb          0  Query     init show full processlist</span><br></pre></td></tr></table></figure><p>You can see that the style is similar to top, and the effect is pretty good.</p><p>For more information about mytop, please refer to the following link: <a href="http://jeremy.zawodny.com/mysql/mytop/">http://jeremy.zawodny.com/mysql/mytop/</a> . There are also a lot of relevant information on the Internet, and you can also see relevant instructions with command  <code>man mytop</code>.</p><p><strong>Zabbix</strong></p><p>Zabbix is a large-scale monitoring tool, not only used to monitor the database, but here we simply use it to monitor MariaDB.</p><p>Regarding the detailed introduction of its structure, functions, etc., the official website includes a lot of information on other websites, so I won’t go into details here. Here only describes the steps of testing on our test environment. (The specific installation steps may be different for different versions of Zabbix, and different OSs are also different. The Zabbix3.0 version that comes with Ubuntu 18.04 is installed here, for reference only)</p><p>Firstly we install Zabbix Server and Zabbix Frontend on the test virtual machine.</p><p>zabbix-server has two versions, PostgreSql and Mysql, according to the built-in database it uses. In order to distinguish it from the target database MariaDB we want to monitor, we install the PostgreSql version here.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install zabbix-server-pgsql zabbix-frontend-php php-pgsql</span><br></pre></td></tr></table></figure><p>Install Zabbix Agent on the master and slave database nodes we want to monitor</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install zabbix-agent</span><br></pre></td></tr></table></figure><p>Zabbix Agent will start automatically after installation.</p><p>Zabbix Server needs to be configured to log in to the node where the Server is located</p><p>First create a database for Zabbix Server’s own use</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo -u postgres createuser --pwprompt zabbix</span><br><span class="line">sudo -u postgres createdb -O zabbix -E Unicode -T template0 zabbix</span><br></pre></td></tr></table></figure><p>Then import the data</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zcat &#x2F;usr&#x2F;share&#x2F;zabbix-server-pgsql&#x2F;schema.sql.gz |sudo -u zabbix psql zabbix</span><br><span class="line">zcat &#x2F;usr&#x2F;share&#x2F;zabbix-server-pgsql&#x2F;images.sql.gz |sudo -u zabbix psql zabbix</span><br><span class="line">zcat &#x2F;usr&#x2F;share&#x2F;zabbix-server-pgsql&#x2F;data.sql.gz |sudo -u zabbix psql zabbix</span><br></pre></td></tr></table></figure><p>Configure the relevant information of the self-use database just set up in the Zabbix Server configuration file</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;zabbix&#x2F;zabbix_server.conf</span><br></pre></td></tr></table></figure><p>Ensure that the following configuration items are configured correctly</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DBHost&#x3D;localhost</span><br><span class="line">DBName&#x3D;zabbix</span><br><span class="line">DBUser&#x3D;zabbix</span><br><span class="line">DBPassword&#x3D;zabbix</span><br></pre></td></tr></table></figure><p>Start Zabbix Server</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service zabbix-server start</span><br><span class="line">update-rc.d zabbix-server enable</span><br></pre></td></tr></table></figure><p>Start configuring Frontend</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;etc&#x2F;apache2&#x2F;conf-enabled</span><br><span class="line">ln -s ..&#x2F;conf-available&#x2F;zabbix-frontend-php.conf zabbix-frontend-php.conf</span><br><span class="line">vi zabbix-frontend-php.conf</span><br></pre></td></tr></table></figure><p>Configure the time zone parameters inside correctly, the default is as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># php_value date.timezone Europe&#x2F;Riga</span><br></pre></td></tr></table></figure><p>We changed to the following settings:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">php_value date.timezone Asia&#x2F;Shanghai</span><br></pre></td></tr></table></figure><p>After changing to the correct time zone, proceed as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service apache2 restart</span><br></pre></td></tr></table></figure><p>Then access the following link from the browser (note that the address should be replaced with the address of your environment, and if you want to access from the public network address, pay attention to configure the HUAWEI CLOUD security group rules to limit the address range to be logged in.)</p><p><a href=""><em>http://ip address of the test machine/zabbix</em></a></p><p>Then click next and follow the prompts to configure step by step.</p><p>Configure the database connection  as shown</p><p><a href="https://i.loli.net/2020/11/20/9BM2ECHVf8X3ybA.png"><img src="https://i.loli.net/2020/11/20/9BM2ECHVf8X3ybA.png" alt="image-20201118144826675"></a></p><p><a href="https://i.loli.net/2020/11/20/9BM2ECHVf8X3ybA.png">image-20201118144826675</a></p><p>Where the server is configured, just use the default value</p><p><a href="https://i.loli.net/2020/11/20/DvoruKhilXWFjcm.png"><img src="https://i.loli.net/2020/11/20/DvoruKhilXWFjcm.png" alt="image-20201118144923513"></a></p><p><a href="https://i.loli.net/2020/11/20/DvoruKhilXWFjcm.png">image-20201118144923513</a></p><p>In the last step, you will be prompted to download a configuration file, download the configuration file and upload it to the <code>/etc/zabbix</code>directory.</p><p><a href="https://i.loli.net/2020/11/20/OIWoK3hz6GBE1tx.png"><img src="https://i.loli.net/2020/11/20/OIWoK3hz6GBE1tx.png" alt="image-20201118150217001"></a></p><p><a href="https://i.loli.net/2020/11/20/OIWoK3hz6GBE1tx.png">image-20201118150217001</a></p><p>Finally restart apache2</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service apache2 restart</span><br></pre></td></tr></table></figure><p>Then re-login to the <em><a href="http://ip">http://ip</a> address of the test machine/zabbix</em> web page, enter the user name Admin, password zabbix, and you can see the Zabbix system page.</p><p>Then we create two Hosts, one is the MariaDB master database and the other is the MariaDB slave database.</p><p>Click Configuration, Hosts, Create host, where Host name, Groups and IP address are required.</p><p><a href="https://i.loli.net/2020/11/20/vzFMQmtLAkD9WZU.png"><img src="https://i.loli.net/2020/11/20/vzFMQmtLAkD9WZU.png" alt="image-20201119101133383"></a></p><p><a href="https://i.loli.net/2020/11/20/vzFMQmtLAkD9WZU.png">image-20201119101133383</a></p><p>After the host is created, you can click the Templates button circled in the above figure, go to the following page, enter mysql to search for the template, and then click the add button to associate the mysql monitoring template to the host</p><p><a href="https://i.loli.net/2020/11/20/TGKsMeYZyQSUHhm.png"><img src="https://i.loli.net/2020/11/20/TGKsMeYZyQSUHhm.png" alt="image-20201119101336228"></a></p><p><a href="https://i.loli.net/2020/11/20/TGKsMeYZyQSUHhm.png">image-20201119101336228</a></p><p>The configuration of Frontend is complete.</p><p>Log in to the virtual machine where the two agents are located and do the following configuration</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp &#x2F;usr&#x2F;share&#x2F;doc&#x2F;zabbix-agent&#x2F;examples&#x2F;userparameter_mysql.conf &#x2F;etc&#x2F;zabbix&#x2F;zabbix_agentd.conf.d&#x2F;</span><br><span class="line">mkdir -p &#x2F;var&#x2F;lib&#x2F;zabbix</span><br></pre></td></tr></table></figure><p>Edit configuration file</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi &#x2F;var&#x2F;lib&#x2F;zabbix&#x2F;.my.cnf</span><br></pre></td></tr></table></figure><p>Enter the following:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[mysql]</span><br><span class="line">user&#x3D;proxysql</span><br><span class="line">password&#x3D;proxypassword</span><br><span class="line">host&#x3D;127.0.0.1</span><br><span class="line"></span><br><span class="line">[mysqladmin]</span><br><span class="line">user&#x3D;proxysql</span><br><span class="line">password&#x3D;proxypassword</span><br><span class="line">host&#x3D;127.0.0.1</span><br></pre></td></tr></table></figure><p>Note that the entered user must be connected to MariaDB on this machine and have the relevant permissions to query.</p><p>Then edit the <code>/etc/zabbix/zabbix_agentd.conf</code>file to ensure that the configuration of the following three items is correct</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Server&#x3D;192.168.0.54</span><br><span class="line">ServerActive&#x3D;192.168.0.54</span><br><span class="line">Hostname&#x3D;mariadb_slave</span><br></pre></td></tr></table></figure><p>Server and ServerActive are the IP addresses of Zabbix Server. And Hostname is the name of the  Agent host name which you configured on Frontend, this configuration of each Agent is different.</p><p>Then restart the agent</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service zabbix-agent restart</span><br></pre></td></tr></table></figure><p>Now log in to the front end again, and you can see the monitoring effect.</p><p>Click Monitoring, Latest Data, you can see many monitoring items, bandwidth, slow query, select, insert, delete request qps and so on.</p><p>Let’s look at a bandwidth monitoring chart, the effect is as follows:</p><p><a href="https://i.loli.net/2020/11/20/Ie3qKTyPkN2DM1h.png"><img src="https://i.loli.net/2020/11/20/Ie3qKTyPkN2DM1h.png" alt="image-20201119102130275"></a></p><p><a href="https://i.loli.net/2020/11/20/Ie3qKTyPkN2DM1h.png">image-20201119102130275</a></p><p>If you feel that the default Mysql template function is not perfect enough, interested friends can customize the template by themselves, or download the customized template from the Percona website and import it. I won’t go into details here</p><p><strong>Prometheus</strong></p><p>Prometheus is also a large-scale monitoring  tool. This time we will only briefly test its database monitoring function. I will not say more about its architecture, functions and other aspects.</p><p>Let’s first install Prometheus Server.</p><p>We download the latest arm64 version of Prometheus from the official website to install it.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;releases&#x2F;download&#x2F;v2.22.2&#x2F;prometheus-2.22.2.linux-arm64.tar.gz</span><br><span class="line">tar -zxvf prometheus-2.22.2.linux-arm64.tar.gz</span><br><span class="line">cd prometheus-2.22.2.linux-arm64</span><br></pre></td></tr></table></figure><p>Download the latest mysqld_exporter on the master and slave database nodes to install</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;mysqld_exporter&#x2F;releases&#x2F;download&#x2F;v0.12.1&#x2F;mysqld_exporter-0.12.1.linux-arm64.tar.gz</span><br><span class="line">tar -zxvf mysqld_exporter-0.12.1.linux-arm64.tar.gz</span><br><span class="line">cd mysqld_exporter-0.12.1.linux-arm64</span><br></pre></td></tr></table></figure><p>Edit the file <code>vi prometheus.yml</code>and add the following configuration. The IP address inside is the IP of the master-slave database node, and the port is the port equivalent to the client process that we will install on the master-slave database node later. Let’s configure it first.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- job_name: &#39;mariadb&#39;</span><br><span class="line">  static_configs:</span><br><span class="line">  - targets: [&#39;192.168.0.204:9104&#39;,&#39;192.168.0.64:9104&#39;]</span><br></pre></td></tr></table></figure><p>Start Prometheus</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;prometheus --config.file &#x3D; prometheus.yml</span><br></pre></td></tr></table></figure><p>Edit the following files on the master and slave database nodes</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi .my.cnf</span><br></pre></td></tr></table></figure><p>Enter the following content, pay attention to ensure that the entered user has the permission to connect to MariaDB locally and has the permission to query:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[client]</span><br><span class="line">user&#x3D;proxysql</span><br><span class="line">password&#x3D;proxypassword</span><br></pre></td></tr></table></figure><p>Start mysqld_exporter</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup .&#x2F;mysqld_exporter --config.my-cnf&#x3D;.&#x2F;.my.cnf &amp;</span><br></pre></td></tr></table></figure><p>Log in to the Prometheus page: (Note that the address should be replaced with the address of your environment. In addition, if you want to access from the public network address, pay attention to configuring the HUAWEI CLOUD security group rules to limit the address range to be logged in.)</p><p>[<a href="http://your">http://your</a> IP address:9090](<a href="http://your">http://your</a> IP address:9090)</p><p>You can see many indicators in the drop-down box next to the Execute button. We select a number of connections and adjust the monitoring time. For example, if we adjust it to the past 30 minutes, then we can see the monitoring chart as shown in the following figure.</p><p><a href="https://i.loli.net/2020/11/20/2RTmQVrpZGzevg1.png"><img src="https://i.loli.net/2020/11/20/2RTmQVrpZGzevg1.png" alt="image-20201120143145987"></a></p><p><a href="https://i.loli.net/2020/11/20/2RTmQVrpZGzevg1.png">image-20201120143145987</a></p><p><strong>Summary</strong></p><p>Through the above tests, we can find that these commonly used monitoring tools can all run well on the arm64 platform, and can monitor MariaDB databases with good results. And these tools are open source and free, interested friends can explore more functions.</p><p><strong>Reference link:</strong></p><p><a href="https://www.cnblogs.com/ivictor/p/5101506.html">https://www.cnblogs.com/ivictor/p/5101506.html</a></p><p><a href="https://www.jianshu.com/p/b8508fe10b8e">https://www.jianshu.com/p/b8508fe10b8e</a></p><p><a href="https://github.com/innotop/innotop">https://github.com/innotop/innotop</a></p><p><a href="https://github.com/jzawodn/mytop">https://github.com/jzawodn/mytop</a></p><p><a href="http://jeremy.zawodny.com/mysql/mytop/">http://jeremy.zawodny.com/mysql/mytop/</a></p><p><a href="https://www.zabbix.com/documentation/4.0/zh/manual/introduction">https://www.zabbix.com/documentation/4.0/zh/manual/introduction</a></p><p><a href="https://prometheus.io/docs/prometheus/latest/getting_started/">https://prometheus.io/docs/prometheus/latest/getting_started/</a></p><p><a href="https://www.cnblogs.com/heian99/p/12189317.html">https://www.cnblogs.com/heian99/p/12189317.html</a></p><p><a href="https://prometheus.io/download/">https://prometheus.io/download/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Author: zhaorenhai&lt;/p&gt;
&lt;p&gt;This article attempts to explore some useful performance monitoring tools on the arm64 platform that can be used to monitor MariaDB databases.&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>How do top programming languages connect to MariaDB database on arm64 platform</title>
    <link href="https://kunpengcompute.github.io/2020/12/08/how-do-top-programming-languages-connect-to-mariadb-database-on-arm64-platform/"/>
    <id>https://kunpengcompute.github.io/2020/12/08/how-do-top-programming-languages-connect-to-mariadb-database-on-arm64-platform/</id>
    <published>2020-12-08T12:48:51.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>Author: zhaorenhai</p><p>This article attempts to explore how several top programming languages connect to MariaDB database on the arm64 platform, and test whether the addition, deletion, modification, and query functions are normal. Include the following languages: C, Java, Node.js, Python, Go, Rust, PHP</p><a id="more"></a><p>The test platform uses Huawei Cloud’s Kunpeng virtual machine, and the OS uses Ubuntu 18.04. In addition, a MariaDB database has been deployed on another virtual machine in the same intranet in advance, and the database user name and password for remote connection have been created. Here assume that our new user name is proxysql and the password is proxypassword. Use the default port number 3306 for the port number.</p><p><strong>C</strong></p><p>First install the connector library</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install libmariadbclient-dev</span><br></pre></td></tr></table></figure><p>Then create a new C language file <code>version.c</code>and enter the following code:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mysql.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"MySQL client version: %s\n"</span>, mysql_get_client_info());</span><br><span class="line">  <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Compile and execute:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gcc version.c -o version  `mysql_config --cflags --libs`</span><br><span class="line">./version</span><br></pre></td></tr></table></figure><p>You can see the output as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MySQL client version: 10.1.47-MariaDB</span><br></pre></td></tr></table></figure><p>It shows that the client version can be printed out successfully.</p><p>Next, we create a new <code>createdb.c</code>file, enter the following code, test the function of creating a database</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mysql.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  MYSQL *con = mysql_init(<span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (con == <span class="literal">NULL</span>)</span><br><span class="line">  &#123;</span><br><span class="line">      <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"%s\n"</span>, mysql_error(con));</span><br><span class="line">      <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (mysql_real_connect(con, <span class="string">"192.168.0.204"</span>, <span class="string">"proxysql"</span>, <span class="string">"proxypassword"</span>,</span><br><span class="line">    <span class="literal">NULL</span>, <span class="number">0</span>, <span class="literal">NULL</span>, <span class="number">0</span>) == <span class="literal">NULL</span>)</span><br><span class="line">  &#123;</span><br><span class="line">      <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"%s\n"</span>, mysql_error(con));</span><br><span class="line">      mysql_close(con);</span><br><span class="line">      <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"CREATE DATABASE testdb"</span>))</span><br><span class="line">  &#123;</span><br><span class="line">      <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"%s\n"</span>, mysql_error(con));</span><br><span class="line">      mysql_close(con);</span><br><span class="line">      <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  mysql_close(con);</span><br><span class="line">  <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Compile and execute</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gcc createdb.c -o createdb  `mysql_config --cflags --libs`</span><br><span class="line">./createdb</span><br></pre></td></tr></table></figure><p>Next, we create a new <code>testdb.c</code>file, enter the following code, and test the functions of adding new tables, inserting records, querying tables, associating queries, updating, deleting records, and deleting tables.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mysql.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">finish_with_error</span><span class="params">(MYSQL *con)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"%s\n"</span>, mysql_error(con));</span><br><span class="line">  mysql_close(con);</span><br><span class="line">  <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  MYSQL *con = mysql_init(<span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (con == <span class="literal">NULL</span>)</span><br><span class="line">  &#123;</span><br><span class="line">      <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"%s\n"</span>, mysql_error(con));</span><br><span class="line">      <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (mysql_real_connect(con, <span class="string">"192.168.0.204"</span>, <span class="string">"proxysql"</span>, <span class="string">"proxypassword"</span>,</span><br><span class="line">    <span class="string">"testdb"</span>, <span class="number">0</span>, <span class="literal">NULL</span>, <span class="number">0</span>) == <span class="literal">NULL</span>)</span><br><span class="line">  &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"DROP TABLE IF EXISTS cars;"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"DROP TABLE IF EXISTS people;"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"CREATE TABLE cars(id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(255), price INT)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;    </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"CREATE TABLE people(id INT, name VARCHAR(255), car_id INT)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;    </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO cars VALUES(1,'Audi',52642)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO cars VALUES(2,'Mercedes',57127)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO cars VALUES(3,'Skoda',9000)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO cars VALUES(4,'Volvo',29000)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO cars VALUES(5,'Bentley',350000)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO cars VALUES(6,'Citroen',21000)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO cars VALUES(7,'Hummer',41400)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO cars VALUES(8,'Volkswagen',21600)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;    </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO people VALUES(1,'Jim',7)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;  </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO people VALUES(1,'Jim',8)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;    </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO people VALUES(2,'Tom',6)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"SELECT * FROM cars"</span>))</span><br><span class="line">  &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  MYSQL_RES *result = mysql_store_result(con);</span><br><span class="line">  <span class="keyword">if</span> (result == <span class="literal">NULL</span>)</span><br><span class="line">  &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">int</span> num_fields = mysql_num_fields(result);</span><br><span class="line">  MYSQL_ROW row;</span><br><span class="line">  <span class="keyword">while</span> ((row = mysql_fetch_row(result)))</span><br><span class="line">  &#123;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_fields; i++)</span><br><span class="line">      &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%s "</span>, row[i] ? row[i] : <span class="string">"NULL"</span>);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  mysql_free_result(result);</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"update cars set price = 42400 where name = 'Hummer'"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;   </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"SELECT people.name,cars.name,cars.price FROM cars,people where cars.id = people.car_id"</span>))</span><br><span class="line">  &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  result = mysql_store_result(con);</span><br><span class="line">  <span class="keyword">if</span> (result == <span class="literal">NULL</span>)</span><br><span class="line">  &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  num_fields = mysql_num_fields(result);</span><br><span class="line">  <span class="keyword">while</span> ((row = mysql_fetch_row(result)))</span><br><span class="line">  &#123;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_fields; i++)</span><br><span class="line">      &#123;</span><br><span class="line">         <span class="built_in">printf</span>(<span class="string">"%s "</span>, row[i] ? row[i] : <span class="string">"NULL"</span>);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">  &#125;  </span><br><span class="line">  mysql_free_result(result);</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"delete from people where id = 1"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"drop table people;"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;  </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"drop table cars;"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;  </span><br><span class="line">    </span><br><span class="line">  mysql_close(con);</span><br><span class="line">  <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Compile and execute:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gcc testdb.c -o testdb  &#96;mysql_config --cflags --libs&#96;</span><br><span class="line">.&#x2F;testdb</span><br></pre></td></tr></table></figure><p>The output is as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1 Audi 52642 </span><br><span class="line">2 Mercedes 57127 </span><br><span class="line">3 Skoda 9000 </span><br><span class="line">4 Volvo 29000 </span><br><span class="line">5 Bentley 350000 </span><br><span class="line">6 Citroen 21000 </span><br><span class="line">7 Hummer 41400 </span><br><span class="line">8 Volkswagen 21600 </span><br><span class="line">Jim Hummer 42400 </span><br><span class="line">Jim Volkswagen 21600 </span><br><span class="line">Tom Citroen 21000</span><br></pre></td></tr></table></figure><p>According to the above test, it can be found that C language connects to MariaDB database, adds, deletes, changes and querys, creates tables, deletes tables, and other functions are all normal on the arm64 platform.</p><p>The above test just demonstrates the installation of the MariaDB Connector from the OS software source. If you want to use the latest version of the Connector, you can also refer to the official documentation to compile the latest version: <a href="https://mariadb.com/kb/en/mariadb-connector-c/">https://mariadb.com/kb/en/mariadb- connector-c/</a></p><p>In addition, MariaDB’s C connector project is issued by LGPL’s license. </p><p><strong>Java</strong></p><p>First, make sure that the latest versions of OpenJDK and Maven have been installed on the test virtual machine.</p><p>Create a sample project with Maven:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mvn archetype:generate -DgroupId=com.mycompany.app -DartifactId=my-app </span><br><span class="line">-DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false</span><br></pre></td></tr></table></figure><p>Then in the generated <code>pom.xml</code>dependencies module, add the following content:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mariadb.jdbc<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mariadb-java-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Then edit <code>src/main/java/com/mycompany/app</code>the <code>App.java</code>file in the directory , the content is as follows</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.mycompany.app;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>  <span class="class"><span class="keyword">class</span>  <span class="title">App</span>  </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String[] args )</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> (Connection conn = DriverManager.getConnection(<span class="string">"jdbc:mariadb://192.168.0.204/"</span>, <span class="string">"proxysql"</span>, <span class="string">"proxypassword"</span>)) &#123;</span><br><span class="line">      <span class="comment">// create a Statement</span></span><br><span class="line">      <span class="keyword">try</span> (Statement stmt = conn.createStatement()) &#123;</span><br><span class="line">        <span class="comment">//execute query</span></span><br><span class="line">        <span class="keyword">try</span> (ResultSet rs = stmt.executeQuery(<span class="string">"SELECT 'Hello World!'"</span>)) &#123;</span><br><span class="line">          <span class="comment">//position result to first</span></span><br><span class="line">          rs.first();</span><br><span class="line">          System.out.println(rs.getString(<span class="number">1</span>)); <span class="comment">//result is "Hello World!"</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125; </span><br><span class="line">    &#125; </span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Then execute the following command to compile</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn install</span><br></pre></td></tr></table></figure><p>Execute the following command to execute</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn exec:java -Dexec.mainClass="com.mycompany.app.App"</span><br></pre></td></tr></table></figure><p>The output is <code>Hello Word!</code></p><p>The above is just a test of relatively simple functions. Below we test the connection pool function and perform functions such as adding, deleting, modifying, and query.</p><p>The revised <code>App.java</code>content is as follows:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.mycompany.app;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>  <span class="class"><span class="keyword">class</span>  <span class="title">App</span>  </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String[] args )</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">    <span class="comment">//option "pool" must be set to indicate that pool has to be used</span></span><br><span class="line">    String connectionString = <span class="string">"jdbc:mariadb://192.168.0.204/testdb?user=proxysql&amp;password=proxypassword&amp;maxPoolSize=10&amp;pool"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> (Connection connection = DriverManager.getConnection(connectionString)) &#123;</span><br><span class="line">      <span class="keyword">try</span> (Statement stmt = connection.createStatement()) &#123;</span><br><span class="line">        ResultSet rs = stmt.executeQuery(<span class="string">"DROP TABLE IF EXISTS cars;"</span>);</span><br><span class="line">        stmt.executeQuery(<span class="string">"CREATE TABLE cars(id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(255), price INT)"</span>);</span><br><span class="line">        stmt.executeQuery(<span class="string">"INSERT INTO cars VALUES(1,'Audi',52642)"</span>);</span><br><span class="line">        stmt.executeQuery(<span class="string">"INSERT INTO cars VALUES(2,'Mercedes',57127)"</span>);</span><br><span class="line">        rs = stmt.executeQuery(<span class="string">"SELECT * FROM cars"</span>);</span><br><span class="line">        rs.next();</span><br><span class="line">        System.out.println(rs.getString(<span class="number">2</span>)); </span><br><span class="line">      &#125; </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> (Connection connection = DriverManager.getConnection(connectionString)) &#123;</span><br><span class="line">      <span class="keyword">try</span> (Statement stmt = connection.createStatement()) &#123;</span><br><span class="line">        stmt.executeQuery(<span class="string">"update cars set name = 'VolksWagen' where id = 1"</span>);</span><br><span class="line">        stmt.executeQuery(<span class="string">"delete from cars where id = 2"</span>);</span><br><span class="line">        ResultSet rs = stmt.executeQuery(<span class="string">"SELECT * FROM cars"</span>);</span><br><span class="line">        rs.next();</span><br><span class="line">        System.out.println(rs.getString(<span class="number">2</span>));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Compile and execute, output successfully</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Audi </span><br><span class="line">VolksWagen</span><br></pre></td></tr></table></figure><p>After the above test, the connection pool function and the addition, deletion, modification, and checking functions are also normal.</p><p>Java’s connector functions are relatively rich, and it also implements load balancing or read-write separation for clusters or primary-replica databases. Next we test the read-write separation function. Before doing so, please make sure that you have set up the MariaDB primary-replica database environment.</p><p>Change <code>App.java</code> to the following code. Note that the replication keyword is added to the jdbc connection string, which represents primary-replica  replication. If it is in other load balancing environments, keywords such as loadbalance are also supported. For details, refer to the official MariaDB documentation. Also note that in the connection string, the primary database address is first and the replica database address is last. When the connection attribute is changed to ReadOnly, the statement will go to the replica database to query.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.mycompany.app;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>  <span class="class"><span class="keyword">class</span>  <span class="title">App</span>  </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String[] args )</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">        String connectionString = <span class="string">"jdbc:mysql:replication://192.168.0.204,192.168.0.64/testdb?user=proxysql&amp;password=proxypassword&amp;maxPoolSize=10&amp;pool"</span>;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">try</span> (Connection connection = DriverManager.getConnection(connectionString)) &#123;</span><br><span class="line">            <span class="keyword">try</span> (Statement stmt = connection.createStatement()) &#123;</span><br><span class="line">                stmt.executeQuery(<span class="string">"INSERT INTO cars VALUES(2,'Mercedes',57127)"</span>);</span><br><span class="line">                connection.setReadOnly(<span class="keyword">true</span>);</span><br><span class="line">                ResultSet rs = stmt.executeQuery(<span class="string">"SELECT * FROM cars"</span>);</span><br><span class="line">                rs.next();</span><br><span class="line">                System.out.println(rs.getString(<span class="number">2</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Compile and execute, the output is as follows</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">VolksWagen</span><br></pre></td></tr></table></figure><p>execution succeed.</p><p>The above demonstrated project uses Maven as an example. If you are not using Maven, you can also refer to the instructions of other tools in the official MariaDB documentation: <a href="https://mariadb.com/kb/en/mariadb-connector-j/">https://mariadb.com/kb/en/mariadb-connector-j/</a></p><p>MariaDB Java connector is also released under the LGPL agreement.</p><p><strong>Python</strong></p><p>First make sure that Python3 and pip3 have been installed on the test virtual machine.</p><p>To connect MariaDB in Python, you need to install the C language connector first, you can execute the following command to install</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt install libmariadb-dev</span><br></pre></td></tr></table></figure><p>However, the current version of the mariadb package that comes with the current Ubuntu 18.04 version is older and cannot meet the python version requirements. We compile one from the source code.</p><p>Execute the following command to download the connector source code and unzip it</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://downloads.mariadb.org/interstitial/connector-c-3.1.11/mariadb-connector-c-3.1.11-src.zip</span><br><span class="line">unzip mariadb-connector-c-3.1.11-src.zip</span><br></pre></td></tr></table></figure><p>Start compiling.</p><p>Create a separate compilation directory, compile in this directory, and install</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir build-mariadb-connector-c</span><br><span class="line">cd build-mariadb-connector-c</span><br><span class="line">cmake ../mariadb-connector-c-3.1.11-src -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/usr/local</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line">echo "/usr/local/lib/mariadb/" &gt; /etc/ld.so.conf.d/mariadb.conf</span><br><span class="line">ldconfig</span><br></pre></td></tr></table></figure><p>Then execute the following command to install the Python connector:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install mariadb</span><br></pre></td></tr></table></figure><p>Next, we directly test the connection pool function, and simply test the query and new functions.</p><p>Edit the following code and name it to <code>testmariadb.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mariadb</span><br><span class="line"></span><br><span class="line">pool = mariadb.ConnectionPool(</span><br><span class="line">        user=<span class="string">"proxysql"</span>,</span><br><span class="line">        password=<span class="string">"proxypassword"</span>,</span><br><span class="line">        host=<span class="string">"192.168.0.204"</span>,</span><br><span class="line">        port=<span class="number">3306</span>,</span><br><span class="line">        pool_name=<span class="string">"web-app"</span>,</span><br><span class="line">        pool_size=<span class="number">20</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    pconn = pool.get_connection()</span><br><span class="line">    cur = pconn.cursor()</span><br><span class="line">    data = [(<span class="number">3</span>, <span class="string">'Skoda'</span>, <span class="number">9000</span>),(<span class="number">4</span>, <span class="string">'Volvo'</span>, <span class="number">29000</span>),(<span class="number">5</span>, <span class="string">'Bently'</span>, <span class="number">350000</span>)]</span><br><span class="line">    cur.executemany(<span class="string">"INSERT INTO testdb.cars(id, name, price) VALUES (?, ?, ?)"</span>, data)</span><br><span class="line">    cur.execute(<span class="string">"select * from testdb.cars"</span>)</span><br><span class="line">    cars = []</span><br><span class="line">    <span class="keyword">for</span> (id,name,price) <span class="keyword">in</span> cur:</span><br><span class="line">        cars.append(<span class="string">f"<span class="subst">&#123;id&#125;</span> <span class="subst">&#123;name&#125;</span> <span class="subst">&#123;price&#125;</span>"</span>)</span><br><span class="line">    print(<span class="string">"\n"</span>.join(cars))</span><br><span class="line"><span class="keyword">except</span> mariadb.PoolError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># Report Error</span></span><br><span class="line">    print(<span class="string">f"Error opening connection from pool: <span class="subst">&#123;e&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><p>Execute:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 testmariadb.py</span><br></pre></td></tr></table></figure><p>The output is as follows</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1 VolksWagen 52642</span><br><span class="line">2 Mercedes 57127</span><br><span class="line">3 Skoda 9000</span><br><span class="line">4 Volvo 29000</span><br><span class="line">5 Bently 350000</span><br></pre></td></tr></table></figure><p>For other functions of the Python connector, you can refer to the following link: <a href="https://mariadb.com/docs/appdev/connector-python/">https://mariadb.com/docs/appdev/connector-python/</a></p><p>Python connector is also LGPL agreement.</p><p><strong>Node.js</strong></p><p>We must firstly install nodejs and npm. The mariadb connector requires at least nodejs version 10.13 or above, so we download an arm64 binary package from the official website and install it manually</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget https://nodejs.org/dist/v14.15.0/node-v14.15.0-linux-arm64.tar.xz </span><br><span class="line">xz -d node-v14.15.0-linux-arm64.tar.xz </span><br><span class="line">tar -xf node-v14 .15.0-linux-arm64.tar </span><br><span class="line">cd node-v14.15.0-linux-arm64/bin </span><br><span class="line">sudo ln -s `pwd`/node/usr/local/bin/ </span><br><span class="line">sudo ln -s `pwd`/npm/usr/local/bin/</span><br></pre></td></tr></table></figure><p>Then install the mariadb connector</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install mariadb</span><br></pre></td></tr></table></figure><p>Then edit a <code>testmariadb.js</code>file with the following content:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> mariadb = <span class="built_in">require</span>(<span class="string">'mariadb'</span>);</span><br><span class="line"><span class="keyword">const</span> pool = mariadb.createPool(&#123;</span><br><span class="line">     host:<span class="string">'192.168.0.204'</span>,</span><br><span class="line">     user:<span class="string">'proxysql'</span>,</span><br><span class="line">     password: <span class="string">'proxypassword'</span>,</span><br><span class="line">     connectionLimit: <span class="number">5</span></span><br><span class="line">&#125;);</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">function</span> <span class="title">asyncFunction</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">let</span> conn;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    conn = <span class="keyword">await</span> pool.getConnection();</span><br><span class="line">    <span class="keyword">const</span> res = <span class="keyword">await</span> conn.query(<span class="string">"INSERT INTO testdb.cars value (?, ?, ?)"</span>, [<span class="number">6</span>,<span class="string">'Citroen'</span>,<span class="number">21000</span>]);</span><br><span class="line">    <span class="built_in">console</span>.log(res);</span><br><span class="line">    <span class="keyword">const</span> rows = <span class="keyword">await</span> conn.query(<span class="string">"SELECT * from testdb.cars"</span>);</span><br><span class="line">    <span class="built_in">console</span>.log(rows);</span><br><span class="line"></span><br><span class="line">  &#125; <span class="keyword">catch</span> (err) &#123;</span><br><span class="line">    <span class="keyword">throw</span> err;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (conn) <span class="keyword">return</span> conn.end();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">asyncFunction();</span><br></pre></td></tr></table></figure><p>Then execute</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node testmariadb.js</span><br></pre></td></tr></table></figure><p>The output is as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">OkPacket &#123; affectedRows: 1, insertId: 6, warningStatus: 0 &#125;</span><br><span class="line">[</span><br><span class="line">  &#123; id: 1, name: &#39;VolksWagen&#39;, price: 52642 &#125;,</span><br><span class="line">  &#123; id: 2, name: &#39;Mercedes&#39;, price: 57127 &#125;,</span><br><span class="line">  &#123; id: 3, name: &#39;Skoda&#39;, price: 9000 &#125;,</span><br><span class="line">  &#123; id: 4, name: &#39;Volvo&#39;, price: 29000 &#125;,</span><br><span class="line">  &#123; id: 5, name: &#39;Bently&#39;, price: 350000 &#125;,</span><br><span class="line">  &#123; id: 6, name: &#39;Citroen&#39;, price: 21000 &#125;,</span><br><span class="line">  meta: [</span><br><span class="line">    ColumnDef &#123;</span><br><span class="line">      _parse: [StringParser],</span><br><span class="line">      collation: [Collation],</span><br><span class="line">      columnLength: 11,</span><br><span class="line">      columnType: 3,</span><br><span class="line">      flags: 16899,</span><br><span class="line">      scale: 0,</span><br><span class="line">      type: &#39;LONG&#39;</span><br><span class="line">    &#125;,</span><br><span class="line">    ColumnDef &#123;</span><br><span class="line">      _parse: [StringParser],</span><br><span class="line">      collation: [Collation],</span><br><span class="line">      columnLength: 1020,</span><br><span class="line">      columnType: 253,</span><br><span class="line">      flags: 0,</span><br><span class="line">      scale: 0,</span><br><span class="line">      type: &#39;VAR_STRING&#39;</span><br><span class="line">    &#125;,</span><br><span class="line">    ColumnDef &#123;</span><br><span class="line">      _parse: [StringParser],</span><br><span class="line">      collation: [Collation],</span><br><span class="line">      columnLength: 11,</span><br><span class="line">      columnType: 3,</span><br><span class="line">      flags: 0,</span><br><span class="line">      scale: 0,</span><br><span class="line">      type: &#39;LONG&#39;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>The connector of MariaDB Node.js also uses the LGPL license.</p><p>For other information about Node.js connectors, please refer to: <a href="https://mariadb.com/kb/en/nodejs-connector/">https://mariadb.com/kb/en/nodejs-connector/</a></p><p><strong>PHP</strong></p><p>The code for PHP to connect to MariaDB and to connect to Mysql is the same.</p><p>First install the php and mysql drivers on the test machine:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install php</span><br><span class="line">sudo apt install php-mysql</span><br></pre></td></tr></table></figure><p>Then edit a <code>testmariadb.php</code>file</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line">$mysqli = <span class="keyword">new</span> mysqli(<span class="string">"192.168.0.204"</span>, <span class="string">"proxysql"</span>, <span class="string">"proxypassword"</span>, <span class="string">"testdb"</span>);</span><br><span class="line">$mysqli-&gt;query(<span class="string">"insert into cars values(7,'Hummer',41400)"</span>);</span><br><span class="line">$result = $mysqli-&gt;query(<span class="string">"SELECT * FROM cars where id = 7"</span>);</span><br><span class="line">$row = $result-&gt;fetch_assoc();</span><br><span class="line"><span class="keyword">echo</span> htmlentities($row[<span class="string">'name'</span>]);</span><br><span class="line"><span class="meta">?&gt;</span></span><br></pre></td></tr></table></figure><p>Execute:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">php testmariadb.php</span><br></pre></td></tr></table></figure><p>Output</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hummer</span><br></pre></td></tr></table></figure><p>For PHP, we only tested two simple scenarios, insert and query, and proved that PHP is feasible to connect to MariaDB on the arm64 platform. For other information about PHP connecting mysql and mariadb databases, you can refer to the following link:</p><p><a href="https://www.php.net/manual/en/mysql.php">https://www.php.net/manual/en/mysql.php</a></p><p><strong>Go</strong></p><p>Connecting to MariaDB database in Go language is the same as connecting to Mysql database.</p><p>First install the Mysql driver of the Go language, (the protocol of this driver is MPL2.0, which is also a relatively loose protocol)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go get -u github.com/go-sql-driver/mysql</span><br></pre></td></tr></table></figure><p>Edit the <code>testmariadb.go</code>file, the content is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">"fmt"</span></span><br><span class="line"><span class="keyword">import</span> <span class="string">"strings"</span></span><br><span class="line"><span class="keyword">import</span> <span class="string">"database/sql"</span></span><br><span class="line"><span class="keyword">import</span> _ <span class="string">"github.com/go-sql-driver/mysql"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">const</span> (</span><br><span class="line">        userName = <span class="string">"proxysql"</span></span><br><span class="line">        password = <span class="string">"proxypassword"</span></span><br><span class="line">        ip = <span class="string">"192.168.0.204"</span></span><br><span class="line">        port = <span class="string">"3306"</span></span><br><span class="line">        dbName = <span class="string">"testdb"</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">var</span> DB *sql.DB</span><br><span class="line">    path := strings.Join([]<span class="keyword">string</span>&#123;userName, <span class="string">":"</span>, password, <span class="string">"@tcp("</span>,ip, <span class="string">":"</span>, port, <span class="string">")/"</span>, dbName, <span class="string">"?charset=utf8"</span>&#125;, <span class="string">""</span>)</span><br><span class="line"></span><br><span class="line">    DB, _ = sql.Open(<span class="string">"mysql"</span>, path)</span><br><span class="line">    DB.SetConnMaxLifetime(<span class="number">100</span>)</span><br><span class="line">    DB.SetMaxIdleConns(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">if</span> err := DB.Ping(); err != <span class="literal">nil</span>&#123;</span><br><span class="line">        fmt.Println(<span class="string">"open database fail"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    fmt.Println(<span class="string">"connnect success"</span>)</span><br><span class="line">    tx, err := DB.Begin()</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span>&#123;</span><br><span class="line">        fmt.Println(<span class="string">"tx fail"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    stmt, err := tx.Prepare(<span class="string">"INSERT INTO cars VALUES (?,?,?)"</span>)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span>&#123;</span><br><span class="line">        fmt.Println(<span class="string">"Prepare fail"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    res, err := stmt.Exec(<span class="number">8</span>, <span class="string">"Mercedes"</span>, <span class="number">57127</span>)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span>&#123;</span><br><span class="line">        fmt.Println(<span class="string">"Exec fail"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    tx.Commit()</span><br><span class="line">    fmt.Println(res.LastInsertId())</span><br><span class="line"></span><br><span class="line">    rows, err := DB.Query(<span class="string">"SELECT * from cars"</span>)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span>&#123;</span><br><span class="line">        fmt.Println(<span class="string">"query fail"</span>)</span><br><span class="line">    &#125;    </span><br><span class="line">    <span class="keyword">for</span> rows.Next()&#123;</span><br><span class="line">        <span class="keyword">var</span> id <span class="keyword">int</span></span><br><span class="line">        <span class="keyword">var</span> name <span class="keyword">string</span></span><br><span class="line">        <span class="keyword">var</span> price <span class="keyword">int</span></span><br><span class="line">        err := rows.Scan(&amp;id, &amp;name, &amp;price)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            fmt.Println(<span class="string">"rows fail"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        fmt.Printf(<span class="string">"%v %q %v \n"</span>, id, name, price)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Then execute</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go run testmariadb.go</span><br></pre></td></tr></table></figure><p>The output is as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">connnect success</span><br><span class="line">8 &lt;nil&gt;</span><br><span class="line">1 &quot;VolksWagen&quot; 52642</span><br><span class="line">2 &quot;Mercedes&quot; 57127</span><br><span class="line">3 &quot;Skoda&quot; 9000</span><br><span class="line">4 &quot;Volvo&quot; 29000</span><br><span class="line">5 &quot;Bently&quot; 350000</span><br><span class="line">6 &quot;Citroen&quot; 21000</span><br><span class="line">7 &quot;Hummer&quot; 41400</span><br><span class="line">8 &quot;Mercedes&quot; 57127</span><br></pre></td></tr></table></figure><p>Above we tested the connection pool, query, transaction, insert and other functions of the MariaDB connector in the Go language, and they were all successful. For other functions, you can refer to the official website of the go language or other related websites.</p><p><strong>Rust</strong></p><p>Rust connects to MariaDB in the same way as connects to Mysql.</p><p>First install the latest version of Rust:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl --proto '=https' --tlsv1.2 https://sh.rustup.rs -sSf | sh</span><br></pre></td></tr></table></figure><p>After installation, you need to <code>$HOME/.cargo/bin</code>add it to the PATH environment variable</p><p>For the current environment, we execute the following commands to take effect temporarily:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source $HOME/.cargo/env</span><br></pre></td></tr></table></figure><p>Then execute the following command to create a new project:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cargo new mariadb_test</span><br></pre></td></tr></table></figure><p>then</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd mariadb_test</span><br></pre></td></tr></table></figure><p>Edit the <code>Cargo.toml</code>configuration file and <code>[dependencies]</code>add the following below to set the driver version:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql &#x3D; &quot;20.0.1&quot;</span><br></pre></td></tr></table></figure><p>Then edit the <code>src/main.rs</code>file:</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> mysql::*;</span><br><span class="line"><span class="keyword">use</span> mysql::prelude::*;</span><br><span class="line"></span><br><span class="line"><span class="meta">#[derive(Debug, PartialEq, Eq)]</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Car</span></span> &#123;</span><br><span class="line">    id: <span class="built_in">i32</span>,</span><br><span class="line">    name: <span class="built_in">String</span>,</span><br><span class="line">    price: <span class="built_in">i32</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">testmariadb</span></span>() -&gt; std::result::<span class="built_in">Result</span>&lt;std::string::<span class="built_in">String</span>, mysql::Error&gt; &#123;</span><br><span class="line">    <span class="keyword">let</span> url = <span class="string">"mysql://proxysql:proxypassword@192.168.0.204:3306/testdb"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">let</span> pool = Pool::new(url)?;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut</span> conn = pool.get_conn()?;</span><br><span class="line">    </span><br><span class="line">    conn.exec_drop(<span class="string">r"delete from cars"</span>,())?;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">let</span> cars = <span class="built_in">vec!</span>[</span><br><span class="line">        Car &#123; id: <span class="number">1</span>, name: <span class="string">"Audi"</span>.into(), price: <span class="number">52642</span> &#125;,</span><br><span class="line">        Car &#123; id: <span class="number">2</span>, name: <span class="string">"Volkswagen"</span>.into(), price: <span class="number">21600</span> &#125;,</span><br><span class="line">        Car &#123; id: <span class="number">3</span>, name: <span class="string">"Skoda"</span>.into(), price: <span class="number">9000</span> &#125;,</span><br><span class="line">    ];</span><br><span class="line">    conn.exec_batch(</span><br><span class="line">        r<span class="string">"INSERT INTO cars (id, name, price)</span></span><br><span class="line"><span class="string">      VALUES (:id, :name, :price)"</span>,</span><br><span class="line">      cars.iter().map(|p| params! &#123;</span><br><span class="line">          <span class="string">"id"</span> =&gt; p.id,</span><br><span class="line">          <span class="string">"name"</span> =&gt; &amp;p.name,</span><br><span class="line">          <span class="string">"price"</span> =&gt; p.price,</span><br><span class="line">      &#125;)</span><br><span class="line">      )?;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">let</span> selected_cars = conn</span><br><span class="line">        .query_map(</span><br><span class="line">            <span class="string">"SELECT id, name, price from cars"</span>,</span><br><span class="line">            |(id, name, price)| &#123;</span><br><span class="line">                Car &#123; id, name, price &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            )?;</span><br><span class="line">    <span class="built_in">assert_eq!</span>(cars, selected_cars);</span><br><span class="line">    <span class="built_in">println!</span>(<span class="string">"Yay!"</span>);</span><br><span class="line">    <span class="literal">Ok</span>(<span class="string">"Yay!"</span>.into())</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">main</span></span>() &#123;</span><br><span class="line">    testmariadb(); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>We tested the connection pool function, and also tested the delete, insert, query and other functions. After a batch query, the query results are put into a list and compared with the data list before inserting. If they are equal, then print<code>Yay!</code></p><p>In the <code>mariadb_test</code>directory, execute the following commands to compile and run</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cargo run</span><br></pre></td></tr></table></figure><p>Output <code>Yay!</code>, run successfully.</p><p>For other functions of the Rust Mysql connector, please refer to:</p><p><a href="https://docs.rs/mysql/20.0.1/mysql/">https://docs.rs/mysql/20.0.1/mysql/</a></p><p>In addition, Rust has a relatively new asynchronous connector. For details, please refer to the following two links:</p><p><a href="https://docs.rs/mysql_async/0.25.0/mysql_async/">https://docs.rs/mysql_async/0.25.0/mysql_async/</a></p><p><a href="https://github.com/blackbeam/mysql_async">https://github.com/blackbeam/mysql_async</a></p><p>Rust’s connector uses the MIT/Apache2.0 license.</p><p><strong>Summary</strong></p><p>We tested C, Java, Node.js, Python, PHP, Go, Rust programming languages. All of them can successfully connect to MariaDB database on the arm64 platform, and the usage is the same as the x86 platform. </p><p><strong>Reference link:</strong></p><p><a href="http://zetcode.com/db/mysqlc/">http://zetcode.com/db/mysqlc/</a></p><p><a href="https://mariadb.com/kb/en/mariadb-connector-c/">https://mariadb.com/kb/en/mariadb-connector-c/</a></p><p><a href="https://mariadb.com/kb/en/mariadb-connector-j/">https://mariadb.com/kb/en/mariadb-connector-j/</a></p><p><a href="https://mariadb.com/docs/appdev/connector-python/">https://mariadb.com/docs/appdev/connector-python/</a></p><p><a href="https://mariadb.com/kb/en/nodejs-connector/">https://mariadb.com/kb/en/nodejs-connector/</a></p><p><a href="https://www.php.net/manual/en/mysql.php">https://www.php.net/manual/en/mysql.php</a></p><p><a href="https://golang.org/pkg/database/sql">https://golang.org/pkg/database/sql</a></p><p><a href="https://github.com/go-sql-driver/mysql">https://github.com/go-sql-driver/mysql</a></p><p><a href="https://doc.rust-lang.org/book/title-page.html">https://doc.rust-lang.org/book/title-page.html</a></p><p><a href="https://docs.rs/mysql/20.0.1/mysql/">https://docs.rs/mysql/20.0.1/mysql/</a></p><p><a href="https://docs.rs/mysql_async/0.25.0/mysql_async/">https://docs.rs/mysql_async/0.25.0/mysql_async/</a></p><p><a href="https://github.com/blackbeam/mysql_async">https://github.com/blackbeam/mysql_async</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Author: zhaorenhai&lt;/p&gt;
&lt;p&gt;This article attempts to explore how several top programming languages connect to MariaDB database on the arm64 platform, and test whether the addition, deletion, modification, and query functions are normal. Include the following languages: C, Java, Node.js, Python, Go, Rust, PHP&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Backup and recovery test of MariaDB on arm64 platform</title>
    <link href="https://kunpengcompute.github.io/2020/12/08/backup-and-recovery-test-of-mariadb-on-arm64-platform/"/>
    <id>https://kunpengcompute.github.io/2020/12/08/backup-and-recovery-test-of-mariadb-on-arm64-platform/</id>
    <published>2020-12-08T12:03:25.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>Author: zhaorenhai</p><p>This article plans to test the operation of MariaDB’s backup and recovery function on the arm64 platform, and provide a reference for those interested in deploying MariaDB on the arm64 platform.</p><p>The test platform chooses the 8C16G Kunpeng virtual machine on Huawei Cloud, the OS is Ubuntu 18.04, and MariaDB we plan to choose the latest version on github for testing.</p><a id="more"></a><p>Then log in to our virtual machine and create a user:</p><p><code>adduser mariadb</code><br>All subsequent work is carried out under this user.</p><p>Switch to this user, download the latest code from github, and compile it.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">su - mariadb </span><br><span class="line">git clone https://github.com/mariadb/server </span><br><span class="line">sudo apt-get install build-essential libncurses5-dev gnutls-dev bison zlib1g-dev ccache libnuma-dev libxml2-dev cmake </span><br><span class="line">mkdir build-mariadb-server </span><br><span class="line">cd build-mariadb-server </span><br><span class="line">cmake ../server -DCMAKE_BUILD_TYPE = RelWithDebInfo </span><br><span class="line">cmake --build .</span><br></pre></td></tr></table></figure><p>Now that we have a database program of the latest version, let’s continue to create the database configuration file and run the database.</p><p>Create a database parameter file:<br><code>vi ~/mariadb.cnf</code></p><p>Set the following parameters:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[mariadb]</span><br><span class="line">datadir&#x3D;&#x2F;home&#x2F;mariadb&#x2F;data&#x2F;dir </span><br><span class="line">lc_messages_dir&#x3D;&#x2F;home&#x2F;mariadb&#x2F;server&#x2F;sql&#x2F;share</span><br><span class="line">innodb_buffer_pool_size &#x3D; 8G </span><br><span class="line">innodb_log_file_size &#x3D; 1G </span><br><span class="line">innodb_flush_method &#x3D; O_DIRECT </span><br><span class="line">skip-grant-tables</span><br><span class="line"># open binlog</span><br><span class="line">log-bin</span><br></pre></td></tr></table></figure><p>Run the database</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /home/mariadb/data/dir </span><br><span class="line">./scripts/mysql_install_db --srcdir=../server --defaults-file=~/mariadb.cnf </span><br><span class="line">sql/mysqld --defaults-file=~/mariadb.cnf</span><br></pre></td></tr></table></figure><p>Now a database is up and running. </p><p>The main purpose of this article is to test the backup and recovery functions. Before that, we have to load some data. We try to load more data. The tool for loading data we use the tpcc-mysql, only its data loading function.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install libmariadbclient-dev </span><br><span class="line">git clone https://github.com/Percona-Lab/tpcc-mysql </span><br><span class="line">cd tpcc-mysql/src </span><br><span class="line">make </span><br><span class="line">cd .. </span><br><span class="line">export PATH=~/build-mariadb-server/client:$PATH   </span><br><span class="line">mysqladmin create tpcc100 </span><br><span class="line">mysql tpcc100 &lt;create_table.sql </span><br><span class="line">./tpcc_load -h127.0.0.1 -d tpcc100 -u root -p "" -w 100 </span><br><span class="line">mysql tpcc100 &lt;add_fkey_idx.sql</span><br></pre></td></tr></table></figure><p>Now start to test the database backup function.</p><p>MariaDB database backup includes logical backup and physical backup.</p><p>The advantage of logical backup is that the files that are backed up are SQL formatted text files, which can be used for other types of databases, and are also convenient for importing or data migration. The disadvantage is that the database performance is greatly affected during backup and during recovery. The tool MariaDB uses for logical backup is mysqldump.</p><p>Physical backup is actually a backup of the physical files of the database, so the impact on the database is relatively small, but the files backed up cannot be used in other types of databases， can only be used for backup and recovery. The tool that MariaDB uses for physical backup is mariabackup.</p><p>MariaDB’s logical backup and physical backup are both for the backup of the database at a certain point in time, and cannot achieve complete lossless recovery. If you want to achieve lossless recovery, you also need to use the replay function of the binlog . MariaDB implements this, the  tool is mysqlbinlog.</p><p>Let’s test these three tools one by one.</p><p><strong>Logical backup and recovery</strong></p><p>First test the logical backup.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqldump --all-databases --master-data=2 --single-transaction &gt; all_databases.sql</span><br></pre></td></tr></table></figure><p>The <code>–master-data=2</code> option in the above command means to record the binlog location during the backup in the backup file. If subsequent  lossless recovery is performed, you can provide the mysqlbinlog tool with the location to start the recovery. <code>--single-transaction</code> means that the backup is executed in one transaction, so that it does not affect the normal operation of other sessions and can also ensure the consistency of the backup data.</p><p>Our data directory occupy a total of 8.6G disk space, and it only took 3 minutes to complete the backup. The backup file is 7.2G, and the performance is still acceptable. Of course, this is also related to the disk used. We are using Huawei Cloud SSD, which should be faster.</p><p>The recovery of logical backup is also very simple, just execute the exported sql file directly. We are only testing here, so we will execute it directly on the original database.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root &lt; all_databases.sql</span><br></pre></td></tr></table></figure><p>The above 7.2G sql was executed for about 21 minutes, and the performance is not bad. If it is restored on a new database, it is estimated that it will be faster.</p><p><strong>Physical backup and recovery</strong></p><p>We continue to test the physical backup.</p><p>First create a directory for storing backup files:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /home/mariadb/data/backup</span><br></pre></td></tr></table></figure><p>Then there can be no <code>skip-grant-tables</code> option when backing up, we first delete this configuration item in the backup file, restart the database, and then execute to <code>mysql</code>enter the database and modify the root user password:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">user</span> <span class="string">'root'</span>@<span class="string">'localhost'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'backuptest'</span>;</span><br></pre></td></tr></table></figure><p>The mariabackup we compiled is in the extra directory of the compilation directory, and we enter that directory to execute:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd ~/build-mariadb-server/extra/mariabackup </span><br><span class="line">./mariabackup --backup --target-dir=/home/mariadb/data/backup -uroot -pbackuptest </span><br><span class="line">./mariabackup --prepare --target-dir=/home/mariadb/data/backup</span><br></pre></td></tr></table></figure><p>The entire backup is completed in about two minutes, and the speed is relatively fast.</p><p>Let’s continue to test the recovery:</p><p>Stop the database during recovery.</p><p>Then we back up the original data directory and create a new directory, because the data file directory must be empty when restoring</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /home/mariadb/data </span><br><span class="line">mv dir dirbak </span><br><span class="line">mkdir dir</span><br></pre></td></tr></table></figure><p>Then start to restore:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~&#x2F;build-mariadb-server&#x2F;extra&#x2F;mariabackup </span><br><span class="line">.&#x2F;mariabackup --copy-back --target-dir&#x3D;&#x2F;home&#x2F;mariadb&#x2F;data&#x2F;backup --datadir&#x3D;&#x2F;home&#x2F;mariadb&#x2F;data&#x2F;dir&#x2F;</span><br></pre></td></tr></table></figure><p>Recovery took only one and a half minutes</p><p> <strong>Lossless recovery</strong></p><p>Let me continue to test the lossless recovery.</p><p>Suppose the following scenario: After a database backup is made, some new tables are created, and some data is inserted. At this time, the database data is all deleted, but the backup file and binlog are still there. We try to use the backup file and binlog to completely restore the database.</p><p>Let’s use the mariabackup tool to back up the database first,  then execute the following SQL to create some tables and insert some data:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span>  <span class="keyword">database</span>  <span class="keyword">test</span> ; </span><br><span class="line"><span class="keyword">use</span>  <span class="keyword">test</span> ; </span><br><span class="line"><span class="keyword">create</span>  <span class="keyword">table</span> binlogtest( <span class="keyword">id</span>  <span class="built_in">int</span> , descs <span class="built_in">varchar</span>(<span class="number">10</span>)); </span><br><span class="line"><span class="keyword">insert</span>  <span class="keyword">into</span> binlogtest <span class="keyword">values</span> ( <span class="number">1</span> , <span class="string">'test1'</span> ); </span><br><span class="line"><span class="keyword">insert</span>  <span class="keyword">into</span> binlogtest <span class="keyword">values</span> ( <span class="number">2</span> , <span class="string">'test2'</span> ); </span><br><span class="line"><span class="keyword">commit</span> ;</span><br></pre></td></tr></table></figure><p>Then execute the following command, to simulate data files are cleared. We use mv here, mainly to preserve the binlog file. After all, in our test environment, binlog did not make multiple copies.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /home/mariadb/data </span><br><span class="line">mv dir dirbak2 </span><br><span class="line">mkdir dir</span><br></pre></td></tr></table></figure><p>Now that the data files are all cleared, then we use mariabackup to restore the files during the backup, first stop the database, and then execute the following command:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~/build-mariadb-server/extra/mariabackup </span><br><span class="line">./mariabackup --copy-back --target-dir=/home/mariadb/data/backup --datadir=/home/mariadb/data/dir/</span><br></pre></td></tr></table></figure><p>Then start the database, log in to the database,</p><p>Use the following sql to query:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> test.binlogtest;</span><br></pre></td></tr></table></figure><p>You will get an error message that the table does not exist.</p><p>Then we try to use binlog to restore.</p><p>First check the xtrabackup_binlog_info file of the backup file, and find the pos point to start the recovery:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat xtrabackup_binlog_info</span><br></pre></td></tr></table></figure><p>The result is as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mariadb-arm-perf-test-bin.000012 358 0-1-7541</span><br></pre></td></tr></table></figure><p>Means that the pos point to start recovery should be 358.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /home/mariadb/data/dirbak2 </span><br><span class="line">mysqlbinlog mariadb-arm-perf-test-bin.000001 --start-position=344| mysql -uroot -pbackuptest</span><br></pre></td></tr></table></figure><p>Then log in to the database again to query the previous binlogtest table, and you will find that the data has been restored.</p><p><strong>Summary</strong></p><p>After testing so far, we can find that MariaDB’s various backup and recovery functions work perfectly on the arm64 platform, and the performance is good.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Author: zhaorenhai&lt;/p&gt;
&lt;p&gt;This article plans to test the operation of MariaDB’s backup and recovery function on the arm64 platform, and provide a reference for those interested in deploying MariaDB on the arm64 platform.&lt;/p&gt;
&lt;p&gt;The test platform chooses the 8C16G Kunpeng virtual machine on Huawei Cloud, the OS is Ubuntu 18.04, and MariaDB we plan to choose the latest version on github for testing.&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Hybrid deployment of MariaDB cluster on x86 and arm64</title>
    <link href="https://kunpengcompute.github.io/2020/12/08/hybrid-deployment-of-mariadb-cluster-on-x86-and-arm64/"/>
    <id>https://kunpengcompute.github.io/2020/12/08/hybrid-deployment-of-mariadb-cluster-on-x86-and-arm64/</id>
    <published>2020-12-08T10:24:09.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>Author: zhaorenhai</p><p>This article attempts to deploy MariaDB clusters on x86 and arm64 platforms to see if the deployment can be successful.<br>The deployment environment is carried out on HUAWEI CLOUD. The OS is openEuler 20.03 version and MariaDB version is 10.3.9.<br>In the official MariaDB documentation, there are two types of high-availability environment deployment methods. The simpler one is replication, and then the Galera cluster.<br>Let’s try these two high-availability environments separately, whether they support mixed deployment on x86 and arm64 platforms.</p><a id="more"></a><p><strong>Replication</strong><br>As the name implies, replication is that one or more slave databases replicate data from a master database in real time, so as to ensure that the data of the two databases are the same. This article will only try one primary - one replica scenario.</p><p>The purpose of replication:<br>• You can distribute read requests to multiple databases, thereby dispersing the pressure on the primary database, the most commonly used scenario is the scenario of more reads and less writes.<br>• Data analysis scenarios. Analyzing the data in the database generally requires more complex SQL. If it is executed on the main database, it will cause a lot of pressure on the main database, because the primary database is generally designed as an OLTP database, which is not suitable for analytical needs. With replication, data analysis can be performed only on the replica database, reducing the pressure on the primary database.<br>• Backup scenes. The backup itself will also have a relatively large performance impact on the database. With replication, and backup on the replica database can avoid the impact on the primary database. For databases that are frequently updated and inserted, you can disconnect them from the main database when you back up, so that the backup is faster and you can restore it after the backup.<br>• Data distribution, which can be replicated to distribute data to different regions.</p><p>The technical principle of replication: first back up the primary database data, and then restore it on the replica database to ensure that the basic data of the two databases are consistent, and then configure the replica database to apply the binlog of the primary database in real time to ensure the two database data consistency.</p><p><strong>The deployment steps of Replication</strong><br>First, purchase two cloud servers in the same network area of ​​Huawei Cloud, one x86 and one arm64, ensure that the intranet can communicate with each other, and openEuler is selected for OS.</p><p>Then deploy the primary database on x86:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb-server</span><br></pre></td></tr></table></figure><p>Create data file directory</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir &#x2F;var&#x2F;db&#x2F;mariadbdatadir</span><br></pre></td></tr></table></figure><p>Configure the database configuration file:<br>Modify <code>/etc/my.cnf.d/mariadb-server.cnf</code>, add the following configuration under <code>[mariadb]</code> (specific parameters can be adjusted according to your own environment):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">datadir &#x3D; &#x2F;var&#x2F;db&#x2F;mariadbdatadir</span><br><span class="line">innodb_buffer_pool_size &#x3D; 6G</span><br><span class="line">innodb_log_file_size &#x3D; 1G</span><br><span class="line">innodb_flush_method &#x3D; O_DIRECT</span><br><span class="line">bind-address &#x3D; 0.0.0.0</span><br><span class="line">log-bin</span><br><span class="line">server_id &#x3D; 1</span><br><span class="line">log-basename &#x3D; primary1</span><br><span class="line">binlog-format &#x3D; mixed</span><br></pre></td></tr></table></figure><p>Start the database:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start mariadb.service</span><br></pre></td></tr></table></figure><p>Check whether the database started successfully</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl status mariadb.service</span><br></pre></td></tr></table></figure><p>Load test data:<br>For simplicity, we use tpcc-mysql tool to load some tpcc data for testing</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb-devel</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;Percona-Lab&#x2F;tpcc-mysql</span><br><span class="line">cd tpcc-mysql&#x2F;src</span><br><span class="line">make</span><br><span class="line">cd ..</span><br><span class="line">mysqladmin create tpcc10</span><br><span class="line">mysql tpcc10 &lt; create_table.sql</span><br><span class="line">.&#x2F;tpcc_load -h127.0.0.1 -d tpcc10  -u root -p &quot;&quot; -w 10</span><br></pre></td></tr></table></figure><p>Create a user:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root</span><br><span class="line">Create user &#39;replication_user&#39;@&#39;%&#39; identified by &#39;bigs3cret&#39;;</span><br><span class="line">grant replication slave on *.* to &#39;replication_user&#39;@&#39;%&#39;;</span><br></pre></td></tr></table></figure><p>Now a working primary database with some data is configured.</p><p>Next we configure a replica database on the arm machine.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb-server</span><br></pre></td></tr></table></figure><p>Create data file directory:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir &#x2F;var&#x2F;db&#x2F;mariadbdatadir</span><br></pre></td></tr></table></figure><p>Configure the database:<br>modify <code>/etc/my.cnf.d/mariadb-server.cnf</code>, add the following configuration under <code>[mariadb]</code> (specific parameters can be adjusted according to your own environment):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">datadir &#x3D; &#x2F;var&#x2F;db&#x2F;mariadbdatadir</span><br><span class="line">innodb_buffer_pool_size &#x3D; 6G</span><br><span class="line">innodb_log_file_size &#x3D; 1G</span><br><span class="line">innodb_flush_method &#x3D; O_DIRECT</span><br><span class="line">server_id &#x3D; 2</span><br></pre></td></tr></table></figure><p>Next, copy the basic data on the primary to replica.<br>Log in to the primary :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root</span><br></pre></td></tr></table></figure><p>Execute the following sql:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">flush tables with read lock;</span><br><span class="line">show master status;</span><br><span class="line">+---------------------+----------+--------------+------------------+</span><br><span class="line">| File                | Position | Binlog_Do_DB | Binlog_Ignore_DB |</span><br><span class="line">+---------------------+----------+--------------+------------------+</span><br><span class="line">| primary1-bin.000011 |      687 |              |                  |</span><br><span class="line">+---------------------+----------+--------------+------------------+</span><br></pre></td></tr></table></figure><p>Record the results of File and Position in the above output.<br>Note that this session cannot be interrupted until the data backup of the primary below is completed.<br>Create a backup directory on primary:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir &#x2F;var&#x2F;db&#x2F;mariadbbackup</span><br></pre></td></tr></table></figure><p>Perform backup:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mariabackup --backup --target-dir&#x3D;&#x2F;var&#x2F;db&#x2F;mariadbbackup&#x2F; --user&#x3D;root --password&#x3D;&quot;&quot;</span><br><span class="line">mariabackup --prepare --target-dir&#x3D;&#x2F;var&#x2F;db&#x2F;mariadbbackup&#x2F;</span><br></pre></td></tr></table></figure><p>Then you can now unlock the above session, and execute the following sql in the above session:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unlock tables;</span><br></pre></td></tr></table></figure><p>Copy the backed up data to the replica:<br>On primary:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;var&#x2F;db </span><br><span class="line">tar -czvf mariadbbackup.tar.gz mariadbbackup</span><br></pre></td></tr></table></figure><p>Operations on replica:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scp root@192.168.0.196: &#x2F;var&#x2F;db&#x2F;mariadbbackup.tar.gz &#x2F;var&#x2F;db&#x2F; </span><br><span class="line">cd &#x2F;var&#x2F;db&#x2F;</span><br><span class="line">tar -zxvf mariadbbackup.tar.gz </span><br><span class="line">mariabackup --copy-back --target-dir&#x3D;&#x2F;var&#x2F;db&#x2F;mariadbbackup&#x2F; --datadir&#x3D;&#x2F;var&#x2F;db&#x2F;mariadbdatadir&#x2F;</span><br><span class="line">cd mariadbdatadir </span><br><span class="line">chown -R mysql: mysql .&#x2F;* </span><br><span class="line">systemctl start mariadb.service</span><br></pre></td></tr></table></figure><p>Configure real-time replication:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;var&#x2F;db&#x2F;mariadbbackup</span><br><span class="line">cat xtrabackup_binlog_info</span><br></pre></td></tr></table></figure><p>The output is as follows, remember these values, and use them for subsequent operations on replica.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">primary1-bin.000011     687     0-1-437</span><br></pre></td></tr></table></figure><p>Operations on replica:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root</span><br><span class="line">set global gtid_slave_pos &#x3D; &#39;0-1-437&#39;;</span><br><span class="line">CHANGE MASTER TO</span><br><span class="line">  MASTER_HOST&#x3D;&#39;192.168.0.196&#39;,</span><br><span class="line">  MASTER_USER&#x3D;&#39;replication_user&#39;,</span><br><span class="line">  MASTER_PASSWORD&#x3D;&#39;bigs3cret&#39;,</span><br><span class="line">  MASTER_PORT&#x3D;3306,</span><br><span class="line">  MASTER_LOG_FILE&#x3D;&#39;primary1-bin.000011 &#39;,</span><br><span class="line">  MASTER_LOG_POS&#x3D;687,</span><br><span class="line">  MASTER_CONNECT_RETRY&#x3D;10,</span><br><span class="line">  MASTER_USE_GTID &#x3D; slave_pos;</span><br><span class="line">START SLAVE;</span><br></pre></td></tr></table></figure><p>Use the following command to view the replication status</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show slave status\G</span><br></pre></td></tr></table></figure><p>If the values of the following two items are both Yes, it is normal.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Slave_IO_Running: Yes</span><br><span class="line">Slave_SQL_Running: Yes</span><br></pre></td></tr></table></figure><p>On the primary, load another database and perform the following test:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd tpcc-mysql&#x2F;</span><br><span class="line">mysql tpcc-test &lt; create_table.sql</span><br><span class="line">.&#x2F;tpcc_load -h127.0.0.1 -d tpcc-test  -u root -p &quot;&quot; -w 10</span><br></pre></td></tr></table></figure><p>Query on replica:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root</span><br><span class="line">show databases;</span><br></pre></td></tr></table></figure><p>The output is as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| mysql              |</span><br><span class="line">| performance_schema |</span><br><span class="line">| tpcc-test          |</span><br><span class="line">| tpcc10             |</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure><p>You can see that the tpcc-test database is also generated normally, and<br>you can execute the following sql for further check:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">use database tpcc-test;</span><br><span class="line">show tables;</span><br><span class="line">select count(*) from customer;</span><br></pre></td></tr></table></figure><p>If everything is normal, it means that the data is synchronized normally.</p><p>After the above test, it can be basically proved that the replication can be mixed deployment on x86 and arm64 environments.</p><p><strong>Galera cluster</strong><br>Galera cluster is a multi-master, multi-active cluster, data is synchronized between multiple nodes, to ensure that the data of multiple database nodes are consistent, unlike Oracle RAC, Galera cluster does not share storage, each database use an independent storage.<br>This has many advantages. Any database node will not affect the business if it is hung up. For applications with more read requests, it can also share the pressure well. Of course, for write requests, due to the existence of synchronization requests, the performance will be a little worse than that of a stand-alone database.<br>The data synchronization between multiple nodes of the Galera cluster is synchronous, not asynchronous, so that the consistency of the database on multiple nodes can be ensured, and there is no data divergence between multiple nodes. For performance considerations, it is not real-time. The synchronization technology uses a kind of virtual synchronization. The principle of virtual synchronization technology is more complicated. You can refer to the following article:<br><a href="https://blog.csdn.net/wzy0623/article/details/102522268">https://blog.csdn.net/wzy0623/article/details/102522268</a></p><p><strong>Galera cluster deployment steps</strong><br>We plan to deploy a three-node x86 Galera cluster, and then deploy a GLB load balancer on the front end.</p><p>First, purchase four x86 and one arm cloud server instances on Huawei Cloud, specifications: 4vCPUs 16GB memory, CPU frequency 2.6GHz. The OS still uses the openEuler20.03 version.<br>Four x86 cloud servers, one is used to deploy GLB, and the other three are used to deploy MariaDB Galera cluster. An arm cloud server is used to deploy MariaDB nodes that will later join the cluster.</p><p>Perform the following operations on the three x86 cluster nodes:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb-server</span><br></pre></td></tr></table></figure><p>Create data file directory</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir &#x2F;var&#x2F;db&#x2F;mariadbdatadir</span><br></pre></td></tr></table></figure><p>Configure the database configuration file:<br>modify <code>/etc/my.cnf.d/mariadb-server.cnf</code>, add the following configuration under <code>[mariadb]</code> (specific parameters can be adjusted according to your own environment):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">datadir &#x3D; &#x2F;var&#x2F;db&#x2F;mariadbdatadir</span><br><span class="line">innodb_buffer_pool_size &#x3D; 6G</span><br><span class="line">innodb_log_file_size &#x3D; 1G</span><br><span class="line">innodb_flush_method &#x3D; O_DIRECT</span><br><span class="line">bind-address &#x3D; 0.0.0.0</span><br></pre></td></tr></table></figure><p>Then execute <code>systemctl start mariadb</code>, start the database</p><p>Each node then performs the following operations:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb-server-galera</span><br></pre></td></tr></table></figure><p>Modify the <code>/etc/my.cnf.d/galera.cnf</code>.<br><code>wsrep_cluster_address</code> item should be configured to the following values, where the value of each IP address is the intranet IP of the three cluster nodes:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsrep_cluster_address&#x3D;&quot;gcomm:&#x2F;&#x2F;192.168.0.159,192.168.0.78,192.168.0.23&quot;</span><br></pre></td></tr></table></figure><p>On the first node, stop the database:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop mariadb</span><br></pre></td></tr></table></figure><p>Execute the following command on the first node to initialize the cluster:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">galera_new_cluster</span><br></pre></td></tr></table></figure><p>Then restart the database on the other two nodes:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart mariadb</span><br></pre></td></tr></table></figure><p>You can log in to the database and use the following command to view the cluster status:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show status like &#39;wsrep%&#39;;</span><br></pre></td></tr></table></figure><p>Now that the cluster is successfully built. </p><p>We are going to build a GLB load balancer next.<br>GLB is a pure TCP layer load balancer, simple and easy to use.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">yum install gcc* libtool</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;codership&#x2F;glb</span><br><span class="line">cd glb&#x2F;</span><br><span class="line">.&#x2F;bootstrap.sh</span><br><span class="line">.&#x2F;configure</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line">cp files&#x2F;glbd.cfg &#x2F;etc&#x2F;default&#x2F;glbd</span><br></pre></td></tr></table></figure><p>Edit the <code>/etc/default/glbd</code><br>configuration as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">LISTEN_ADDR&#x3D;&quot;8010&quot;</span><br><span class="line">CONTROL_ADDR&#x3D;&quot;127.0.0.1:8011&quot;</span><br><span class="line">CONTROL_FIFO&#x3D;&quot;&#x2F;var&#x2F;run&#x2F;glbd.fifo&quot;</span><br><span class="line">THREADS&#x3D;&quot;16&quot;</span><br><span class="line">MAX_CONN&#x3D;256</span><br><span class="line">DEFAULT_TARGETS&#x3D;&quot;192.168.0.159:3306 192.168.0.78:3306 192.168.0.23:3306&quot;</span><br></pre></td></tr></table></figure><p>Then start the load balancer</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd files</span><br><span class="line">.&#x2F;glbd.sh</span><br></pre></td></tr></table></figure><p>Then log in to any database of the cluster node and execute the following sql to create a user</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create user &#39;tpcc&#39;@&#39;192.168.0.%&#39; identified by &#39;123456&#39;;</span><br><span class="line">grant all privileges on *.* to &#39;tpcc&#39;@&#39;192.168.0.%&#39; with grant option;</span><br></pre></td></tr></table></figure><p>Now start to load some test data.<br>On the node where the load balancer is located:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb</span><br><span class="line">yum install mariadb-devel</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;Percona-Lab&#x2F;tpcc-mysql</span><br><span class="line">cd tpcc-mysql&#x2F;src</span><br><span class="line">make</span><br><span class="line">cd ..</span><br></pre></td></tr></table></figure><p>Load test data</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mysqladmin -h 127.0.0.1 -P 8010 -u tpcc -p123456 create tpcc10</span><br><span class="line">mysql -h 127.0.0.1 -P 8010 -u tpcc -p123456 tpcc10 &lt; create_table.sql</span><br><span class="line">.&#x2F;tpcc_load -h127.0.0.1 -P8010 -d tpcc10  -u tpcc -p &quot;123456&quot; -w 10</span><br><span class="line">mysql -h 127.0.0.1 -P 8010 -u tpcc -p123456 tpcc10 &lt; add_fkey_idx.sql</span><br></pre></td></tr></table></figure><p>Next, stop one of the x86 cluster nodes and prepare to replace it with an arm64 node.<br>Log in to one of the cluster nodes:<br><code>systemctl stop mariadb</code><br>and then log in to the other two cluster nodes, change the value of <code>wsrep_cluster_address</code> in <code>/etc/my.cnf.d/galera.cnf</code> from the IP of the stopped node to the IP of the am64 node.</p><p>Perform the following operations on the arm64 node:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb-server</span><br></pre></td></tr></table></figure><p>Create data file directory</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir &#x2F;var&#x2F;db&#x2F;mariadbdatadir</span><br></pre></td></tr></table></figure><p>Configure the database configuration file:<br>Modify <code>/etc/my.cnf.d/mariadb-server.cnf</code>, add the following configuration under <code>[mariadb]</code> (specific parameters can be adjusted according to your own environment):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">datadir &#x3D; &#x2F;var&#x2F;db&#x2F;mariadbdatadir</span><br><span class="line">innodb_buffer_pool_size &#x3D; 6G</span><br><span class="line">innodb_log_file_size &#x3D; 1G</span><br><span class="line">innodb_flush_method &#x3D; O_DIRECT</span><br><span class="line">bind-address &#x3D; 0.0.0.0</span><br></pre></td></tr></table></figure><p>Then execute <code>systemctl start mariadb</code>, start the database</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb-server-galera</span><br></pre></td></tr></table></figure><p>Modify the <code>/etc/my.cnf.d/galera.cnf</code>,<code>wsrep_cluster_address</code> should be configured to the following values, which is consistent with the other two x86 nodes.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsrep_cluster_address&#x3D;&quot;gcomm:&#x2F;&#x2F;192.168.0.159,192.168.0.78,192.168.0.173&quot;</span><br></pre></td></tr></table></figure><p>Then execute <code>systemctl restart mariadb</code> and wait for the restart to succeed, which may be slower.<br>Then log in to the database and enter the following command to view the cluster status</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show status like &#39;wsrep%&#39;;</span><br></pre></td></tr></table></figure><p>Then log in to the database on the arm64 node to check whether the data is synchronized normally.<br>If everything is normal, we log in to the load balancer node, we can unload the previous data, then reload the data, and test the large-scale deletion and new data scenarios.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd tpcc-mysql</span><br><span class="line">mysqladmin -h 127.0.0.1 -P 8010 -u tpcc -p123456 drop tpcc10</span><br><span class="line">mysqladmin -h 127.0.0.1 -P 8010 -u tpcc -p123456 create tpcc10</span><br><span class="line">mysql -h 127.0.0.1 -P 8010 -u tpcc -p123456 tpcc10 &lt; create_table.sql</span><br><span class="line">.&#x2F;tpcc_load -h127.0.0.1 -P8010 -d tpcc10  -u tpcc -p &quot;123456&quot; -w 10</span><br><span class="line">mysql -h 127.0.0.1 -P 8010 -u tpcc -p123456 tpcc10 &lt; add_fkey_idx.sql</span><br></pre></td></tr></table></figure><p>Then log in to each database to query whether the data is consistent.<br>After the above test, it can be proved that the Galera cluster can also be mixed deployment on x86 and arm64.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Author: zhaorenhai&lt;/p&gt;
&lt;p&gt;This article attempts to deploy MariaDB clusters on x86 and arm64 platforms to see if the deployment can be successful.&lt;br&gt;The deployment environment is carried out on HUAWEI CLOUD. The OS is openEuler 20.03 version and MariaDB version is 10.3.9.&lt;br&gt;In the official MariaDB documentation, there are two types of high-availability environment deployment methods. The simpler one is replication, and then the Galera cluster.&lt;br&gt;Let’s try these two high-availability environments separately, whether they support mixed deployment on x86 and arm64 platforms.&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>自旋锁的原理与优化</title>
    <link href="https://kunpengcompute.github.io/2020/12/07/zi-xuan-suo-de-yuan-li-yu-you-hua/"/>
    <id>https://kunpengcompute.github.io/2020/12/07/zi-xuan-suo-de-yuan-li-yu-you-hua/</id>
    <published>2020-12-07T09:36:00.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者: <a href="https://github.com/wangxiyuan">wangxiyuan</a></p><p>本文介绍自旋锁的概念、实现以及优化</p><a id="more"></a><h1 id="什么是自旋锁"><a href="#什么是自旋锁" class="headerlink" title="什么是自旋锁"></a>什么是自旋锁</h1><p>自旋锁（spin lock）与互斥锁（mutex）类似，任时刻只有一个线程能够获得锁。当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。</p><p>在获取锁的过程中，线程一直处于活跃状态。因此与mutex不同，spinlock不会导致线程的状态切换(用户态-&gt;内核态)，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快。</p><p>由于自旋时不释放CPU，如果持有自旋锁的线程一直不释放自旋锁，那么等待该自旋锁的线程会一直浪费CPU时间。因此，自旋锁主要适用于被持有时间短，线程不希望在重新调度上花过多时间的情况。</p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p>根据自旋锁的原理，我们先尝试写出伪代码实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;循环检查锁状态，并尝试获取，直到成功</span><br><span class="line">while (true):</span><br><span class="line">  locked &#x3D; get_lock()</span><br><span class="line">  if locked &#x3D;&#x3D; false:</span><br><span class="line">    locked &#x3D; true</span><br><span class="line">    break</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;上锁后执行相关任务</span><br><span class="line">do_something</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;执行完毕，释放锁</span><br><span class="line">locked &#x3D; false</span><br></pre></td></tr></table></figure><p>细心的同学可以发现上面的逻辑在并发场景会遇到问题：两个线程可能会同时进入<code>if</code>语句，同时获取锁，导致锁不生效。</p><p>如何解决这个问题？我们可以把查询锁（get_lock）和设置锁（locked=true）组合成一个原子操作，保证同一时间只有一个线程在执行。如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;这里get_and_set(locked)就是一个原子操作，执行成功后把locked设置成true。</span><br><span class="line">while (get_and_set(locked) &#x3D;&#x3D; false):</span><br><span class="line">  continue</span><br><span class="line"></span><br><span class="line">do_something</span><br><span class="line"></span><br><span class="line">locked &#x3D; false</span><br></pre></td></tr></table></figure><p>如此，就实现了一个简单的自旋锁。</p><p>那么现在的问题是如何实现<code>get_and_set(locked)</code>这个原子操作？这里有两个常用的方法可以使用：<code>TAS（test and set）</code>和<code>CAS （compare and swap）</code>。</p><ol><li><p><code>TAS</code>：一个TAS指令包括两个子步骤，把给定的内存地址设置为1，然后返回之前的旧值。</p></li><li><p><code>CAS</code>：CAS指令需要三个参数，一个内存位置(V)、一个期望旧值(A)、一个新值(B)。过程如下：</p><p> a. 比较内存V的值是否与A相等？<br> b. 如果相等，则用新值B替换内存位置V的旧值<br> c. 如果不相等，不做任何操作。<br> d. 无论哪个情况，CAS都会把内存V原来的值返回。</p></li></ol><p>很多语言都提供了封装后的TAS和CAS调用方法。</p><ol><li>以C++ 11为例，<code>atomic</code>标准库提供了相关方法：<a href="https://en.cppreference.com/w/c/atomic/atomic_flag_test_and_set">std::atomic_flag::test_and_set</a>和<a href="http://www.cplusplus.com/reference/atomic/atomic/compare_exchange_strong/">std::atomic::compare_exchange_strong</a></li><li>GCC编译器也内置了相关方法：<code>__atomic_test_and_set</code>和<code>__atomic_compare_exchange_n</code>.</li><li>Java也提供了例如<code>java.util.concurrent.atomic.AtomicReference.compareAndSet</code>等方法。</li></ol><p>使用这些方法替换伪代码中的<code>get_and_set(locked)</code>，就能快速实现自旋锁。</p><h1 id="参考实现"><a href="#参考实现" class="headerlink" title="参考实现"></a>参考实现</h1><p>下面我们看看一些顶级开源项目中是如何实现自旋锁的。注意，以下每个软件的代码可能在很多地方都实现/使用了自旋锁，这里只选取了其中某一些加以分析。</p><h2 id="MariaDB-10-4"><a href="#MariaDB-10-4" class="headerlink" title="MariaDB 10.4"></a>MariaDB 10.4</h2><p>代码路径：<code>/storage/innobase/include/ib0mutex.h</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">struct TTASMutex &#123;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    &#x2F;** Try and lock the mutex.</span><br><span class="line">    @return true on success *&#x2F;</span><br><span class="line">    bool try_lock() UNIV_NOTHROW</span><br><span class="line">    &#123;</span><br><span class="line">        uint32_t oldval &#x3D; MUTEX_STATE_UNLOCKED;</span><br><span class="line">        return m_lock_word.compare_exchange_strong(</span><br><span class="line">            oldval,</span><br><span class="line">            MUTEX_STATE_LOCKED,</span><br><span class="line">            std::memory_order_acquire,</span><br><span class="line">            std::memory_order_relaxed);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;** Acquire the mutex.</span><br><span class="line">    @param max_spins max number of spins</span><br><span class="line">    @param max_delay max delay per spin *&#x2F;</span><br><span class="line">    void enter(uint32_t max_spins, uint32_t max_delay,</span><br><span class="line">            const char*, uint32_t) UNIV_NOTHROW</span><br><span class="line">    &#123;</span><br><span class="line">        const uint32_t step &#x3D; max_spins;</span><br><span class="line">        uint32_t n_spins &#x3D; 0;</span><br><span class="line"></span><br><span class="line">        while (!try_lock()) &#123;</span><br><span class="line">            ut_delay(max_delay);</span><br><span class="line">            if (++n_spins &#x3D;&#x3D; max_spins) &#123;</span><br><span class="line">                os_thread_yield();</span><br><span class="line">                max_spins+&#x3D; step;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        m_policy.add(n_spins, 0);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如上所示，在<code>TTASMutex</code>结构里，有个<code>enter</code>方法，里面实现了上锁+自旋的功能。其中上锁动作调用了<code>try_lock</code>方法，里面使用了CAS原子操作。</p><p>在我们之前写的简单伪代码中，while循环内什么都没做（直接continue），即每次自旋之间无停顿、无其他操作。而mariaDB这里却做了一些动作，在每次while循环中：</p><ol><li>执行<code>ut_delay</code>。即每次上锁失败后，会等待一段时间，然后再去尝试上锁。</li><li>判断自旋次数，当自旋次数达到某个阈值，不再自旋，直接线程挂起。</li></ol><p>这样做可以防止某些自旋锁无限空转、浪费CPU资源的情况。</p><h2 id="PostgreSQL"><a href="#PostgreSQL" class="headerlink" title="PostgreSQL"></a>PostgreSQL</h2><p>代码路径：<code>src/include/storage/s_lock.h</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line"> * s_lock(lock) - platform-independent portion of waiting for a spinlock.</span><br><span class="line"> *&#x2F;</span><br><span class="line">int</span><br><span class="line">s_lock(volatile slock_t *lock, const char *file, int line, const char *func)</span><br><span class="line">&#123;</span><br><span class="line">    SpinDelayStatus delayStatus;</span><br><span class="line"></span><br><span class="line">    init_spin_delay(&amp;delayStatus, file, line, func);</span><br><span class="line"></span><br><span class="line">    while (TAS_SPIN(lock))</span><br><span class="line">    &#123;</span><br><span class="line">        perform_spin_delay(&amp;delayStatus);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    finish_spin_delay(&amp;delayStatus);</span><br><span class="line"></span><br><span class="line">    return delayStatus.delays;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们可以看到，与MariaDB类似，while的判断中不断获取锁，while语句中加入delay。<code>TAS_SPIN</code>的实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">static __inline__ int</span><br><span class="line">tas(volatile slock_t *lock)</span><br><span class="line">&#123;</span><br><span class="line">    return __sync_lock_test_and_set(lock, 1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>__sync_lock_test_and_set</code>是gcc内置的老版本的TAS原子操作，推荐使用<code>__atomic_test_and_set</code></p><h2 id="Linux-kernel"><a href="#Linux-kernel" class="headerlink" title="Linux kernel"></a>Linux kernel</h2><p>Kernel的spin lock很复杂，有多种实现，以arm64为例，在4.16版本之前，使用的是汇编实现。4.16之后<a href="https://github.com/torvalds/linux/commit/c11090474d70590170cf5fa6afe85864ab494b37#diff-b986d1b5abd625b1eb28906791cef2146c371d10e4e8e5b23bb9f4158c2f16f1">使用</a>了通用的qspinlock，qspinlock较复杂，请参考<a href="https://blog.csdn.net/feisezaiyue/article/details/106171499">这里</a>和<a href="https://blog.csdn.net/yhb1047818384/article/details/84677203">这里</a>。以后另开文章分析。</p><h1 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h1><h2 id="原子操作优化"><a href="#原子操作优化" class="headerlink" title="原子操作优化"></a>原子操作优化</h2><p>首先我们先看看TAS和CAS在汇编层面是什么样的？这里需要特别说明一下硬件架构和编译工具的选择，在不同硬件架构上使用不同版本的编译器，得到的汇编指令是不同的。本文以ARM64为例，使用gcc 8.2编译器, 编译参入为<code>-O2 -std=c++11</code>，分别执行<code>__atomic_test_and_set</code>和<code>__atomic_compare_exchange_n</code>。</p><h3 id="atomic-test-and-set"><a href="#atomic-test-and-set" class="headerlink" title="__atomic_test_and_set"></a><code>__atomic_test_and_set</code></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;atomic&gt;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">   int val &#x3D; 456;</span><br><span class="line">   __atomic_test_and_set(&amp;val, 1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>汇编结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">main:</span><br><span class="line">        sub     sp, sp, #16</span><br><span class="line">        mov     w0, 456</span><br><span class="line">        add     x2, sp, 12</span><br><span class="line">        str     w0, [sp, 12]</span><br><span class="line">        mov     w0, 1</span><br><span class="line">.L2:</span><br><span class="line">        ldaxrb  w1, [x2]</span><br><span class="line">        stxrb   w3, w0, [x2]</span><br><span class="line">        cbnz    w3, .L2</span><br><span class="line">        mov     w0, 0</span><br><span class="line">        add     sp, sp, 16</span><br><span class="line">        ret</span><br></pre></td></tr></table></figure><h3 id="atomic-compare-exchange-n"><a href="#atomic-compare-exchange-n" class="headerlink" title="__atomic_compare_exchange_n"></a><code>__atomic_compare_exchange_n</code></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;atomic&gt;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">   int expected &#x3D; 123;</span><br><span class="line">   int val &#x3D; 456;</span><br><span class="line">   __atomic_compare_exchange_n(&amp;val, &amp;expected, 1 , false, __ATOMIC_ACQUIRE, __ATOMIC_ACQUIRE); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>汇编结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">main:</span><br><span class="line">        sub     sp, sp, #16</span><br><span class="line">        mov     w0, 456</span><br><span class="line">        add     x2, sp, 12</span><br><span class="line">        str     w0, [sp, 12]</span><br><span class="line">        mov     w0, 1</span><br><span class="line">.L2:</span><br><span class="line">        ldaxr   w1, [x2]</span><br><span class="line">        cmp     w1, 123</span><br><span class="line">        bne     .L3</span><br><span class="line">        stxr    w3, w0, [x2]</span><br><span class="line">        cbnz    w3, .L2</span><br><span class="line">.L3:</span><br><span class="line">        mov     w0, 0</span><br><span class="line">        add     sp, sp, 16</span><br><span class="line">        ret</span><br></pre></td></tr></table></figure><p>分析可以发现，TAS是直接写操作，CAS是先比较，满足条件后再写。而<code>写</code>是一个相对耗时的操作，因此在高并发、频繁使用锁的场景，CAS性能会更好。</p><p>因此在Postgre中，使用CAS代替TAS就是一个优化点了。详见<a href="https://www.postgresql.org/message-id/flat/CAB10pyamDkTFWU_BVGeEVmkc8%3DEhgCjr6QBk02SCdJtKpHkdFw%40mail.gmail.com">社区讨论</a>。</p><h2 id="锁等待优化"><a href="#锁等待优化" class="headerlink" title="锁等待优化"></a>锁等待优化</h2><p>在MariaDB和Postgre的源码中，我们可以看到在不断获取锁的过程中，都有delay的操作。适当的delay操作可以避免很多无意义的锁获取动作。那么当执行delay操作时，数据库到底在干什么？</p><p>我们先看MariaDB的<code>ut_delay</code>方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line">  Run a delay loop while waiting for a shared resource to be released.</span><br><span class="line">  @param delay originally, roughly microseconds on 100 MHz Intel Pentium</span><br><span class="line">*&#x2F;</span><br><span class="line">static inline void ut_delay(unsigned delay)</span><br><span class="line">&#123;</span><br><span class="line">  unsigned i&#x3D; my_cpu_relax_multiplier &#x2F; 4 * delay;</span><br><span class="line">  HMT_low();</span><br><span class="line">  while (i--)</span><br><span class="line">    MY_RELAX_CPU();</span><br><span class="line">  HMT_medium();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static inline void MY_RELAX_CPU(void)</span><br><span class="line">&#123;</span><br><span class="line">#ifdef _WIN32</span><br><span class="line">  &#x2F;*</span><br><span class="line">    In the Win32 API, the x86 PAUSE instruction is executed by calling</span><br><span class="line">    the YieldProcessor macro defined in WinNT.h. It is a CPU architecture-</span><br><span class="line">    independent way by using YieldProcessor.</span><br><span class="line">  *&#x2F;</span><br><span class="line">  YieldProcessor();</span><br><span class="line">#elif defined HAVE_PAUSE_INSTRUCTION</span><br><span class="line">  &#x2F;*</span><br><span class="line">    According to the gcc info page, asm volatile means that the</span><br><span class="line">    instruction has important side-effects and must not be removed.</span><br><span class="line">    Also asm volatile may trigger a memory barrier (spilling all registers</span><br><span class="line">    to memory).</span><br><span class="line">  *&#x2F;</span><br><span class="line">#ifdef __SUNPRO_CC</span><br><span class="line">  asm (&quot;pause&quot; );</span><br><span class="line">#else</span><br><span class="line">  __asm__ __volatile__ (&quot;pause&quot;);</span><br><span class="line">#endif</span><br><span class="line">#elif defined(_ARCH_PWR8)</span><br><span class="line">  __ppc_get_timebase();</span><br><span class="line">#elif defined __GNUC__ &amp;&amp; (defined __arm__ || defined __aarch64__)</span><br><span class="line">  &#x2F;* Mainly, prevent the compiler from optimizing away delay loops *&#x2F;</span><br><span class="line">  __asm__ __volatile__ (&quot;&quot;:::&quot;memory&quot;);</span><br><span class="line">#else</span><br><span class="line">  int32 var, oldval &#x3D; 0;</span><br><span class="line">  my_atomic_cas32_strong_explicit(&amp;var, &amp;oldval, 1, MY_MEMORY_ORDER_RELAXED,</span><br><span class="line">                                  MY_MEMORY_ORDER_RELAXED);</span><br><span class="line">#endif</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们可以看到<code>ut_delay</code>中调用了<code>MY_RELAX_CPU</code>方法， 而<code>MY_RELAX_CPU</code>方法中，根据不同架构和平台，执行了不同的命令。在X86架构中，调用了底层汇编指令<code>PAUSE</code>。在ARM64平台执行内存屏障(<code>__asm__ __volatile__ (&quot;&quot;:::&quot;memory&quot;);</code>)，防止编译优化，保持CPU空转（即不执行任何操作）。注意，ARM64的这个内存屏障代码是在这个<a href="https://github.com/MariaDB/server/commit/24f510bba4f0c3236aa5a8c96c209eb71317f4fe#diff-e877f572e9fd77c43d5e7af075f92e74">优化提交</a>中新增的。</p><p>在这个提交没有合入前，在ARM64平台上，默认执行的是<code>my_atomic_cas32_strong_explicit</code>方法，这是一个模拟无效的CAS的方法。为什么会有这样的修改？</p><p>最早的时候，社区开发者在48核的Qualcomm Centriq 2400 ARM服务上测试后，发现模拟CAS操作能提高ARM的性能。但随着代码不断迭代以及ARM服务器的不断发展。在MariaDB 10.4分支，开发者发现，在ARM上保持CPU空转的性能更高，有大概12%的性能提升。</p><p>所以我们很难在原理上讲清为什么CPU空转比模拟CAS性能高，但事实就是如此。通过这个优化，我也学到了两点，分享给大家：</p><ol><li>性能优化有时不是看代码就能发现的，更多的还是需要实际测试。</li><li>性能是实时变化的，以前的优化可能放到后来就是负担。性能优化需要持续不断的展开。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: &lt;a href=&quot;https://github.com/wangxiyuan&quot;&gt;wangxiyuan&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍自旋锁的概念、实现以及优化&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Test MariaDB&#39;s S3 storage engine on arm64 platform</title>
    <link href="https://kunpengcompute.github.io/2020/12/04/test-mariadb-s-s3-storage-engine-on-arm64-platform/"/>
    <id>https://kunpengcompute.github.io/2020/12/04/test-mariadb-s-s3-storage-engine-on-arm64-platform/</id>
    <published>2020-12-04T06:37:04.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>Author: zhaorenhai </p><p>MariaDB has many other storage engines besides the default InnoDB storage engine, which are useful in some ways. Starting with this article, we’ll look at the functionality and performance of these non-default storage engines on the arm64 platform.</p><p>The first storage engine we studied was S3. The S3 storage engine is a new feature introduced since version 10.5. The S3 storage engine actually stores data in cloud storage that supports the S3 protocol. Tables stored on them are read-only . You can transfer data to S3 by changing the storage engine for tables, add and delete columns to tables of the S3 storage engine, or index them, but you cannot add, update or delete records directly. Readers should be wondering what the S3 storage engine does. In fact, it is useful for scenarios where some data is no longer updated, but the amount of data is large, and the data is important and cannot be deleted. At this time, you can change the storage engine of these tables to S3, because the S3 storage engine is cheaper and more reliable than local storage and is a good choice.</p><p>Here we will simply look at the functionality and performance of the S3 storage engine on the arm64 platform from several aspects.</p><a id="more"></a><p>First let’s run through the S3 Storage Engine test cases to see if they pass on the arm64 platform (MariaDB currently does not have the S3 Storage Engine-related test cases as the default required test cases, as mentioned in this document:Https://mariadb.com/kb/en/s3-storage-engine/), and then look at the functionality and performance of the S3 storage engine on arm64.</p><p>The test platform uses the Kunpeng Virtual Machine of Huawei Cloud and the OS is Ubuntu 18.04. Our S3 Storage Service uses Huawei Cloud’s OBS service. Created access key and secret key, bucket in advance. (Note that the storage area selected in OBS is best in the same area as the one used by the Kunpeng  virtual machine for testing in order to reduce network latency. OBS-related guidance refers to the official guidance of Huawei Cloud, which is not detailed here.)</p><p>We created a new user on the OS <code>adduser mariadb</code>, and all subsequent work will be done under this user.</p><p>Download the source code for the latest version of MariaDB from GitHub and compile it</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">su - mariadb</span><br><span class="line">sudo apt-get install build-essential libncurses5-dev gnutls-dev libcurl4-gnutls-dev </span><br><span class="line">sudo apt-get install zlib1g-dev ccache libnuma-dev libxml2-dev cmake bison</span><br><span class="line">git clone https://github.com/mariadb/server</span><br><span class="line">cd server</span><br><span class="line">git submodule update --init</span><br><span class="line">cd ..</span><br><span class="line">mkdir build-mariadb-server</span><br><span class="line">cd build-mariadb-server</span><br><span class="line">cmake ../server -DCMAKE_BUILD_TYPE=RelWithDebInfo</span><br><span class="line">cmake --build .</span><br></pre></td></tr></table></figure><p><strong>Test Cases</strong></p><p>Edit the configuration file required for the test case:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi ~/server/mysql-test/suite/s3/my.cnf</span><br></pre></td></tr></table></figure><p>The configuration format is as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">!include include&#x2F;default_mysqld.cnf</span><br><span class="line">!include include&#x2F;default_client.cnf</span><br><span class="line"></span><br><span class="line">[mysqld.1]</span><br><span class="line">#Currently, the maturity of S3 storage engine in version 10.6 is gamma, which is not stable. </span><br><span class="line">#It has not been loaded by default and needs to be configured manually</span><br><span class="line">plugin-maturity &#x3D; gamma</span><br><span class="line">plugin-load-add&#x3D;@ENV.HA_S3_SO</span><br><span class="line">s3&#x3D;ON</span><br><span class="line">s3-host-name&#x3D;obs.cn-north-4.myhuaweicloud.com</span><br><span class="line">#The name of the bucket previously created on the OBS</span><br><span class="line">s3-bucket&#x3D;mariadb</span><br><span class="line">s3-access-key&#x3D;please replace with your access key</span><br><span class="line">s3-secret-key&#x3D;please replace with your secret key</span><br><span class="line">#OBS area</span><br><span class="line">s3-region&#x3D;cn-north-4</span><br></pre></td></tr></table></figure><p>Edit the following configuration file</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi ~/server/mysql-test/suite/s3/slave.cnf</span><br></pre></td></tr></table></figure><p>The configuration format is as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[mysqld.2]</span><br><span class="line">plugin-maturity &#x3D; gamma</span><br><span class="line">plugin-load-add&#x3D;@ENV.HA_S3_SO</span><br><span class="line">s3&#x3D;ON</span><br><span class="line">s3-slave-ignore-updates&#x3D;1</span><br><span class="line">s3-host-name&#x3D;obs.cn-north-4.myhuaweicloud.com</span><br><span class="line">s3-bucket&#x3D;mariadb</span><br><span class="line">s3-access-key&#x3D;please replace with your access key</span><br><span class="line">s3-secret-key&#x3D;please replace with your secret key</span><br><span class="line">s3-region&#x3D;cn-north-4</span><br></pre></td></tr></table></figure><p>Start running test cases</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~/build-mariadb-server/mysql-test</span><br><span class="line">./mysql-test-run --suite=s3</span><br></pre></td></tr></table></figure><p>The output is as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">Logging: &#x2F;home&#x2F;mariadb&#x2F;server&#x2F;mysql-test&#x2F;mysql-test-run.pl  --suite&#x3D;s3</span><br><span class="line">vardir: &#x2F;home&#x2F;mariadb&#x2F;build-mariadb-server&#x2F;mysql-test&#x2F;var</span><br><span class="line">Checking leftover processes...</span><br><span class="line">Removing old var directory...</span><br><span class="line">Creating var directory &#39;&#x2F;home&#x2F;mariadb&#x2F;build-mariadb-server&#x2F;mysql-test&#x2F;var&#39;...</span><br><span class="line">Checking supported features...</span><br><span class="line">MariaDB Version 10.6.0-MariaDB</span><br><span class="line"></span><br><span class="line"> - SSL connections supported</span><br><span class="line">   Using suites: s3</span><br><span class="line">   Collecting tests...</span><br><span class="line">   Installing system database...</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">TEST                                      RESULT   TIME (ms) or COMMENT</span><br><span class="line">--------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">worker[1] Using MTR_BUILD_THREAD 300, with reserved ports 16000..16019</span><br><span class="line">s3.partition_create_fail                 [ skipped ]  Requires debug build</span><br><span class="line">s3.encryption                            [ pass ]    777</span><br><span class="line">s3.partition_move &#39;innodb&#39;               [ pass ]   4059</span><br><span class="line">s3.alter &#39;innodb&#39;                        [ pass ]   5361</span><br><span class="line">s3.innodb &#39;innodb&#39;                       [ pass ]   1444</span><br><span class="line">s3.alter2                                [ pass ]   1702</span><br><span class="line">s3.partition                             [ pass ]  24858</span><br><span class="line">s3.no_s3                                 [ pass ]      7</span><br><span class="line">s3.arguments                             [ pass ]   1231</span><br><span class="line">s3.basic                                 [ pass ]    830</span><br><span class="line">s3.discovery                             [ pass ]   3288</span><br><span class="line">s3.amazon                                [ skipped ]  Not connected to AWS</span><br><span class="line">s3.backup                                [ pass ]    581</span><br><span class="line">s3.mysqldump                             [ pass ]   3156</span><br><span class="line">s3.select                                [ pass ]    600</span><br><span class="line">s3.unsupported                           [ pass ]    329</span><br><span class="line">s3.replication_delayed &#39;innodb,mix&#39;      [ pass ]   3444</span><br><span class="line">s3.replication_mixed &#39;mix&#39;               [ pass ]   8435</span><br><span class="line">s3.replication_partition &#39;innodb,mix&#39;    [ pass ]  20100</span><br><span class="line"></span><br><span class="line">s3.replication_stmt &#39;stmt&#39;               [ pass ]   8967</span><br><span class="line">--------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">The servers were restarted 10 times</span><br><span class="line">Spent 89.169 of 107 seconds executing testcases</span><br><span class="line"></span><br><span class="line">Completed: All 18 tests were successful.</span><br><span class="line"></span><br><span class="line">2 tests were skipped, 1 by the test itself.</span><br></pre></td></tr></table></figure><p>We can see that except one test case needs to be tested by debug version, and another needs to be connected to AWS (we use Huawei Cloud OBS) for testing, and these two are skipped, all the other test cases are successful. It shows that the basic function of S3 storage engine is no problem on arm64 platform.</p><p><strong>Performance</strong></p><p>Let’s take a look at  performance of S3 storage engine.</p><p>First, configure the MySQL client path. We use the latest MySQL client just compiled and add the following code to <code>~/.bashrc</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH&#x3D;~&#x2F;build-mariadb-server&#x2F;client:$PATH</span><br></pre></td></tr></table></figure><p>Then</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~&#x2F;.bashrc</span><br></pre></td></tr></table></figure><p>Edit configuration file:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi ~/mariadb.cnf</span><br></pre></td></tr></table></figure><p>Enter the content in the following format:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[mariadbd]</span><br><span class="line">datadir &#x3D; &#x2F;home&#x2F;mariadb&#x2F;data&#x2F;dir</span><br><span class="line">lc_messages_dir &#x3D; &#x2F;home&#x2F;mariadb&#x2F;server&#x2F;sql&#x2F;share</span><br><span class="line"></span><br><span class="line">innodb_buffer_pool_size &#x3D; 8G</span><br><span class="line">innodb_log_file_size &#x3D; 1G</span><br><span class="line">innodb_flush_method &#x3D; O_DIRECT</span><br><span class="line"></span><br><span class="line">plugin-maturity &#x3D; gamma</span><br><span class="line">plugin_dir &#x3D; &#x2F;home&#x2F;mariadb&#x2F;build-mariadb-server&#x2F;storage&#x2F;maria</span><br><span class="line">plugin-load-add&#x3D;ha_s3</span><br><span class="line">s3&#x3D;ON</span><br><span class="line">s3-host-name&#x3D;obs.cn-north-4.myhuaweicloud.com</span><br><span class="line">s3-bucket&#x3D;mariadb</span><br><span class="line">s3-access-key&#x3D;please replace with your access key </span><br><span class="line">s3-secret-key&#x3D;please replace with your secret key</span><br><span class="line">s3-region&#x3D;cn-north-4</span><br></pre></td></tr></table></figure><p>Then start the database:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;home&#x2F;mariadb&#x2F;data&#x2F;dir</span><br><span class="line">cd ~&#x2F;build-mariadb-server</span><br><span class="line">.&#x2F;scripts&#x2F;mysql_install_db --srcdir&#x3D;..&#x2F;server --defaults-file&#x3D;~&#x2F;mariadb.cnf</span><br><span class="line">sql&#x2F;mysqld --defaults-file&#x3D;~&#x2F;mariadb.cnf</span><br></pre></td></tr></table></figure><p>After the database is successfully started, we need to create some test data.</p><p>Start a new shell window, login to MariaDB user and login to database.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql --socket=/tmp/mysql.sock</span><br></pre></td></tr></table></figure><p>Create a test table by executing the following SQL.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test.s3test(<span class="keyword">id</span> <span class="built_in">int</span>, description <span class="built_in">varchar</span>(<span class="number">40</span>));</span><br></pre></td></tr></table></figure><p>Then exit to the shell environment, ready to import a large amount of data into the table.</p><p>Using shell script to generate a 2.6G CSV file, about 60 million records, the first column is a number, the second column is a randomly generated string,  this file named <code>s3test.csv</code>.</p><p>Import it to the database with the following command:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqlimport --socket=/tmp/mysql.sock --fields-terminated-by=, test /home/mariadb/s3test.csv</span><br></pre></td></tr></table></figure><p>After importing it to the database, check the database file <code>/home/mariadb/data/dir/test/s3test.ibd</code>. The database file size is 4.4G.</p><p>Next, let’s test how long it takes to store this  table to the S3 storage engine.</p><p>Login to the database and execute the following SQL:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> test.s3test <span class="keyword">engine</span>=s3;</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Query OK, 63649280 rows affected (2 min 49.886 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>You can see that it took only two minutes and 49 seconds.</p><p>And the local data file  <code>/home/mariadb/data/dir/test/s3test.ibd</code> no longer exists, indicating that the table has been saved to cloud storage.</p><p>Then let’s take a look at the files on the OBS.</p><p>Download an obsutil tool:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget https://obs-community.obs.cn-north-1.myhuaweicloud.com/obsutil/current/obsutil_linux_arm64.tar.gz</span><br><span class="line">tar -zxvf obsutil_linux_arm64.tar.gz</span><br><span class="line">cd obsutil_linux_arm64_5.2.10</span><br><span class="line"><span class="meta">#</span><span class="bash">Configure obsutil tool</span></span><br><span class="line">./obsutil config -i=accesskey -k=secrectkey -e=obs.cn-north-4.myhuaweicloud.com</span><br></pre></td></tr></table></figure><p>Check the data on OBS:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./obsutil ls obs://mariadb/test/s3test/</span><br></pre></td></tr></table></figure><p>Query the data size on OBS, which is 3.22GB,  even a little smaller than InnoDB storage:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Total size of prefix [test&#x2F;s3test&#x2F;] is: 3.22GB</span><br><span class="line">Folder number is: 0</span><br><span class="line">File number is: 826</span><br></pre></td></tr></table></figure><p>Next, let’s query the data to see how fast it is. Log in to MariaDB database and enter the following SQL:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> test.s3test <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">8</span>;</span><br></pre></td></tr></table></figure><p> the time is as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">512 rows in set (1 min 17.121 sec)</span><br></pre></td></tr></table></figure><p>It took about 1 minute and 17 seconds to query 512 records out of 60 million non indexed records, and the speed was acceptable.</p><p>Next, let’s look at the speed of indexing and the speed of queries after indexing.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; alter table test.s3test add index idx_id(id);</span><br><span class="line">Query OK, 63649280 rows affected (7 min 12.253 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>You can see that the creation of index took seven minutes.</p><p>Query again:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; pager md5sum; select * from test.s3test where id &#x3D; 8;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (25.146 sec)</span><br></pre></td></tr></table></figure><p>You can see it’s a little bit faster this time, 25 seconds.</p><p>Take a look at the size of index</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./obsutil ls obs://mariadb/test/s3test/</span><br></pre></td></tr></table></figure><p>The output size is 3.81GB, which is 600M more than that of no index, indicating that the index takes up about 600M.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Total size of prefix [test&#x2F;s3test&#x2F;] is: 3.81GB</span><br><span class="line">Folder number is: 0</span><br><span class="line">File number is: 979</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./obsutil ls obs://mariadb/test/s3test/index</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Total size of prefix [test&#x2F;s3test&#x2F;index] is: 609.02MB</span><br><span class="line">Folder number is: 0</span><br><span class="line">File number is: 153</span><br></pre></td></tr></table></figure><p>Let’s delete the index and see how long it takes.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; alter table test.s3test drop index idx_id;</span><br><span class="line">Query OK, 63649280 rows affected (4 min 18.969 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>It took four minutes to delete the index.</p><p>Change the table storage engine to InnoDB to see how long it takes.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; alter table test.s3test engine&#x3D;innodb;</span><br><span class="line">Query OK, 63649280 rows affected (6 min 6.302 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>It took about six minutes.</p><p><strong>Compare with InnoDB</strong></p><p>Now the table is in the local database, and the storage engine is InnoDB. Let’s repeat the above operation and compare the performance of InnoDB engine and S3 engine.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id &#x3D; 8;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (30.218 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; alter table test.s3test add index idx_id(id);</span><br><span class="line">Query OK, 0 rows affected (2 min 1.309 sec)</span><br><span class="line">Records: 0  Duplicates: 0  Warnings: 0</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id &#x3D; 8;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (0.009 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; alter table test.s3test drop index idx_id;</span><br><span class="line">Query OK, 0 rows affected (0.009 sec)</span><br><span class="line">Records: 0  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>It can be seen that the local InnoDB engine is still much faster than S3. However, except for index query and index deletion, there is no order of magnitude difference of no index query and create index operation, and the speed of S3 is acceptable.</p><p><strong>Performance after increasing S3 page buffer</strong></p><p>Let’s increase the page buffer size of S3 storage engine to see if the performance is improved.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi ~&#x2F;mariadb.cnf</span><br></pre></td></tr></table></figure><p>Add the following configuration to set the page buffer size of S3 to 5G (the default is 128M)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s3_pagecache_buffer_size&#x3D;5368709120</span><br></pre></td></tr></table></figure><p>Restart the database.</p><p>Now let’s repeat the previous operation:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; alter table test.s3test engine&#x3D;s3;</span><br><span class="line">Query OK, 63649280 rows affected (5 min 8.340 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id &#x3D; 8;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (39.110 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id &#x3D; 8;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (10.019 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; alter table test.s3test add index idx_id(id);</span><br><span class="line">Query OK, 63649280 rows affected (5 min 45.067 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id &#x3D; 8;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (17.479 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id &#x3D; 8;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (0.015 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; alter table test.s3test drop index idx_id;</span><br><span class="line">Query OK, 63649280 rows affected (4 min 46.552 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; alter table test.s3test engine&#x3D;innodb;</span><br><span class="line">Query OK, 63649280 rows affected (6 min 28.248 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>It can be seen that the operations such as changing the engine and creating and deleting indexes have not improved substantially. However, the speed of querying data, especially the second query, has an order of magnitude improvement, which is mainly due to the large buffer’s caching of data.</p><p><strong>Performance with compression enabled</strong></p><p>Next, let’s test the compression parameter <code>COMPRESSION_ALGORITHM=zlib</code>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; alter table test.s3test engine&#x3D;s3  COMPRESSION_ALGORITHM&#x3D;zlib;</span><br><span class="line">Query OK, 63649280 rows affected (6 min 21.659 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>It took more than 6 minutes, more than twice as long as it was not compressed. </p><p>Let’s see how much space can be saved.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./obsutil ls obs://mariadb/test/s3test/</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Total size of prefix [test&#x2F;s3test&#x2F;] is: 1.51GB</span><br><span class="line">Folder number is: 0</span><br><span class="line">File number is: 826</span><br></pre></td></tr></table></figure><p>You can see that more than half of the space is saved. However, our data is random, if it is regular data in reality, it is estimated that it can have higher compression rate. In addition, we use the default size of 4MB block, if the size of the block is changed to a larger point, the estimated compression rate should also be improved.</p><p>Let’s look at the query speed:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id = 8;</span><br><span class="line">PAGER <span class="keyword">set</span> <span class="keyword">to</span> <span class="string">'md5sum'</span></span><br><span class="line"><span class="number">46</span>ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line"><span class="number">512</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">56.610</span> sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(<span class="keyword">none</span>)]&gt; pager md5sum;<span class="keyword">select</span> * <span class="keyword">from</span> test.s3test <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">8</span>;</span><br><span class="line">PAGER <span class="keyword">set</span> <span class="keyword">to</span> <span class="string">'md5sum'</span></span><br><span class="line"><span class="number">46</span>ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line"><span class="number">512</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">10.110</span> sec)</span><br></pre></td></tr></table></figure><p>We can see that the first query is less than twice as slow as that without compression, and it is still acceptable. For the second query, due to the existence of large cache, the speed of compression and non compression is the same.</p><p>Let’s look at the indexing situation:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; alter table test.s3test add index idx_id(id);</span><br><span class="line">Query OK, 63649280 rows affected (9 min 26.030 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>It took nine minutes, and it was less than twice as slow.</p><p>Look at the space occupied by the index:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./obsutil ls obs://mariadb/test/s3test/index</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Total size of prefix [test&#x2F;s3test&#x2F;index] is: 261.54MB</span><br><span class="line">Folder number is: 0</span><br><span class="line">File number is: 153</span><br></pre></td></tr></table></figure><p>You can see that after compression is enabled, the index is much smaller, which is  smaller than half of that without compression.</p><p>Let’s look at the query:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id &#x3D; 8;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (27.094 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id &#x3D; 8;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (0.009 sec)</span><br></pre></td></tr></table></figure><p>The first query is less than twice as slow as that without compression, and the second query is the same as that without compression due to the existence of large cache.</p><p>Finally, take a look at the time taken to delete the index and change the engine back to InnoDB:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; alter table test.s3test drop index idx_id;</span><br><span class="line">Query OK, 63649280 rows affected (6 min 39.382 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; alter table test.s3test engine&#x3D;innodb;</span><br><span class="line">Query OK, 63649280 rows affected (6 min 36.595 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>Deleting indexes is less than twice as slow, and changing the engine takes as much time as without compression.</p><p><strong>Summary</strong></p><p>Through our test, we can find that S3 storage engine works well on arm64, and its performance is also good.</p><p>Let us summarize the results of the above tests in a table:</p><table><thead><tr><th>/</th><th>InnoDB</th><th>S3 with default options</th><th>S3 after setting pagecache buffer size  to 5G</th><th>S3 after compression enabled and page buffer size 5G</th></tr></thead><tbody><tr><td>Altering  table to S3</td><td>/</td><td>2 min 49.886 sec</td><td>5 min 8.340 sec</td><td>6 min 21.659 sec</td></tr><tr><td>Query without index</td><td>30.218 sec</td><td>1 min 17.121 sec</td><td>39.110 sec</td><td>56.610 sec</td></tr><tr><td>Query without index 2nd time</td><td>28.628 sec</td><td>51.305 sec</td><td>10.019 sec</td><td>10.110 sec</td></tr><tr><td>Creating index</td><td>2 min 1.309 sec</td><td>7 min 12.253 sec</td><td>5 min 45.067 sec</td><td>9 min 26.030 sec</td></tr><tr><td>Query with index</td><td>0.009 sec</td><td>25.146 sec</td><td>17.479 sec</td><td>27.094 sec</td></tr><tr><td>Query with index 2nd time</td><td>0.002 sec</td><td>29.264 sec</td><td>0.015 sec</td><td>0.009 sec</td></tr><tr><td>Dropping index</td><td>0.009 sec</td><td>4 min 18.969 sec</td><td>4 min 46.552 sec</td><td>6 min 39.382 sec</td></tr><tr><td>Altering  table to InnoDB</td><td>/</td><td>6 min 6.302 sec</td><td>6 min 28.248 sec</td><td>6 min 36.595 sec</td></tr><tr><td>Table size</td><td>4.4GB</td><td>3.22GB</td><td>3.22GB</td><td>1.51GB</td></tr><tr><td>Index size</td><td>1.1GB</td><td>609.02MB</td><td>609.02MB</td><td>261.54MB</td></tr></tbody></table><p><strong>Reference link</strong></p><p><a href="https://mariadb.com/kb/en/s3-storage-engine/">https://mariadb.com/kb/en/s3-storage-engine/</a></p><p><a href="https://mariadb.com/kb/en/plugin-overview/#installing-a-plugin">https://mariadb.com/kb/en/plugin-overview/#installing-a-plugin</a></p><p><a href="https://www.percona.com/blog/2020/07/17/mariadb-s3-engine-implementation-and-benchmarking/">https://www.percona.com/blog/2020/07/17/mariadb-s3-engine-implementation-and-benchmarking/</a></p><p><a href="https://support.huaweicloud.com/en-us/obs_gls/index.html">https://support.huaweicloud.com/en-us/obs_gls/index.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Author: zhaorenhai &lt;/p&gt;
&lt;p&gt;MariaDB has many other storage engines besides the default InnoDB storage engine, which are useful in some ways. Starting with this article, we’ll look at the functionality and performance of these non-default storage engines on the arm64 platform.&lt;/p&gt;
&lt;p&gt;The first storage engine we studied was S3. The S3 storage engine is a new feature introduced since version 10.5. The S3 storage engine actually stores data in cloud storage that supports the S3 protocol. Tables stored on them are read-only . You can transfer data to S3 by changing the storage engine for tables, add and delete columns to tables of the S3 storage engine, or index them, but you cannot add, update or delete records directly. Readers should be wondering what the S3 storage engine does. In fact, it is useful for scenarios where some data is no longer updated, but the amount of data is large, and the data is important and cannot be deleted. At this time, you can change the storage engine of these tables to S3, because the S3 storage engine is cheaper and more reliable than local storage and is a good choice.&lt;/p&gt;
&lt;p&gt;Here we will simply look at the functionality and performance of the S3 storage engine on the arm64 platform from several aspects.&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>MariaDB的S3存储引擎在arm64平台上的表现</title>
    <link href="https://kunpengcompute.github.io/2020/12/04/mariadb-de-s3-cun-chu-yin-qing-zai-arm64-ping-tai-shang-de-biao-xian/"/>
    <id>https://kunpengcompute.github.io/2020/12/04/mariadb-de-s3-cun-chu-yin-qing-zai-arm64-ping-tai-shang-de-biao-xian/</id>
    <published>2020-12-04T06:27:07.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者: zhaorenhai</p><p>MariaDB除了默认的InnoDB存储引擎以外，还有很多其他的存储引擎，这些引擎在某些方面也是比较有用的。从这篇文章开始，我们开始关注一下这些非默认的存储引擎在arm64平台上的表现。</p><p>我们第一个研究的存储引擎是S3。 S3存储引擎是在10.5版本才引进的一个新功能。S3存储引擎其实就把数据存储在支持S3协议的云存储上。当前存储在上面的表只能读，不能写。可以通过改变表的存储引擎的方式，把数据传到S3上面去，也可以对S3存储引擎的表进行增加列和删除列，也可以建立索引，但是就是不能直接对表增删改（后续版本可能会有这些功能）。看到这里读者应该会有疑问，S3存储引擎有啥用？其实在对一些不再更新的数据，但是数据量又比较大，而且数据比较重要，又不能删除的场景比较有用，这时候就可以把这些表的存储引擎改为S3，因为S3存储引擎相对于本地存储比较便宜，而且可靠，是一个很好的选择。</p><p>下面我们就简单的从几个方面看一下S3存储引擎在arm64平台上的表现</p><a id="more"></a><p>首先我们先跑一下S3存储引擎的测试用例，看看在arm64平台上是否都能通过（MariaDB当前还没有把S3存储引擎相关的测试用例作为默认必跑的测试用例，这个文档里有提到：<a href="https://mariadb.com/kb/en/s3-storage-engine/">https://mariadb.com/kb/en/s3-storage-engine/</a>  )，然后会看看S3存储引擎在arm64上功能和性能方面的情况。</p><p>测试平台用的是华为云的鲲鹏虚拟机，OS是Ubuntu18.04。我们S3存储服务用的是华为云的OBS服务，提前创建好访问key和密钥，桶，后续测试时需要配置（注意OBS选择的存储区域最好和测试用的鲲鹏虚拟机在同一个区域，以减少网络延迟。OBS相关指导请参考华为云官方指导，这里不再详述）。</p><p>我们在OS上新建一个用户<code>adduser mariadb</code>，后续所有的工作都在这个用户下进行。</p><p>从github上下载一下MariaDB最新版本的源码，并编译</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">su - mariadb</span><br><span class="line">sudo apt-get install build-essential libncurses5-dev gnutls-dev libcurl4-gnutls-dev </span><br><span class="line">sudo apt-get install zlib1g-dev ccache libnuma-dev libxml2-dev cmake bison</span><br><span class="line">git clone https://github.com/mariadb/server</span><br><span class="line">cd server</span><br><span class="line">git submodule update --init</span><br><span class="line">cd ..</span><br><span class="line">mkdir build-mariadb-server</span><br><span class="line">cd build-mariadb-server</span><br><span class="line"><span class="meta">#</span><span class="bash">加入RelWithDebInfo选项，是为了后续如果需要进行对性能分析的话，可以用perf工具看到代码</span></span><br><span class="line">cmake ../server -DCMAKE_BUILD_TYPE=RelWithDebInfo</span><br><span class="line">cmake --build .</span><br></pre></td></tr></table></figure><p><strong>测试用例</strong></p><p>编辑测试用例需要的配置文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi ~/server/mysql-test/suite/s3/my.cnf</span><br></pre></td></tr></table></figure><p>最终配置格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">!include include&#x2F;default_mysqld.cnf</span><br><span class="line">!include include&#x2F;default_client.cnf</span><br><span class="line"></span><br><span class="line">[mysqld.1]</span><br><span class="line">#当前S3存储引擎在10.6版本的成熟度是gamma，还不是stable，没有默认被加载，需要手动配置一下</span><br><span class="line">plugin-maturity &#x3D; gamma</span><br><span class="line">plugin-load-add&#x3D;@ENV.HA_S3_SO</span><br><span class="line">s3&#x3D;ON</span><br><span class="line">s3-host-name&#x3D;obs.cn-north-4.myhuaweicloud.com</span><br><span class="line">#之前在OBS上创建的桶的名字</span><br><span class="line">s3-bucket&#x3D;mariadb</span><br><span class="line">s3-access-key&#x3D;please replace with your access key</span><br><span class="line">s3-secret-key&#x3D;please replace with your secret key</span><br><span class="line">#OBS区域</span><br><span class="line">s3-region&#x3D;cn-north-4</span><br></pre></td></tr></table></figure><p>编辑如下配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi ~/server/mysql-test/suite/s3/slave.cnf</span><br></pre></td></tr></table></figure><p>最终配置格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[mysqld.2]</span><br><span class="line">plugin-maturity &#x3D; gamma</span><br><span class="line">plugin-load-add&#x3D;@ENV.HA_S3_SO</span><br><span class="line">s3&#x3D;ON</span><br><span class="line">s3-slave-ignore-updates&#x3D;1</span><br><span class="line">s3-host-name&#x3D;obs.cn-north-4.myhuaweicloud.com</span><br><span class="line">s3-bucket&#x3D;mariadb</span><br><span class="line">s3-access-key&#x3D;please replace with your access key</span><br><span class="line">s3-secret-key&#x3D;please replace with your secret key</span><br><span class="line">s3-region&#x3D;cn-north-4</span><br></pre></td></tr></table></figure><p>开始跑测试用例</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~/build-mariadb-server/mysql-test</span><br><span class="line">./mysql-test-run --suite=s3</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">Logging: &#x2F;home&#x2F;mariadb&#x2F;server&#x2F;mysql-test&#x2F;mysql-test-run.pl  --suite&#x3D;s3</span><br><span class="line">vardir: &#x2F;home&#x2F;mariadb&#x2F;build-mariadb-server&#x2F;mysql-test&#x2F;var</span><br><span class="line">Checking leftover processes...</span><br><span class="line">Removing old var directory...</span><br><span class="line">Creating var directory &#39;&#x2F;home&#x2F;mariadb&#x2F;build-mariadb-server&#x2F;mysql-test&#x2F;var&#39;...</span><br><span class="line">Checking supported features...</span><br><span class="line">MariaDB Version 10.6.0-MariaDB</span><br><span class="line"></span><br><span class="line"> - SSL connections supported</span><br><span class="line">   Using suites: s3</span><br><span class="line">   Collecting tests...</span><br><span class="line">   Installing system database...</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">TEST                                      RESULT   TIME (ms) or COMMENT</span><br><span class="line">--------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">worker[1] Using MTR_BUILD_THREAD 300, with reserved ports 16000..16019</span><br><span class="line">s3.partition_create_fail                 [ skipped ]  Requires debug build</span><br><span class="line">s3.encryption                            [ pass ]    777</span><br><span class="line">s3.partition_move &#39;innodb&#39;               [ pass ]   4059</span><br><span class="line">s3.alter &#39;innodb&#39;                        [ pass ]   5361</span><br><span class="line">s3.innodb &#39;innodb&#39;                       [ pass ]   1444</span><br><span class="line">s3.alter2                                [ pass ]   1702</span><br><span class="line">s3.partition                             [ pass ]  24858</span><br><span class="line">s3.no_s3                                 [ pass ]      7</span><br><span class="line">s3.arguments                             [ pass ]   1231</span><br><span class="line">s3.basic                                 [ pass ]    830</span><br><span class="line">s3.discovery                             [ pass ]   3288</span><br><span class="line">s3.amazon                                [ skipped ]  Not connected to AWS</span><br><span class="line">s3.backup                                [ pass ]    581</span><br><span class="line">s3.mysqldump                             [ pass ]   3156</span><br><span class="line">s3.select                                [ pass ]    600</span><br><span class="line">s3.unsupported                           [ pass ]    329</span><br><span class="line">s3.replication_delayed &#39;innodb,mix&#39;      [ pass ]   3444</span><br><span class="line">s3.replication_mixed &#39;mix&#39;               [ pass ]   8435</span><br><span class="line">s3.replication_partition &#39;innodb,mix&#39;    [ pass ]  20100</span><br><span class="line"></span><br><span class="line">s3.replication_stmt &#39;stmt&#39;               [ pass ]   8967</span><br><span class="line">--------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">The servers were restarted 10 times</span><br><span class="line">Spent 89.169 of 107 seconds executing testcases</span><br><span class="line"></span><br><span class="line">Completed: All 18 tests were successful.</span><br><span class="line"></span><br><span class="line">2 tests were skipped, 1 by the test itself.</span><br></pre></td></tr></table></figure><p>我们可以看到，除了一个测试用例需要debug版本来测，另外一个需要连接AWS（我们用的是华为云）来测，被skip了以外，其余测试用例全部都是成功的。说明在arm64平台上，S3存储引擎的基本功能是没问题的。</p><p><strong>性能</strong></p><p>接下来我们看看S3性能方面的情况。</p><p>首先配置一下mysql客户端路径，我们使用刚刚编译出来的最新mysql客户端，将如下代码加入<code>~/.bashrc</code>里</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH&#x3D;~&#x2F;build-mariadb-server&#x2F;client:$PATH</span><br></pre></td></tr></table></figure><p>然后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~&#x2F;.bashrc</span><br></pre></td></tr></table></figure><p>编辑配置文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi ~/mariadb.cnf</span><br></pre></td></tr></table></figure><p>按如下格式输入内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[mariadbd]</span><br><span class="line">#数据文件目录</span><br><span class="line">datadir &#x3D; &#x2F;home&#x2F;mariadb&#x2F;data&#x2F;dir</span><br><span class="line">#path to source dir + sql&#x2F;share</span><br><span class="line">lc_messages_dir &#x3D; &#x2F;home&#x2F;mariadb&#x2F;server&#x2F;sql&#x2F;share</span><br><span class="line"></span><br><span class="line">innodb_buffer_pool_size &#x3D; 8G</span><br><span class="line">innodb_log_file_size &#x3D; 1G</span><br><span class="line">innodb_flush_method &#x3D; O_DIRECT</span><br><span class="line"></span><br><span class="line">plugin-maturity &#x3D; gamma</span><br><span class="line">plugin_dir &#x3D; &#x2F;home&#x2F;mariadb&#x2F;build-mariadb-server&#x2F;storage&#x2F;maria</span><br><span class="line">plugin-load-add&#x3D;ha_s3</span><br><span class="line">s3&#x3D;ON</span><br><span class="line">s3-host-name&#x3D;obs.cn-north-4.myhuaweicloud.com</span><br><span class="line">s3-bucket&#x3D;mariadb</span><br><span class="line">s3-access-key&#x3D;please replace with your access key </span><br><span class="line">s3-secret-key&#x3D;please replace with your secret key</span><br><span class="line">s3-region&#x3D;cn-north-4</span><br></pre></td></tr></table></figure><p>然后启动数据库：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;home&#x2F;mariadb&#x2F;data&#x2F;dir</span><br><span class="line">cd ~&#x2F;build-mariadb-server</span><br><span class="line">.&#x2F;scripts&#x2F;mysql_install_db --srcdir&#x3D;..&#x2F;server --defaults-file&#x3D;~&#x2F;mariadb.cnf</span><br><span class="line">sql&#x2F;mysqld --defaults-file&#x3D;~&#x2F;mariadb.cnf</span><br></pre></td></tr></table></figure><p>数据库启动成功以后，我们创建一些测试数据</p><p>新启动一个shell窗口，登陆到mariadb用户，登陆数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql --socket=/tmp/mysql.sock</span><br></pre></td></tr></table></figure><p>执行如下sql创建一个测试表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test.s3test(<span class="keyword">id</span> <span class="built_in">int</span>, description <span class="built_in">varchar</span>(<span class="number">40</span>));</span><br></pre></td></tr></table></figure><p>然后退出到shell环境，准备给这个表导入大量的数据。</p><p>用shell脚本生成了一个大小为2.6G的csv文件，大概6000多万条记录，里面第一列为数字，第二列为随机生成的字符串,命名为<code>s3test.csv</code></p><p>用如下命令导入数据库：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqlimport --socket=/tmp/mysql.sock --fields-terminated-by=, test /home/mariadb/s3test.csv</span><br></pre></td></tr></table></figure><p>导入数据库以后，查看数据库文件 <code>/home/mariadb/data/dir/test/s3test.ibd</code>，数据库文件的大小为4.4G。</p><p>接下来我们测试一下，将这个4.4G大小的表，存储到S3存储引擎，时间需要多久。</p><p>登陆到数据库，执行如下sql</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> test.s3test <span class="keyword">engine</span>=s3;</span><br></pre></td></tr></table></figure><p>输出如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Query OK, 63649280 rows affected (2 min 49.886 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>可以看到4.4G大小的表，存储引擎改为S3，只用了两分钟49秒，速度还是挺快的。</p><p>我们再看本地的数据文件<code>/home/mariadb/data/dir/test/s3test.ibd</code>已经不存在了，说明表的确已经被存到了云存储上面。</p><p>然后我们查看一下OBS上面的文件。</p><p>下载一个obsutil工具：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget https://obs-community.obs.cn-north-1.myhuaweicloud.com/obsutil/current/obsutil_linux_arm64.tar.gz</span><br><span class="line">tar -zxvf obsutil_linux_arm64.tar.gz</span><br><span class="line">cd obsutil_linux_arm64_5.2.10</span><br><span class="line"><span class="meta">#</span><span class="bash">配置obsutil工具</span></span><br><span class="line">./obsutil config -i=accesskey -k=secrectkey -e=obs.cn-north-4.myhuaweicloud.com</span><br></pre></td></tr></table></figure><p>查询一下OBS上的数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./obsutil ls obs://mariadb/test/s3test/</span><br></pre></td></tr></table></figure><p>输出的大小如下, 为3.22GB, 比innodb的存储还要小一点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Total size of prefix [test&#x2F;s3test&#x2F;] is: 3.22GB</span><br><span class="line">Folder number is: 0</span><br><span class="line">File number is: 826</span><br></pre></td></tr></table></figure><p>接下来我们查询一下数据，看看速度怎么样，登陆mariadb数据库，输入如下sql</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> test.s3test <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">8</span>;</span><br></pre></td></tr></table></figure><p>输出除了结果以外，显示用的时间如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">512 rows in set (1 min 17.121 sec)</span><br></pre></td></tr></table></figure><p>从6000多万条无索引的记录里，查询512条记录，大概用了1分钟17秒，速度还是可以接受的。</p><p>接下来我们看看建索引的速度，以及建索引后查询的速度。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; alter table test.s3test add index idx_id(id);</span><br><span class="line">Query OK, 63649280 rows affected (7 min 12.253 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>可以看到创建索引用了7分钟。</p><p>重新查询一下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; pager md5sum; select * from test.s3test where id &#x3D; 8;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (25.146 sec)</span><br></pre></td></tr></table></figure><p>可以看到这次速度快了一点，25秒。</p><p>我们看看索引占用的大小</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./obsutil ls obs://mariadb/test/s3test/</span><br></pre></td></tr></table></figure><p>输出的大小为3.81GB，比不建索引多了600M，说明索引占用了600M左右</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Total size of prefix [test&#x2F;s3test&#x2F;] is: 3.81GB</span><br><span class="line">Folder number is: 0</span><br><span class="line">File number is: 979</span><br></pre></td></tr></table></figure><p>可以证实一下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./obsutil ls obs://mariadb/test/s3test/index</span><br></pre></td></tr></table></figure><p>输出如下，的确占用600M左右</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Total size of prefix [test&#x2F;s3test&#x2F;index] is: 609.02MB</span><br><span class="line">Folder number is: 0</span><br><span class="line">File number is: 153</span><br></pre></td></tr></table></figure><p>接下来我们把索引删掉，看看用时多久。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; alter table test.s3test drop index idx_id;</span><br><span class="line">Query OK, 63649280 rows affected (4 min 18.969 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>删除索引用了4分钟。</p><p>把表的存储引擎改为InnoDB，看看用时多久。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; alter table test.s3test engine&#x3D;innodb;</span><br><span class="line">Query OK, 63649280 rows affected (6 min 6.302 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>用了6分钟左右。</p><p><strong>和InnoDB对比</strong></p><p>现在表在本地数据库，存储引擎是InnoDB，我们把以上操作重新来一遍，对比一下InnoDB引擎和S3引擎的性能。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id &#x3D; 8;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (30.218 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; alter table test.s3test add index idx_id(id);</span><br><span class="line">Query OK, 0 rows affected (2 min 1.309 sec)</span><br><span class="line">Records: 0  Duplicates: 0  Warnings: 0</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id &#x3D; 8;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (0.009 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; alter table test.s3test drop index idx_id;</span><br><span class="line">Query OK, 0 rows affected (0.009 sec)</span><br><span class="line">Records: 0  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>可以看出来，本地InnoDB引擎还是比S3要快不少，除了索引查询和删除索引操作外，无索引查询和创建索引两个操作并没有数量级上的差异，S3的速度还是可以接受的。</p><p><strong>增大S3 Page Buffer之后的性能</strong></p><p>我们把S3存储引擎的Buffer改大，看看性能有没有改善。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi ~&#x2F;mariadb.cnf</span><br></pre></td></tr></table></figure><p>添加如下配置，将s3的page buffer的大小设置为5个G（默认是128M）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s3_pagecache_buffer_size&#x3D;5368709120</span><br></pre></td></tr></table></figure><p>重启数据库</p><p>现在我们再来重复一遍之前的操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; alter table test.s3test engine&#x3D;s3;</span><br><span class="line">Query OK, 63649280 rows affected (5 min 8.340 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id &#x3D; 8;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (39.110 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id &#x3D; 8;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (10.019 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; alter table test.s3test add index idx_id(id);</span><br><span class="line">Query OK, 63649280 rows affected (5 min 45.067 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id &#x3D; 8;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (17.479 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id &#x3D; 8;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (0.015 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; alter table test.s3test drop index idx_id;</span><br><span class="line">Query OK, 63649280 rows affected (4 min 46.552 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; alter table test.s3test engine&#x3D;innodb;</span><br><span class="line">Query OK, 63649280 rows affected (6 min 28.248 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>可以看到改变引擎，创建删除索引等操作并没有实质上的提升，但是查询数据，特别是第二遍以后的查询数据，速度有数量级上的提升，主要应该归功于大的Buffer对数据的缓存。</p><p><strong>启用压缩之后的性能</strong></p><p>接下来我们测试一下创建表的时候，加上压缩参数 <code>COMPRESSION_ALGORITHM=zlib</code>，看看效果如何。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; alter table test.s3test engine&#x3D;s3  COMPRESSION_ALGORITHM&#x3D;zlib;</span><br><span class="line">Query OK, 63649280 rows affected (6 min 21.659 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>时间用了6分多钟，比不压缩长了一倍多一点。我们看下空间能节省多少。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./obsutil ls obs://mariadb/test/s3test/</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Total size of prefix [test&#x2F;s3test&#x2F;] is: 1.51GB</span><br><span class="line">Folder number is: 0</span><br><span class="line">File number is: 826</span><br></pre></td></tr></table></figure><p>可以看到空间节省了一半以上。不过我们的数据是随机的，如果是现实中有规律的数据，估计能有更高的压缩率。另外我们用了默认大小的4MB的块，如果把块的大小改大点，估计压缩率也会有提升。</p><p>我们看看查询速度怎么样：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id = 8;</span><br><span class="line">PAGER <span class="keyword">set</span> <span class="keyword">to</span> <span class="string">'md5sum'</span></span><br><span class="line"><span class="number">46</span>ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line"><span class="number">512</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">56.610</span> sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(<span class="keyword">none</span>)]&gt; pager md5sum;<span class="keyword">select</span> * <span class="keyword">from</span> test.s3test <span class="keyword">where</span> <span class="keyword">id</span> = <span class="number">8</span>;</span><br><span class="line">PAGER <span class="keyword">set</span> <span class="keyword">to</span> <span class="string">'md5sum'</span></span><br><span class="line"><span class="number">46</span>ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line"><span class="number">512</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">10.110</span> sec)</span><br></pre></td></tr></table></figure><p>可以看到首次查询，比没有压缩时慢了不到一倍，还是可以接受的，第二次查询，由于大的缓存的存在，压缩和没压缩的速度都是一样的。</p><p>再看看建立索引的情况：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; alter table test.s3test add index idx_id(id);</span><br><span class="line">Query OK, 63649280 rows affected (9 min 26.030 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>耗时了9分钟，也是慢了不到一倍。</p><p>看看索引占用的空间：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./obsutil ls obs://mariadb/test/s3test/index</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Total size of prefix [test&#x2F;s3test&#x2F;index] is: 261.54MB</span><br><span class="line">Folder number is: 0</span><br><span class="line">File number is: 153</span><br></pre></td></tr></table></figure><p>可以看到启用压缩以后，索引也变小了不少，比不压缩时的一半还要小一点。</p><p>再看看查询的情况：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id &#x3D; 8;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (27.094 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; pager md5sum;select * from test.s3test where id &#x3D; 8;</span><br><span class="line">PAGER set to &#39;md5sum&#39;</span><br><span class="line">46ebf3c834a4023edec7fe311f50438d  -</span><br><span class="line">512 rows in set (0.009 sec)</span><br></pre></td></tr></table></figure><p>第一次查询和没压缩时相比，慢了不到一倍，第二次查询由于大的缓存的存在，就和没压缩时一样了。</p><p>最后看看删除索引和将引擎改回InnoDB的耗时：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [(none)]&gt; alter table test.s3test drop index idx_id;</span><br><span class="line">Query OK, 63649280 rows affected (6 min 39.382 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; alter table test.s3test engine&#x3D;innodb;</span><br><span class="line">Query OK, 63649280 rows affected (6 min 36.595 sec)</span><br><span class="line">Records: 63649280  Duplicates: 0  Warnings: 0</span><br></pre></td></tr></table></figure><p>删除索引慢了不到一倍，改变引擎和没有压缩时耗时一样。</p><p><strong>总结</strong></p><p>通过我们的测试，可以发现，S3存储引擎在arm64上工作良好，性能也不错。</p><p>我们把上面测试的结果通过一个表格总结一下：</p><table><thead><tr><th>/</th><th>InnoDB</th><th>S3 with default options</th><th>S3 after setting pagecache buffer size  to 5G</th><th>S3 after compression enabled and page buffer size 5G</th></tr></thead><tbody><tr><td>Altering  table to S3</td><td>/</td><td>2 min 49.886 sec</td><td>5 min 8.340 sec</td><td>6 min 21.659 sec</td></tr><tr><td>Query without index</td><td>30.218 sec</td><td>1 min 17.121 sec</td><td>39.110 sec</td><td>56.610 sec</td></tr><tr><td>Query without index 2nd time</td><td>28.628 sec</td><td>51.305 sec</td><td>10.019 sec</td><td>10.110 sec</td></tr><tr><td>Creating index</td><td>2 min 1.309 sec</td><td>7 min 12.253 sec</td><td>5 min 45.067 sec</td><td>9 min 26.030 sec</td></tr><tr><td>Query with index</td><td>0.009 sec</td><td>25.146 sec</td><td>17.479 sec</td><td>27.094 sec</td></tr><tr><td>Query with index 2nd time</td><td>0.002 sec</td><td>29.264 sec</td><td>0.015 sec</td><td>0.009 sec</td></tr><tr><td>Dropping index</td><td>0.009 sec</td><td>4 min 18.969 sec</td><td>4 min 46.552 sec</td><td>6 min 39.382 sec</td></tr><tr><td>Altering  table to InnoDB</td><td>/</td><td>6 min 6.302 sec</td><td>6 min 28.248 sec</td><td>6 min 36.595 sec</td></tr><tr><td>Table size</td><td>4.4GB</td><td>3.22GB</td><td>3.22GB</td><td>1.51GB</td></tr><tr><td>Index size</td><td>1.1GB</td><td>609.02MB</td><td>609.02MB</td><td>261.54MB</td></tr></tbody></table><p><strong>参考链接</strong></p><p><a href="https://mariadb.com/kb/en/s3-storage-engine/">https://mariadb.com/kb/en/s3-storage-engine/</a></p><p><a href="https://mariadb.com/kb/en/plugin-overview/#installing-a-plugin">https://mariadb.com/kb/en/plugin-overview/#installing-a-plugin</a></p><p><a href="https://www.percona.com/blog/2020/07/17/mariadb-s3-engine-implementation-and-benchmarking/">https://www.percona.com/blog/2020/07/17/mariadb-s3-engine-implementation-and-benchmarking/</a></p><p><a href="https://blog.csdn.net/weixin_39132936/article/details/103260024">https://blog.csdn.net/weixin_39132936/article/details/103260024</a></p><p><a href="https://support.huaweicloud.com/obs/index.html">https://support.huaweicloud.com/obs/index.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: zhaorenhai&lt;/p&gt;
&lt;p&gt;MariaDB除了默认的InnoDB存储引擎以外，还有很多其他的存储引擎，这些引擎在某些方面也是比较有用的。从这篇文章开始，我们开始关注一下这些非默认的存储引擎在arm64平台上的表现。&lt;/p&gt;
&lt;p&gt;我们第一个研究的存储引擎是S3。 S3存储引擎是在10.5版本才引进的一个新功能。S3存储引擎其实就把数据存储在支持S3协议的云存储上。当前存储在上面的表只能读，不能写。可以通过改变表的存储引擎的方式，把数据传到S3上面去，也可以对S3存储引擎的表进行增加列和删除列，也可以建立索引，但是就是不能直接对表增删改（后续版本可能会有这些功能）。看到这里读者应该会有疑问，S3存储引擎有啥用？其实在对一些不再更新的数据，但是数据量又比较大，而且数据比较重要，又不能删除的场景比较有用，这时候就可以把这些表的存储引擎改为S3，因为S3存储引擎相对于本地存储比较便宜，而且可靠，是一个很好的选择。&lt;/p&gt;
&lt;p&gt;下面我们就简单的从几个方面看一下S3存储引擎在arm64平台上的表现&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>在ARM平台上玩转perl</title>
    <link href="https://kunpengcompute.github.io/2020/11/27/zai-arm-ping-tai-shang-wan-zhuan-perl/"/>
    <id>https://kunpengcompute.github.io/2020/11/27/zai-arm-ping-tai-shang-wan-zhuan-perl/</id>
    <published>2020-11-27T06:27:49.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者: <a href="https://github.com/bzhaoopenstack">bzhaoopenstack</a></p><p>在ARM平台上如何愉快的切换perl版本，给自己的项目提供方便，请看过来。</p><a id="more"></a><h1 id="在ARM平台上玩转perl"><a href="#在ARM平台上玩转perl" class="headerlink" title="在ARM平台上玩转perl"></a>在ARM平台上玩转perl</h1><p>最近在给开源项目接入ARM CI的工作中，遇到了很多问题。因为一般开源的主战场都在github，几乎所有在github上开源的项目在做自己的自动化构建时都会知道一个名字<code>travis</code> , 作为github上首屈一指的自动化构建平台，<code>travis</code>也是唯一一家支持ARM自动化构建的。所以，在给使用<code>travis</code>的开源项目接入ARM自动化构建平台时，我们往往也会想到直接用<code>travis</code>的ARM资源来进行构建。</p><p>但是，travis ARM 资源确实与AMD64资源相比差距比较大，注意，这里的差距比较大不是性能方面的，而是如下：</p><blockquote><ol><li>Travis ARM资源缺少很多基本库，原因之一是很多基本库是支持X86的，但是在aarch64平台上travis ARM测试床没有做很好的适配。</li><li>Travis build-in的工具没有及时跟进ARM平台，导致很多在X86平台已经配置好的travis job<strong>不能</strong>平滑的迁往ARM。即便项目是基于上层语言（Java/Python）。</li><li>由于Travis CI系统非常庞大，周边使用它的github项目也非常多，所以往往造成它的支撑团队没有足够的精力来处理成吨的需求，所以往往对于当前还处于成长期的ARM平台/资源来说，优先级可想而知。所以这会造成对于ARM的需求，无法很及时的响应。</li></ol></blockquote><p>根据上面的问题，我们主要举几个例子：</p><ol><li><p>例如在travis job配置过程中，用的ubuntu镜像，在使用apt安装某一个软件时，在X86正常安装，但是在ARM平台上报找不到包。这个时候就必须在ARM的travis job中手动换源，换成包含这个包的ubuntu源。</p></li><li><p>对于build-in工具，拿个最近遇到的case来举例：</p><blockquote><p>a. travis openjdk-ea安装。</p><p>​    我们知道在travis测试环境中安装jdk，travis是有通用字段可以描述的,看<a href="https://docs.travis-ci.com/user/languages/java/#testing-against-multiple-jdks">这里</a>。但是在ARM平台上就是装不了，是因为它内建的工具大部分没有做ARM适配，比如在这里遇到的<code>jdk_switcher</code>, 在ARM的ubuntu镜像里没有安装这个工具。所以给了接入travis ARM CI一定的难度。有两个方案：</p><blockquote><p>1). 推动上游travis支持。— 太慢</p><p>2). 手动模拟travis行为，尽可能模仿，一旦将来travis完善了，也能做到平滑迁移。</p></blockquote><p>当然，这两个方案可以同时进行，现在travis上游提出需求，同时在对想接入travis ARM CI的项目进行适配。这里我们找到了对应问题的项目：</p><p><a href="https://github.com/michaelklishin/jdk_switcher">jdk_switcher</a> 和 安装不同版本jdk的<a href="https://github.com/sormuras/bach/tree/master">install-jdk.sh</a>,比较幸运的是，后者还有人维护，并且新版本开始支持ARM平台的jdk版本下载及安装。所以在项目接入过程中，手动进行ARM适配。</p></blockquote></li></ol><p>接下来，介绍的perl在ARM上的精力也是在调试travis ARM CI过程中遇到的，所以为了一次捋顺，让我们看看如何搞。</p><p>我们用ubuntu bionic环境来举例。以下对于选用多个perl版本的项目比较有用。我们将会用到多个工具，我会做简要的介绍。</p><p>通常在ubuntu内建的软件包中都会含有perl，因为在apt-get工具对perl有强依赖，所以在安装和更新perl，甚至是替换<code>which perl</code>目录的软连接需要小心，你可能在破坏apt-get工具的依赖，导致apt-get都用不了。</p><p>这里我们先用系统内建的perl来安装后面我们需要的工具，如：</p><p>perlbrew 可以用源码方式安装perl的各种版本，可以容纳多个perl版本共存，并随意切换。</p><p>cpan 用于安装perl module的工具，比较古老，并且内置的cpan工具不太好用，而且做不到版本隔离。</p><p>cpanm 克服了cpan的一系列缺点，相当好用。</p><ol><li><p>首先通过内建的cpan安装perlbrew，因为内建的perl是与cpan关联的，由它开始是最为稳妥的办法，直接以源码或deb/yum包安装perl很快就会让你抓耳挠腮。命令如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># 由内建的cpan工具安装perlbrew，与内建的perl关联</span><br><span class="line">echo &quot;yes&quot; | sudo cpan App::perlbrew ;</span><br><span class="line"></span><br><span class="line"># 初始化，在用户家目录下导入必备的环境变量等等</span><br><span class="line">perlbrew init ;</span><br><span class="line"></span><br><span class="line"># 导入当前perlbrew的环境变量，为了确保我将它写在了~&#x2F;.bashrc里</span><br><span class="line">source ~&#x2F;perl5&#x2F;perlbrew&#x2F;etc&#x2F;bashrc ;</span><br><span class="line">cat ~&#x2F;perl5&#x2F;perlbrew&#x2F;etc&#x2F;bashrc &gt;&gt; ~&#x2F;.bashrc ;</span><br><span class="line"></span><br><span class="line"># 查看现在可用的所有perl版本</span><br><span class="line">perlbrew available</span><br><span class="line"> </span><br><span class="line"># 下载并安装对应的版本，它会默认安装到~&#x2F;perl5目录下，并且以版本号区分好，真正引用的时候可以用perlbrew非常方便的切换</span><br><span class="line">perlbrew install 5.18.2</span><br><span class="line">perlbrew install perl-5.8.1</span><br><span class="line">perlbrew install perl-5.19.9</span><br><span class="line"> </span><br><span class="line"># 查看已安装在本地的perl列表</span><br><span class="line">perlbrew list</span><br><span class="line"> </span><br><span class="line"># 设置默认使用的perl版本，注意，这个不影响apt-get命令，因为所有的bin&#x2F;lib都是与其分开的</span><br><span class="line">perlbrew switch perl-5.18.2</span><br><span class="line"> </span><br><span class="line"># 暂时切换使用的perl版本，比如在一个shell脚本里，切换完成后，你可以查看perl当前的版本是否对应</span><br><span class="line">perlbrew use perl-5.8.1</span><br><span class="line">perl -v</span><br></pre></td></tr></table></figure></li><li><p>下面在通过perlbrew安装cpanm，不要简单的认为，你能从apt-get 或者 官方脚本下载安装，这样非常不安全，因为如果从apt-get下载会与内建的perl耦合，非常影响后续的测试，官方脚本下载安装会重新创建一个新的perl运行环境，与之前利用内建cpan安装的perlbrew环境是隔离的，对之后版本切换以及perl module安装配置会很麻烦。所以，我们需要利用第一步安装的perlbrew来安装cpanm，这样既不破坏内建perl又能做到隔离，避免后续相当麻烦的配置工作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">perlbrew install-cpanm ;</span><br></pre></td></tr></table></figure></li><li><p>cpanm已经安装好后，下面我们可以根据不同版本的perl来安装不同的perl module环境，类似于python的虚环境。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 安装local::lib并切换导入的lib路径，你完全可以自定义。</span><br><span class="line">cpanm --local-lib&#x3D;~&#x2F;perl5 local::lib &amp;&amp; eval $(perl -I ~&#x2F;perl5&#x2F;lib&#x2F;perl5&#x2F; -Mlocal::lib)</span><br><span class="line"></span><br><span class="line"># 用cpanm安装一个perl module，后面的-n表示不执行测试进行安装，主要是为了提升安装速度。</span><br><span class="line">cpanm IPC::Run -n</span><br></pre></td></tr></table></figure></li></ol><p>在最后，提醒一点，这样操作后，你可以在编译基于perl的项目时仍然遇到缺少perl module的情况，如缺少XXX.pm等等，那是因为上面只是给当前运行的perl导入了lib路径，在编译是你仍然需要做的是, 导入LD_LIBRARY_PATH，举个我在编译PG时的例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">export LD_LIBRARY_PATH&#x3D;$HOME&#x2F;perl5&#x2F;lib&#x2F;perl5&#x2F;</span><br><span class="line">sudo apt install libperl-dev -y</span><br><span class="line">cd</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;postgres&#x2F;postgres</span><br><span class="line">cd postgres</span><br><span class="line">.&#x2F;configure --prefix&#x3D;$HOME&#x2F;pgsql-install --enable-tap-tests --with-perl</span><br><span class="line">make -j</span><br><span class="line">make install</span><br><span class="line"></span><br><span class="line">## Fix </span><br><span class="line">#pg@pg-test:~&#x2F;postgres&#x2F;src&#x2F;bin&#x2F;pg_basebackup$ pg_recvlogical -S test -d postgres --create-slot</span><br><span class="line">#pg_recvlogical: error: could not send replication command &quot;CREATE_REPLICATION_SLOT &quot;test&quot; LOGICAL &quot;test_decoding&quot; NOEXPORT_SNAPSHOT&quot;: ERROR:  could not access file &quot;test_decoding&quot;: No such file or directory</span><br><span class="line">cd contrib&#x2F;test_decoding&#x2F;</span><br><span class="line">make all</span><br><span class="line">cp test_decoding.so &#96;pg_config --pkglibdir&#96;</span><br><span class="line"></span><br><span class="line">export PGDATA&#x3D;$HOME&#x2F;pgsql-install&#x2F;data</span><br><span class="line">export LD_LIBRARY_PATH&#x3D;$HOME&#x2F;pgsql-install&#x2F;lib:$LD_LIBRARY_PATH</span><br><span class="line">export PATH&#x3D;$PATH:$HOME&#x2F;pgsql-install&#x2F;bin&#x2F;</span><br><span class="line">which psql</span><br><span class="line">psql --version</span><br></pre></td></tr></table></figure><p>好了，以上就可以用perl在ARM平台上快乐的玩耍了，非常方便易用。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: &lt;a href=&quot;https://github.com/bzhaoopenstack&quot;&gt;bzhaoopenstack&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在ARM平台上如何愉快的切换perl版本，给自己的项目提供方便，请看过来。&lt;/p&gt;
    
    </summary>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/categories/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/tags/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>在arm64平台上测试Percona Toolkit</title>
    <link href="https://kunpengcompute.github.io/2020/11/26/zai-arm64-ping-tai-shang-ce-shi-percona-toolkit/"/>
    <id>https://kunpengcompute.github.io/2020/11/26/zai-arm64-ping-tai-shang-ce-shi-percona-toolkit/</id>
    <published>2020-11-26T13:07:56.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者: zhaorenhai</p><p>Percona Toolkit是Mysql， MariaDB数据库领域一个很流行的工具箱，里面包含了很多个有用的工具。今天我们就来测试下这些工具是否可以在arm64平台上良好运行。</p><a id="more"></a><p>测试平台仍然选用华为云的鲲鹏虚拟机，OS采用Ubuntu18.04。 并提前已经在同内网内其他虚拟机上部署好了一对MariaDB主从数据库，并建好了供远程连接的数据库用户名和密码。MariaDB采用Ubuntu自带的10.1版本。</p><p>安装Percona Toolkit很简单，arm64版本的Ubuntu上面就已经自带了安装源，直接用如下命令安装即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install percona-toolkit</span><br></pre></td></tr></table></figure><p>当然也可以直接从源码编译，可以参考官网指导：</p><p><a href="https://github.com/percona/percona-toolkit">https://github.com/percona/percona-toolkit</a></p><p>安装好后，用<code>man percona-toolkit</code>可以看到工具列表，然后man 各个具体的工具名字，能看到进一步详细的说明。</p><p>工具本身的配置参数，在 <code>/etc/percona-toolkit</code>目录下面，所有工具的全局配置文件为<code>percona-toolkit.conf</code>, 每个工具单独的配置也在这个目录下面，配置文件就是 工具的名称.conf 这样的格式。 一般刚安装默认不需要配置什么东西，除非后续你有特殊的需求。</p><p>接下来我们从中挑一些工具来测试一下，看看是否能在arm64平台上良好运行。</p><p><strong>pt-align</strong></p><p>pt-align严格来说并不能算一个数据库工具，因为运行的时候，并不需要连接到数据库，这个是将输出对齐用的一个小工具。</p><p>比如有文件<code>test.txt</code>,内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DATABASE TABLE   ROWS</span><br><span class="line">          foo      bar      100</span><br><span class="line">          long_db_name table  1</span><br><span class="line">          another  long_name 500</span><br></pre></td></tr></table></figure><p>用<code>pt-align test.txt</code>对齐以后，格式如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DATABASE     TABLE     ROWS</span><br><span class="line">foo          bar        100</span><br><span class="line">long_db_name table        1</span><br><span class="line">another      long_name  500</span><br></pre></td></tr></table></figure><p>是不是好看多了。</p><p>再比如有<code>test1.csv</code>文件，内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">visit_district,pv_count,visitor_count,avg_visit_time,avg_visit_pages,new_visitor_count,ip_count</span><br><span class="line">中国,1285315,492801,300,3.9,453298,413898</span><br><span class="line">广东,226512,63361,320,4.3,52541,54010</span><br><span class="line">江苏,151617,62896,320,3.9,55049,59432</span><br><span class="line">北京,120757,44391,299,3.5,39392,35887</span><br><span class="line">上海,96754,41583,278,3.6,39806,33524</span><br><span class="line">新加坡,87186,11359,409,5.3,3875,8955</span><br><span class="line">浙江,84031,36488,264,3.2,34423,30087</span><br><span class="line">湖北,74910,34522,258,3.4,33746,25516</span><br><span class="line">山东,58738,25100,250,3.2,24352,19409</span><br></pre></td></tr></table></figure><p>将逗号替换成空格，再用pt-align对齐</p><p>命令如下</p><p><code>sed &#39;s/,/ /g&#39; test1.csv |pt-align</code></p><p>对齐后格式如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">visit_district pv_count visitor_count avg_visit_time avg_visit_pages new_visitor_count ip_count</span><br><span class="line">中国           1285315       492801            300             3.9            453298   413898</span><br><span class="line">广东           226512         63361            320             4.3             52541    54010</span><br><span class="line">江苏           151617         62896            320             3.9             55049    59432</span><br><span class="line">北京           120757         44391            299             3.5             39392    35887</span><br><span class="line">上海            96754         41583            278             3.6             39806    33524</span><br><span class="line">新加坡          87186         11359            409             5.3              3875     8955</span><br><span class="line">浙江            84031         36488            264             3.2             34423    30087</span><br><span class="line">湖北            74910         34522            258             3.4             33746    25516</span><br><span class="line">山东            58738         25100            250             3.2             24352    19409</span><br></pre></td></tr></table></figure><p>更容易方便人看文件的内容</p><p><strong>pt-archiver</strong></p><p>pt-archiver 是一个归档工具，可以把表的内容归档到一个文件，文件以后可以被其他导入工具或者导入命令用来导入数据库。也可以直接归档到另外一个数据库的表里面。这个工具可以添加where条件，只导表的一部分内容也可以。 </p><p>需要注意的一点是，这个工具归档完表里的内容，会把原表的内容删掉，如果不想删除原表的内容，要加<code>--no-delete</code>参数</p><p>我们来实际用一下，我们先把<code>testdb</code>数据库里面的表<code>cars</code>，归档到<code>cars_bak</code>里面（仅仅是演示，环境限制，就不归档到远程数据库了，大家明白这个工具有这个能力即可），然后再把这个<code>cars_bak</code>表归档到一个文件里。</p><p>首先登陆到数据库里面，创建一个<code>cars_bak</code>空表，然后再创建一个索引(pt-archiver默认要求表至少有一个索引才能正常被归档)，</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> testdb.cars_bak <span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> testdb.cars <span class="keyword">where</span> <span class="number">1</span>=<span class="number">2</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">index</span> idx_cars <span class="keyword">on</span> testdb.cars_bak(<span class="keyword">name</span>);</span><br></pre></td></tr></table></figure><p>然后执行如下命令，测试归档到表的功能</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pt-archiver --source h=192.168.0.204,u=proxysql,p=proxypassword,D=testdb,t=cars \</span><br><span class="line">            --dest h=192.168.0.204,u=proxysql,p=proxypassword,D=testdb,t=cars_bak \</span><br><span class="line">            --where "1=1" --limit 1000 --commit-each</span><br></pre></td></tr></table></figure><p>命令执行完没有任何提示（可以加–progress参数,能看到导的过程），我们把这个表导出来，看看里面的内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /var/log/archive</span><br><span class="line">pt-archiver --source h=192.168.0.204,u=proxysql,p=proxypassword,D=testdb,t=cars_bak \</span><br><span class="line">            --file '/var/log/archive/%Y-%m-%d-%D.%t' \</span><br><span class="line">            --where "1=1" --limit 1000</span><br></pre></td></tr></table></figure><p>查看文件内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /var/log/archive/2020-11-25-testdb.cars_bak</span><br></pre></td></tr></table></figure><p>内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1       Audi    52642</span><br><span class="line">3       Skoda   9000</span><br><span class="line">2       Volkswagen      21600</span><br></pre></td></tr></table></figure><p>pt-archiver这个工具还有很多其他参数，有兴趣的小伙伴可以自行研究一下</p><p><strong>pt-config-diff</strong></p><p>pt-config-diff是一个比较Mysql或者MariaDB配置的工具，这个配置可以是两个配置文件，也可以是两个不同的数据库实例上<code>show variables</code>命令的结果（也就是内存里实际正在生效的配置），也可以一个是数据库实例上内存里的配置，另外一个是配置文件。</p><p>根据我们的环境，我们先比较一下主库上的配置文件和从库上的配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp 192.168.0.204:/etc/mysql/mariadb.conf.d/50-server.cnf primary.cnf</span><br><span class="line">scp 192.168.0.64:/etc/mysql/mariadb.conf.d/50-server.cnf replica.cnf</span><br><span class="line">pt-config-diff primary.cnf replica.cnf</span><br></pre></td></tr></table></figure><p>两个配置文件没啥不同，输出为空</p><p>我们再比较下主库和从库上内存里的配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pt-config-diff h=192.168.0.204,u=proxysql,p=proxypassword h=192.168.0.64,u=proxysql,p=proxypassword</span><br></pre></td></tr></table></figure><p>这下可以看到不同了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">17 config differences</span><br><span class="line">Variable                  mariadb-2                 mariadb-3</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">binlog_format             MIXED                     STATEMENT</span><br><span class="line">general_log_file          primary1.log              mariadb-3.log</span><br><span class="line">gtid_binlog_pos           0-1-129</span><br><span class="line">gtid_binlog_state         0-1-129</span><br><span class="line">gtid_slave_pos                                      0-1-129</span><br><span class="line">hostname                  mariadb-2                 mariadb-3</span><br><span class="line">log_bin                   ON                        OFF</span><br><span class="line">log_bin_basename          &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;primary...</span><br><span class="line">log_bin_index             &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;primary...</span><br><span class="line">log_error                 .&#x2F;primary1.err            &#x2F;var&#x2F;log&#x2F;mysql&#x2F;error.log</span><br><span class="line">pid_file                  &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;primary... &#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld...</span><br><span class="line">relay_log                 primary1-relay-bin</span><br><span class="line">relay_log_basename        &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;primary...</span><br><span class="line">relay_log_index           &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;primary...</span><br><span class="line">server_id                 1                         2</span><br><span class="line">slow_query_log_file       primary1-slow.log         mariadb-3-slow.log</span><br><span class="line">wsrep_node_name           mariadb-2                 mariadb-3</span><br></pre></td></tr></table></figure><p>我们再比较下主库的内存和配置文件的不同</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pt-config-diff h=192.168.0.204,u=proxysql,p=proxypassword primary.cnf</span><br></pre></td></tr></table></figure><p>我们可以看到也是有两个不同的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2 config differences</span><br><span class="line">Variable                  mariadb-2                 primary.cnf</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">log_error                 .&#x2F;primary1.err            &#x2F;var&#x2F;log&#x2F;mysql&#x2F;error.log</span><br><span class="line">pid_file                  &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;primary... &#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld...</span><br></pre></td></tr></table></figure><p><strong>pt-diskstats</strong></p><p>pt-diskstats是一个查看磁盘io信息的工具，输出和iostat差不多，比iostat稍微详细点。另外这个工具可以把io信息保存到文件里，后续可以用pt-diskstats再打开这个文件分析，相当于一个io信息采集工具，这个用法用<code>man pt-diskstats</code>可以看到，我们就不再演示。这里只演示下默认的直接显示当前主机io的效果, 直接输入<code>pt-diskstats</code>回车即可</p><p>默认输出效果如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">ts device    rd_s rd_avkb rd_mb_s rd_mrg rd_cnc   rd_rt    wr_s wr_avkb wr_mb_s wr_mrg wr_cnc   wr_rt busy in_prg    io_s  qtime stime</span></span><br><span class="line">0.5 vda        0.0     0.0     0.0     0%    0.0     0.0     6.6   204.0     1.3    98%    0.0     0.0   0%      0     6.6    0.0   0.0</span><br><span class="line">0.5 vda2       0.0     0.0     0.0     0%    0.0     0.0     6.6   204.0     1.3    98%    0.0     0.0   0%      0     6.6    0.0   0.0</span><br><span class="line"></span><br><span class="line">1.0 vda        2.0     4.0     0.0     0%    0.0     0.0   109.0   718.6    76.5     7%    0.4     3.5   8%      0   111.0    1.6   0.6</span><br><span class="line">1.0 vda2       2.0     4.0     0.0     0%    0.0     0.0   109.0   718.6    76.5     7%    0.4     3.5   8%      8   111.0    1.5   0.6</span><br><span class="line"></span><br><span class="line">1.0 vda        3.0     4.0     0.0     0%    0.0     0.0   768.0   563.0   422.3     2%   20.4    26.0  66%      0   771.0   23.5   0.8</span><br><span class="line">1.0 vda2       3.0     4.0     0.0     0%    0.0     0.0   768.0   563.0   422.3     2%   20.4    26.0  84%     24   771.0   28.1   1.1</span><br><span class="line"></span><br><span class="line">1.0 vda        8.0    15.5     0.1     0%    0.0     0.5   444.0   577.2   250.3     2%   19.4    42.8  69%      0   452.0   29.6   1.5</span><br><span class="line">1.0 vda2       8.0    15.5     0.1     0%    0.0     0.5   444.0   577.2   250.3     2%   19.4    42.8  98%      2   452.0   31.8   2.1</span><br><span class="line"></span><br><span class="line">1.0 vda        3.0     4.0     0.0     0%    0.8   273.3   432.0   557.1   235.0     3%    2.7     6.0  76%      0   435.0    2.2   1.7</span><br><span class="line">1.0 vda2       3.0     4.0     0.0     0%    0.8   273.3   430.0   559.7   235.0     3%    2.7     6.0  76%      8   433.0    2.2   1.7</span><br></pre></td></tr></table></figure><p>按个<code>?</code>键，就可以显示用于交互的命令键</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">You can control this program by key presses:</span><br><span class="line">------------------- Key ------------------- ---- Current Setting ----</span><br><span class="line">A, D, S) Set the group-by mode              A</span><br><span class="line">c) Enter a Perl regex to match column names .</span><br><span class="line">&#x2F;) Enter a Perl regex to match disk names   (none)</span><br><span class="line">z) Set the sample size in seconds           1</span><br><span class="line">i) Hide inactive disks                      yes</span><br><span class="line">p) Pause the program</span><br><span class="line">q) Quit the program</span><br><span class="line">space) Print headers</span><br><span class="line">------------------- Press any key to continue -----------------------</span><br></pre></td></tr></table></figure><p><strong>pt-find</strong></p><p>pt-find是一个数据库搜索工具，可以根据你提供的条件搜索满足符合这些条件的对象，还可以加一个执行动作，对这些搜索出来的对象批量执行这个动作，这是一个很有用的工具。</p><p>我们来测试一下</p><p>比如我们要找到testdb数据库里面的空表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pt-find --empty -h192.168.0.204 -uproxysql -pproxypassword testdb</span><br></pre></td></tr></table></figure><p>输出如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#96;testdb&#96;.&#96;cars_bak&#96;</span><br><span class="line">&#96;testdb&#96;.&#96;football_clubs&#96;</span><br><span class="line">&#96;testdb&#96;.&#96;testtable&#96;</span><br></pre></td></tr></table></figure><p>如果想找到空表的同时，还要把这些表删掉，可以这么操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pt-find --empty -h192.168.0.204 -uproxysql -pproxypassword testdb --exec-plus "DROP TABLE %s"</span><br></pre></td></tr></table></figure><p>pt-find还有很多其他各种各样的搜索条件，以及可以加各种各样的执行动作，具体的请参考 <code>man pt-find</code></p><p>这是一个很强大的工具，但是操作起来，也有一定得风险，特别是你的执行动作是删除之类的操作的话，建议研究测试清楚以后，再执行。</p><p><strong>pt-heartbeat</strong></p><p>pt-heartbeat是一个监控主从数据库之间有没有延迟的工具。原理很简单，在主库起一个后台进程，这个进程会在数据库里创建一个心跳表，然后不停的往里更新数据。然后在从库上监控这些更新的数据有没有过来。</p><p>用法也很简单，首先在主数据库上起一个daemon进程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pt-heartbeat -D testdb --update -h 192.168.0.204 -uproxysql -pproxypassword --daemonize --create-table</span><br></pre></td></tr></table></figure><p>然后在从库上监控</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pt-heartbeat -D testdb --monitor -h 192.168.0.64 -uproxysql -pproxypassword</span><br></pre></td></tr></table></figure><p>输出如下, 第一列是当前延时，括号里面的分别是过去1分钟，5分钟，15分钟的延时平均值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1.00s [  0.02s,  0.00s,  0.00s ]</span><br><span class="line">1.00s [  0.03s,  0.01s,  0.00s ]</span><br><span class="line">2.00s [  0.07s,  0.01s,  0.00s ]</span><br><span class="line">0.00s [  0.07s,  0.01s,  0.00s ]</span><br><span class="line">0.00s [  0.07s,  0.01s,  0.00s ]</span><br><span class="line">0.00s [  0.07s,  0.01s,  0.00s ]</span><br><span class="line">0.00s [  0.07s,  0.01s,  0.00s ]</span><br><span class="line">0.00s [  0.07s,  0.01s,  0.00s ]</span><br><span class="line">0.00s [  0.07s,  0.01s,  0.00s ]</span><br><span class="line">0.00s [  0.07s,  0.01s,  0.00s ]</span><br><span class="line">0.00s [  0.07s,  0.01s,  0.00s ]</span><br><span class="line">0.00s [  0.07s,  0.01s,  0.00s ]</span><br><span class="line">0.00s [  0.07s,  0.01s,  0.00s ]</span><br><span class="line">0.00s [  0.07s,  0.01s,  0.00s ]</span><br></pre></td></tr></table></figure><p>如果只想查看一下，可以用check命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pt-heartbeat -D testdb --check -h 192.168.0.64 -uproxysql -pproxypassword</span><br></pre></td></tr></table></figure><p>只会输出一个时间延迟</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.00</span><br></pre></td></tr></table></figure><p><strong>pt-mysql-summary</strong></p><p>pt-mysql-summary是一个查看数据库信息的工具，这个工具可以看到数据库各个方面的信息，参数，配置，进程列表，对象数目，存储引擎信息，各项指标等等。 如果一下子想摸清数据库的基本情况，这个工具必不可少。</p><p>使用起来也很简单</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pt-mysql-summary -h 192.168.0.204 -u proxysql -p proxypassword --databases testdb</span><br></pre></td></tr></table></figure><p>具体的输出太多了，就不贴在这里了。</p><p>有兴趣的小伙伴可以自己试用一下。</p><p><strong>总结</strong>：</p><p>在我们测试的几款工具里，都可以正确的在arm64平台上运行，和x86上并没有差异。当然toolkit里还有很多其他好用的工具，篇幅所限，这里就不一一测试了，这些工具都是用perl编写的，理论上都是可以跨平台使用的。感兴趣的小伙伴可以自己试用一下。用<code>man percona-toolkit</code>命令可以看到具体的工具列表。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: zhaorenhai&lt;/p&gt;
&lt;p&gt;Percona Toolkit是Mysql， MariaDB数据库领域一个很流行的工具箱，里面包含了很多个有用的工具。今天我们就来测试下这些工具是否可以在arm64平台上良好运行。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>PG内建的pg_basebackup在ARM平台的表现</title>
    <link href="https://kunpengcompute.github.io/2020/11/26/pg-nei-jian-de-pg-basebackup-zai-arm-ping-tai-de-biao-xian/"/>
    <id>https://kunpengcompute.github.io/2020/11/26/pg-nei-jian-de-pg-basebackup-zai-arm-ping-tai-de-biao-xian/</id>
    <published>2020-11-26T07:13:15.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者: <a href="https://github.com/bzhaoopenstack">bzhaoopenstack</a></p><p>PG内建的热备工具pg_basebackup在ARM上能用吗？看看便知。</p><a id="more"></a><h2 id="PG内建的pg-basebackup在ARM平台的表现"><a href="#PG内建的pg-basebackup在ARM平台的表现" class="headerlink" title="PG内建的pg_basebackup在ARM平台的表现"></a>PG内建的pg_basebackup在ARM平台的表现</h2><p>pg_basebackup是postgresql提供的一个方便基础备份的工具（9.1开始提供），这个工具会把整个数据库实例的数据都拷贝出来，而不只是把实例中的部分（如某个数据库或表）单独备份出来，该工具使用replication协议连接到数据库实例上，所以主数据库中的pg_hba.conf必须允许replication连接，类似如下：<br>host    replication      replica                               trust<br>在9.2之后支持级连复制，所以在之后的版本中，pg_basebackup也可以从另外一个standby库上做基础备份，都需注意如下几方面：<br>1、备份中没有备份历史文件；<br>2、不确保所有需要的WAL文件都备份了，如果想确保，需要加命令行参数 ”-x”;<br>3、如果在备份过程中standby被提升为主库，则备份会失败；<br>4、要求主库中打开了“full_page_writes”参数，WAL文件不能被类似pg_compresslog的工具去掉full_page_writes信息。</p><p>对于这个工具，我们可以在<a href="https://postgresql.org/docs/10/app-pgbasebackup.html">官方文档</a>中查看工具的全部参数说明。</p><ol><li><h3 id="概念梳理"><a href="#概念梳理" class="headerlink" title="概念梳理"></a>概念梳理</h3><blockquote><p>对于postgresql常见的备份方式：</p><p>a. 文件系统级别的冷备份。</p><p>​    该备份方式需要关闭主数据库，然后拷贝数据文件的完整目录导备机。恢复数据库时，只需将数据目录复制到原来的位置。该方式实际工作中很少使用。</p><p>b.SQL转储</p><p>​    对于PG中，常用的工具为pg_dump和pg_dumpall。</p><p>​    这种方式可以在数据库正在使用的时候进行完整一致的备份，并不阻塞其它用户对数据库的访问。它会产生一个脚本文件，里面包含备份开始时，已创建的各种数据库对象的SQL语句和每个表中的数据。可以使用数据库提供的工具pg_dumpall和pg_dump来进行备份。pg_dump只备份数据库集群中的某个数据库的数据，它不会导出角色和表空间相关的信息，因为这些信息是整个数据库集群共用的，不属于某个单独的数据库。pg_dumpall，对集簇中的每个数据库调用pg_dump来完成该工作,还会还转储对所有数据库公用的全局对象（pg_dump不保存这些对象）。 目前这包括适数据库用户和组、表空间以及适合所有数据库的访问权限等属性。</p><p>​        例如，使用如下命令对名为dbname的数据库进行备份：</p><p>​        pg_dump  –h 127.0.0.1  -p  5432  -U  postgres -c  -C –f  dbname.sql  dbname</p><p>​        使用如下命令可对全部pg数据库进行备份。</p><p>​        pg_dumpall –h 127.0.0.1 –p 5432 -U postgres –c  -C –f db_bak.sql</p><p>​        恢复方式很简单。执行恢复命令即可：</p><p>​        psql –h 127.0.0.1 -p 5432 -U postgres –f db_bak.sql</p><p>c.连续归档</p><p>   这种方式的策略是把一个文件系统级别的全量备份和WAL(预写式日志)级别的增量备份结合起来。当需要恢复时，我们先恢复文件系统级别的备份，然后重放备份的WAL文件，把系统恢复到之前的某个状态。这种备份有显著的优点：</p><ol><li>不需要一个完美的一致的文件系统备份作为开始点。备份中的任何内部不一致性将通过日志重放来修正。</li><li>可以结合一个无穷长的WAL文件序列用于重放，可以通过简单地归档WAL文件来达到连续备份。</li><li>不需要重放WAL项一直到最后。可以在任何点停止重放，并使数据库恢复到当时的一致状态。</li><li>可以连续地将一系列WAL文件输送给另一台已经载入了相同基础备份文件的机器，得到一个实时的热备份系统。</li></ol></blockquote><p>好了，对于本文中pg_basebackup工具更倾向于第一种备份方式，但是又不完全是，因为pg_basebackup是用作热备份的，不需要停止主库服务，主库仍然能够接收请求。</p></li><li><h3 id="测试内容"><a href="#测试内容" class="headerlink" title="测试内容"></a>测试内容</h3></li></ol><p>本文中，我们会在ARM机器上测试pg_basebackup工具的可用性。</p><p>环境信息如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pg@pg-test:~&#x2F;postgres&#x2F;src&#x2F;bin&#x2F;pg_basebackup$ uname -a</span><br><span class="line">Linux pg-test 4.15.0-70-generic #79-Ubuntu SMP Tue Nov 12 10:36:10 UTC 2019 aarch64 aarch64 aarch64 GNU&#x2F;Linux</span><br><span class="line"></span><br><span class="line">OS: Ubuntu bionic</span><br><span class="line">ARCH: aarch64</span><br></pre></td></tr></table></figure><p>首先，我们看看如何在ARM上执行pg_basebackup的in-tree tests。可以参照如下脚本来进行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 构建perl环境，包括perlbrew版本管理，cpanm perl module安装工具</span><br><span class="line">echo &quot;yes&quot; | sudo cpan App::perlbrew ;</span><br><span class="line">perlbrew init ;</span><br><span class="line">source ~&#x2F;perl5&#x2F;perlbrew&#x2F;etc&#x2F;bashrc ;</span><br><span class="line">cat ~&#x2F;perl5&#x2F;perlbrew&#x2F;etc&#x2F;bashrc &gt;&gt; ~&#x2F;.bashrc ;</span><br><span class="line">perlbrew install-cpanm ;</span><br><span class="line"># 安装perl-local-lib命名空间</span><br><span class="line">cpanm --local-lib&#x3D;~&#x2F;perl5 local::lib &amp;&amp; eval $(perl -I ~&#x2F;perl5&#x2F;lib&#x2F;perl5&#x2F; -Mlocal::lib)</span><br><span class="line"># 安装测试依赖的perl IPR::Run模块</span><br><span class="line">cpanm IPC::Run -n</span><br><span class="line">export LD_LIBRARY_PATH&#x3D;$HOME&#x2F;perl5&#x2F;lib&#x2F;perl5&#x2F;</span><br><span class="line"># 安装 PG 在with-perl选项下依赖的libperl</span><br><span class="line">sudo apt install libperl-dev -y</span><br><span class="line"></span><br><span class="line"># 安装编译</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;postgres&#x2F;postgres</span><br><span class="line">cd postgres</span><br><span class="line"># 因为pg_basebackup测试是tap tests,所以需要加上--enable-tap-tests 这个option</span><br><span class="line">.&#x2F;configure --prefix&#x3D;$HOME&#x2F;pgsql-install --enable-tap-tests --with-perl</span><br><span class="line">make -j</span><br><span class="line">make install</span><br><span class="line"></span><br><span class="line"># 安装测试依赖的test_decoding.so文件</span><br><span class="line">make -C contrib&#x2F;test_decoding&#x2F; install</span><br><span class="line"></span><br><span class="line"># 开始跑测试</span><br><span class="line">make -C src&#x2F;bin&#x2F;pg_basebackup installcheck</span><br><span class="line"></span><br><span class="line"># 当然你也可以单跑其中的某个测试，如</span><br><span class="line">TESTDIR&#x3D;&#39;&#x2F;home&#x2F;pg&#x2F;postgres&#x2F;src&#x2F;bin&#x2F;pg_basebackup&#39; PATH&#x3D;&quot;&#x2F;home&#x2F;pg&#x2F;pgsql-install&#x2F;bin:$PATH&quot; PGPORT&#x3D;&#39;65432&#39; top_builddir&#x3D;&#39;&#x2F;home&#x2F;pg&#x2F;postgres&#x2F;src&#x2F;bin&#x2F;pg_basebackup&#x2F;..&#x2F;..&#x2F;..&#39; PG_REGRESS&#x3D;&#39;&#x2F;home&#x2F;pg&#x2F;postgres&#x2F;src&#x2F;bin&#x2F;pg_basebackup&#x2F;..&#x2F;..&#x2F;..&#x2F;src&#x2F;test&#x2F;regress&#x2F;pg_regress&#39; REGRESS_SHLIB&#x3D;&#39;&#x2F;home&#x2F;pg&#x2F;postgres&#x2F;src&#x2F;test&#x2F;regress&#x2F;regress.so&#39; &#x2F;usr&#x2F;bin&#x2F;prove -I ..&#x2F;..&#x2F;..&#x2F;src&#x2F;test&#x2F;perl&#x2F; -I . t&#x2F;010_pg_basebackup.pl -v</span><br></pre></td></tr></table></figure><p>在ARM上测试pg_basebackup目录下的in-tree testcase结果均为PASS。说明在ARM上基本功能是可用的。然而我们仍然需要更深入的测试。</p><p>创建测试用的目录、用户、表空间，表和索引等</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># 创建tablespace 真实路径</span><br><span class="line">mkdir ~&#x2F;tablespace</span><br><span class="line">mkdir ~&#x2F;tablespace&#x2F;testa_ts</span><br><span class="line">mkdir ~&#x2F;tablespace&#x2F;testb_ts</span><br><span class="line"></span><br><span class="line"># 创建用户</span><br><span class="line">psql -c &quot;create user testa with encrypted password &#39;pgonarm&#39;;&quot;</span><br><span class="line">psql -c &quot;create user testb with encrypted password &#39;pgonarm&#39;;&quot;</span><br><span class="line"></span><br><span class="line"># 创建对应用户的tablespace</span><br><span class="line">psql -c &quot;create tablespace testats owner testa location &#39;&#x2F;home&#x2F;pg&#x2F;tablespace&#x2F;testa_ts&#39;;&quot;</span><br><span class="line">psql -c &quot;create tablespace testbts owner testb location &#39;&#x2F;home&#x2F;pg&#x2F;tablespace&#x2F;testb_ts&#39;;&quot;</span><br><span class="line"></span><br><span class="line"># 在tablespace中创建db</span><br><span class="line">psql -c &quot;create database testadb with owner testa template template1 encoding &#39;UTF8&#39; tablespace&#x3D;testats;&quot;</span><br><span class="line">psql -c &quot;create database testbdb with owner testb template template1 encoding &#39;UTF8&#39; tablespace&#x3D;testbts;&quot;</span><br><span class="line"></span><br><span class="line"># 创建测试表及插入测试数据</span><br><span class="line">psql -U testa -d testadb -c &quot;create table test1(id numeric); insert into test1 values (1);&quot;</span><br><span class="line">psql -U testb -d testbdb -c &quot;create table test1(id numeric); insert into test1 values (1);&quot;</span><br><span class="line"></span><br><span class="line"># 在各自tablespace为双方设置权限</span><br><span class="line">psql -d testbdb -c &quot;grant create on tablespace testa_ts to testb; grant create on tablespace testb_ts to testa;&quot;</span><br><span class="line"></span><br><span class="line"># 创建索引</span><br><span class="line">psql -U testa -d testadb -c &quot;create index test1_index on test1(id) tablespace testb_ts;&quot;</span><br><span class="line">psql -U testb -d testbdb -c &quot;create index test1_index on test1(id) tablespace testa_ts;&quot;</span><br></pre></td></tr></table></figure><p>OK，数据准备好后，需要在pg_hba.conf中设置replication访问权限，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">host  replication   postgres    172.17.0.4&#x2F;32 trust</span><br></pre></td></tr></table></figure><p>表示为replication connection, 允许流复制链接。后面的32位IP地址可以跟一个网段，或者任意IP地址(4个0)。对应的允许用户当前为postgres，也可以根据需要进行设置，比如创建一个rep user等等。</p><p>好了，现在我们开始一步步测试。</p><h3 id="TEST-1-本地备份"><a href="#TEST-1-本地备份" class="headerlink" title="TEST 1  本地备份"></a>TEST 1  本地备份</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pg_basebackup -D testbk -Ft -z -P</span><br></pre></td></tr></table></figure><p>备份到本地testbk路径，-Ft选用的输出格式为tar. -z并启用gzip压缩，最后-P启用进度显示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">pg@pg-test:~$ pg_basebackup -D testbk -Ft -z -P</span><br><span class="line">46955&#x2F;46955 kB (100%), 3&#x2F;3 tablespaces</span><br><span class="line">pg@pg-test:~$ ls testbk&#x2F;</span><br><span class="line">16389.tar.gz  16390.tar.gz  backup_manifest  base.tar.gz  pg_wal.tar.gz</span><br><span class="line">pg@pg-test:~$</span><br><span class="line">pg@pg-test:~$ psql</span><br><span class="line">psql (14devel)</span><br><span class="line">Type &quot;help&quot; for help.</span><br><span class="line"></span><br><span class="line">pg&#x3D;# select oid, spcname from pg_tablespace;</span><br><span class="line">  oid  |  spcname</span><br><span class="line">-------+------------</span><br><span class="line">  1663 | pg_default</span><br><span class="line">  1664 | pg_global</span><br><span class="line"> 16389 | testats</span><br><span class="line"> 16390 | testbts</span><br><span class="line">(4 rows)</span><br><span class="line"></span><br><span class="line">pg&#x3D;#</span><br></pre></td></tr></table></figure><p>可以看到有3个表空间备份了，分别是16389 testats， 16390 testbts 还有 base pg_default三个表空间。</p><h3 id="TEST-2-远程备份"><a href="#TEST-2-远程备份" class="headerlink" title="TEST 2 远程备份"></a>TEST 2 远程备份</h3><p>使用另一台机器执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pg_basebackup -h HOST_TARGET -D BACKUP_DIR -Ft -z -P</span><br></pre></td></tr></table></figure><p>结果本地备份一致。</p><h3 id="TEST-3-单tablespace本地数据库备份"><a href="#TEST-3-单tablespace本地数据库备份" class="headerlink" title="TEST 3 单tablespace本地数据库备份"></a>TEST 3 单tablespace本地数据库备份</h3><p>执行备份命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pg@pg-test:~$ pg_basebackup -D --Ft -X fetch | bzip2 &gt; bac.tar.bz2</span><br><span class="line">pg_basebackup: error: directory &quot;&#x2F;home&#x2F;pg&#x2F;tablespace&#x2F;testb_ts&quot; exists but is not empty</span><br><span class="line">pg_basebackup: removing data directory &quot;--Ft&quot;</span><br><span class="line">pg@pg-test:~$</span><br></pre></td></tr></table></figure><p>失败了，因为本地数据库中包含有多个tablespace，所以失败，通常pg_basebackup对于tablespace备份时是单对单的，这样才能成功。</p><h3 id="TEST-4-tablespace重定向"><a href="#TEST-4-tablespace重定向" class="headerlink" title="TEST 4 tablespace重定向"></a>TEST 4 tablespace重定向</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pg@pg-test:~$ pg_basebackup -D bacdata_test -T &#x2F;home&#x2F;pg&#x2F;tablespace&#x2F;testa_ts&#x3D;&#x2F;home&#x2F;pg&#x2F;tablespace2&#x2F;testa_new_ts -T &#x2F;home&#x2F;pg&#x2F;tablespace&#x2F;testb_ts&#x3D;&#x2F;home&#x2F;pg&#x2F;tablespace2&#x2F;testb_new_ts -Ft -P</span><br><span class="line">46955&#x2F;46955 kB (100%), 3&#x2F;3 tablespaces</span><br><span class="line">pg@pg-test:~$ ls bacdata_test</span><br><span class="line">16389.tar  16390.tar  backup_manifest  base.tar  pg_wal.tar</span><br></pre></td></tr></table></figure><p>这种会将老的tablespace下的testa_ts、testb_ts重定向到tablespace2目录并且重命名。比如在新备机上，对存储路径有需求就可以这么玩。</p><p>至此，所有的测试项目均完成，可以看到pg_basebackup工具是能够在ARM平台上正常工作的。所以我们要对这个硬件平台新成员有足够的信心。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: &lt;a href=&quot;https://github.com/bzhaoopenstack&quot;&gt;bzhaoopenstack&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PG内建的热备工具pg_basebackup在ARM上能用吗？看看便知。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Arm64平台上都有哪些好用的MariaDB客户端软件</title>
    <link href="https://kunpengcompute.github.io/2020/11/24/arm64-ping-tai-shang-du-you-na-xie-hao-yong-de-mariadb-ke-hu-duan-ruan-jian/"/>
    <id>https://kunpengcompute.github.io/2020/11/24/arm64-ping-tai-shang-du-you-na-xie-hao-yong-de-mariadb-ke-hu-duan-ruan-jian/</id>
    <published>2020-11-24T13:36:10.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者: zhaorenhai</p><p>虽然MariaDB自带的命令行客户端，已经可以完成所有功能，可是从易用性方面，图形化展示方面，还是不够方便。本文计划测试下arm64平台上都有哪些好用的客户端软件，可以用来连接并使用MariaDB。</p><p>测试平台仍然选用华为云的鲲鹏虚拟机，OS采用Ubuntu18.04，MariaDB采用Ubuntu自带的10.1版本。 并提前已经在同内网内其他虚拟机上部署好了MariaDB数据库，并建好了供远程连接的数据库用户名和密码。</p><p>本文准备测试三个不同开发环境下的工具，这些工具都是可以公开下载，免费使用的。</p><p>一个是arm64 Linux后台命令行工具mycli，但是比默认的命令行客户端增加了自动完成，语法高亮等功能。</p><p>一个是免费的Linux图形化界面工具DBeaver，这个我们装在Windows上，但是连接到arm64平台上的MairaDB数据库。 图形化界面用Windows，后台服务用Linux，当前应该还是国内开发环境的主流。</p><p>最后一个是Web版本的工具，大名鼎鼎的phpMyAdmin，我们将这个工具的Server端也部署在arm64平台上，当然客户端就是浏览器，可以跨各个平台使用。</p><p>以上这三种开发环境应该可以匹配绝大多数的数据库开发环境了。</p><a id="more"></a><p><strong>mycli</strong></p><p>mycli是用python写的一个命令行工具，比默认的客户端多了语法高亮和自动完成功能，还可以编辑sql文件，用起来还是很好用的。</p><p>安装比较简单，可以用pip安装，也可以直接<code>apt install mycli</code></p><p>使用也很简单，如下，命令行直接设置要连接的数据库的地址，用户名，密码，端口号，数据库就行了，其他参数可以<code>mycli --help</code>看一下用法.</p><p>我们这里直接用apt安装一个，并连接到我们的测试库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt install mycli</span><br><span class="line">mycli -uproxysql -pproxypassword -h192.168.0.204 -P3306 testdb</span><br></pre></td></tr></table></figure><p>效果这里就不截图了，在官网能看到视频，效果还是很好的，自动完成和语法高亮功能很方便。</p><p>官网：<a href="https://www.mycli.net/">https://www.mycli.net/</a></p><p>关于具体的参数设置，颜色设置之类的，还有其他的高级功能，都可以查看官网的doc。</p><p><strong>DBeaver</strong></p><p>DBeaver是一个用Java开发的图形化数据库客户端工具，基于Eclipse平台开发的。可以用来连接各种各样支持jdbc的数据库。</p><p>我们从官网下载一个社区版，直接图形化安装即可，很简单。</p><p><a href="https://dbeaver.io/download/">https://dbeaver.io/download/</a></p><p>安装完直接运行，会提示要不要创建一个示例数据库，点击no，然后进入一个选择连接类型的对话框，选择MariaDB。</p><p>然后进入一个如下页面：</p><p><img src="https://i.loli.net/2020/11/24/Oj69z2ZEheXoB3b.png" alt="image-20201124174740317"></p><p>设置数据库的地址，端口号，数据库名称，用户名和密码，点击完成即可。</p><p>然后在树形栏里面点击配置好的数据库，第一次可能会提示你下载一个驱动，下载好之后，就可以看到如下界面了，工具很不错，尽情享用吧。</p><p><img src="https://i.loli.net/2020/11/24/BX1LJ7vPZqR4noj.png" alt="image-20201124175626573"></p><p><strong>phpMyAdmin</strong></p><p>phpMyAdmin可以提供一个WEB服务器，使用浏览器就可以访问服务器，使用图形化管理数据库的功能。</p><p>安装也比较方便，我们直接从apt安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install phpmyadmin</span><br></pre></td></tr></table></figure><p>安装过程中会让你选择web服务器，我们就选择apache即可。</p><p>并且会提示让你配置一个数据库，我们选择No，稍后手动配置。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/phpmyadmin/config-db.php</span><br></pre></td></tr></table></figure><p>输入如下内容，保存</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$dbuser&#x3D;&#39;proxysql&#39;;</span><br><span class="line">$dbpass&#x3D;&#39;proxypassword&#39;;</span><br><span class="line">$basepath&#x3D;&#39;&#39;;</span><br><span class="line">$dbname&#x3D;&#39;testdb&#39;;</span><br><span class="line">$dbserver&#x3D;&#39;192.168.0.204&#39;;</span><br><span class="line">$dbport&#x3D;&#39;3306&#39;;</span><br><span class="line">$dbtype&#x3D;&#39;mysql&#39;;</span><br></pre></td></tr></table></figure><p>然后</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ln -s /etc/phpmyadmin/apache.conf /etc/apache2/sites-enabled/apache.conf</span><br><span class="line">service apache2 restart</span><br></pre></td></tr></table></figure><p>浏览器访问如下地址：</p><p>http://服务器IP/phpmyadmin/</p><p>就可以看到如下页面了，输入用户名和密码，就可以登陆了</p><p><img src="https://i.loli.net/2020/11/24/dm5A7kLEaYzUnvC.png" alt="image-20201124212121163"></p><p>进去以后就可以看到如下页面</p><p><img src="https://i.loli.net/2020/11/24/kzQRFB85ncl1CPH.png" alt="image-20201124212521106"></p><p>测试了一下各项功能，都可以正常使用。</p><p><img src="https://i.loli.net/2020/11/24/TVkO3xsNcbf2rpR.png" alt="image-20201124213101235"></p><p><strong>参考链接：</strong></p><p><a href="https://mariadb.com/kb/en/graphical-and-enhanced-clients/">https://mariadb.com/kb/en/graphical-and-enhanced-clients/</a></p><p><a href="https://www.mycli.net/">https://www.mycli.net/</a></p><p><a href="https://dbeaver.io/">https://dbeaver.io/</a></p><p><a href="https://github.com/dbeaver/dbeaver">https://github.com/dbeaver/dbeaver</a></p><p><a href="https://docs.phpmyadmin.net/zh_CN/latest/">https://docs.phpmyadmin.net/zh_CN/latest/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: zhaorenhai&lt;/p&gt;
&lt;p&gt;虽然MariaDB自带的命令行客户端，已经可以完成所有功能，可是从易用性方面，图形化展示方面，还是不够方便。本文计划测试下arm64平台上都有哪些好用的客户端软件，可以用来连接并使用MariaDB。&lt;/p&gt;
&lt;p&gt;测试平台仍然选用华为云的鲲鹏虚拟机，OS采用Ubuntu18.04，MariaDB采用Ubuntu自带的10.1版本。 并提前已经在同内网内其他虚拟机上部署好了MariaDB数据库，并建好了供远程连接的数据库用户名和密码。&lt;/p&gt;
&lt;p&gt;本文准备测试三个不同开发环境下的工具，这些工具都是可以公开下载，免费使用的。&lt;/p&gt;
&lt;p&gt;一个是arm64 Linux后台命令行工具mycli，但是比默认的命令行客户端增加了自动完成，语法高亮等功能。&lt;/p&gt;
&lt;p&gt;一个是免费的Linux图形化界面工具DBeaver，这个我们装在Windows上，但是连接到arm64平台上的MairaDB数据库。 图形化界面用Windows，后台服务用Linux，当前应该还是国内开发环境的主流。&lt;/p&gt;
&lt;p&gt;最后一个是Web版本的工具，大名鼎鼎的phpMyAdmin，我们将这个工具的Server端也部署在arm64平台上，当然客户端就是浏览器，可以跨各个平台使用。&lt;/p&gt;
&lt;p&gt;以上这三种开发环境应该可以匹配绝大多数的数据库开发环境了。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>在arm64平台上监控MariaDB数据库的性能和运行情况</title>
    <link href="https://kunpengcompute.github.io/2020/11/20/zai-arm64-ping-tai-shang-jian-kong-mariadb-shu-ju-ku-de-xing-neng-he-yun-xing-qing-kuang/"/>
    <id>https://kunpengcompute.github.io/2020/11/20/zai-arm64-ping-tai-shang-jian-kong-mariadb-shu-ju-ku-de-xing-neng-he-yun-xing-qing-kuang/</id>
    <published>2020-11-20T06:40:52.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者: zhaorenhai</p><p>本篇文章尝试探索下，在arm64平台上都有哪些好用的性能监控工具，可以用来监控MariaDB数据库。</p><p>测试平台仍然选用华为云的鲲鹏虚拟机，OS采用Ubuntu18.04。 并提前已经在同内网内其他虚拟机上部署好了一对MariaDB主从数据库，并建好了供远程连接的数据库用户名和密码。MariaDB采用Ubuntu自带的10.1版本。</p><p>本文准备测试四个免费并且开源的工具，两个命令行工具，两个大型的图形化运维工具。</p><p>两个命令行工具是innotop和mytop</p><p>图形化运维工具就是Zabbix和Prometheus</p><a id="more"></a><p><strong>innotop</strong></p><p>innotop是一个用Perl写成的监控工具。源码在这里<a href="https://github.com/innotop/innotop">https://github.com/innotop/innotop</a> 。</p><p>我们直接下载下最新代码。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/innotop/innotop</span><br></pre></td></tr></table></figure><p>编译：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd innotop</span><br><span class="line">perl Makefile.PL</span><br></pre></td></tr></table></figure><p>安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo make install</span><br></pre></td></tr></table></figure><p>运行 （-w参数代表持久化模式，配置会持久化到文件里）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">innotop -w</span><br></pre></td></tr></table></figure><p>再输入一个？号，就可以看到使用指导。如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Query List (? for help)                                                                                                                                                                                                 Servers: server</span><br><span class="line"></span><br><span class="line">Switch to a different mode:</span><br><span class="line">   A  Dashboard         I  InnoDB I&#x2F;O Info     Q  Query List</span><br><span class="line">   B  InnoDB Buffers    K  InnoDB Lock Waits   R  InnoDB Row Ops</span><br><span class="line">   C  Command Summary   L  Locks               S  Variables &amp; Status</span><br><span class="line">   D  InnoDB Deadlocks  M  Replication Status  T  InnoDB Txns</span><br><span class="line">   F  InnoDB FK Err     O  Open Tables         U  User Statistics</span><br><span class="line"></span><br><span class="line">Actions:</span><br><span class="line">   a  Toggle the innotop process    n  Switch to the next connection</span><br><span class="line">   c  Choose visible columns        p  Pause innotop</span><br><span class="line">   d  Change refresh interval       q  Quit innotop</span><br><span class="line">   e  Explain a thread&#39;s query      r  Reverse sort order</span><br><span class="line">   f  Show a thread&#39;s full query    s  Change the display&#39;s sort column</span><br><span class="line">   h  Toggle the header on and off  t  Toggle slave processes</span><br><span class="line">   i  Toggle idle processes         x  Kill a query</span><br><span class="line">   k  Kill a query&#39;s connection</span><br><span class="line"></span><br><span class="line">Other:</span><br><span class="line"> TAB  Switch to the next server group   &#x2F;  Quickly filter what you see</span><br><span class="line">   !  Show license and warranty         &#x3D;  Toggle aggregation</span><br><span class="line"></span><br><span class="line">   #  Select&#x2F;create server groups       @  Select&#x2F;create server connections</span><br><span class="line"></span><br><span class="line">   $  Edit configuration settings       \  Clear quick-filters</span><br><span class="line">Press any key to continue</span><br></pre></td></tr></table></figure><p>我们输入一个@，回车，来配置一个数据库，监控一下看看效果。</p><p>第一次输入之后，没有任何连接，可以先随便输入一个名字，命名一下新的连接，然后回车。</p><p>接下来按照提示输入数据库地址，端口，配置数据库地址和端口的格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DBI:mysql:;host&#x3D;192.168.0.204;port&#x3D;3306</span><br></pre></td></tr></table></figure><p>然后回车，会提示你用户名，密码等等，提示还是很友好的。</p><p>配置好以后，就可以正常使用了。</p><p>我们可以输入大写的A，B，Q，I等命令分别切换不同的界面看到数据库的缓存信息，IO信息，TPS等等。</p><p>为了能在同一个界面显示多个数据库，我们可以先输入@，配置一下多个数据库链接。</p><p>然后再输入一个#，创建一个服务器组，输入已建好的多个连接名字，用空格分隔，就可以了。</p><p>操作的时候我们选择服务器组，就可以在同一个界面显示多个服务器。</p><p>比如我们配置了一个master数据库，一个slave数据库，并将他们放置到了server组内。</p><p>下面是显示的这两个数据库的IO界面，效果还是很不错的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">InnoDB I&#x2F;O Info (? for help)                                                                                                                                                                                            Servers: server</span><br><span class="line"></span><br><span class="line">_____________________________ I&#x2F;O Threads ______________________________</span><br><span class="line">CXN     Thread  Purpose               Thread Status</span><br><span class="line">master       0  insert buffer thread  waiting for completed aio requests</span><br><span class="line">master       1  log thread            waiting for completed aio requests</span><br><span class="line">master       2  read thread           waiting for completed aio requests</span><br><span class="line">master       3  read thread           waiting for completed aio requests</span><br><span class="line">master       4  read thread           waiting for completed aio requests</span><br><span class="line">master       5  read thread           waiting for completed aio requests</span><br><span class="line">master       6  write thread          waiting for completed aio requests</span><br><span class="line">master       7  write thread          waiting for completed aio requests</span><br><span class="line">master       8  write thread          waiting for completed aio requests</span><br><span class="line">master       9  write thread          waiting for completed aio requests</span><br><span class="line">slave        0  insert buffer thread  waiting for completed aio requests</span><br><span class="line">slave        1  log thread            waiting for completed aio requests</span><br><span class="line">slave        2  read thread           waiting for completed aio requests</span><br><span class="line">slave        3  read thread           waiting for completed aio requests</span><br><span class="line">slave        4  read thread           waiting for completed aio requests</span><br><span class="line">slave        5  read thread           waiting for completed aio requests</span><br><span class="line">slave        6  write thread          waiting for completed aio requests</span><br><span class="line">slave        7  write thread          waiting for completed aio requests</span><br><span class="line">slave        8  write thread          waiting for completed aio requests</span><br><span class="line">slave        9  write thread          waiting for completed aio requests</span><br><span class="line"></span><br><span class="line">________________________________ Pending I&#x2F;O _________________________________</span><br><span class="line">CXN     Async Rds  Async Wrt  IBuf Async Rds  Sync I&#x2F;Os  Log Flushes  Log I&#x2F;Os</span><br><span class="line">master                                     0          0            0         0</span><br><span class="line">slave                                      0          0            0         0</span><br><span class="line"></span><br><span class="line">____________________________ File I&#x2F;O Misc _____________________________</span><br><span class="line">CXN     OS Reads  OS Writes  OS fsyncs  Reads&#x2F;Sec  Writes&#x2F;Sec  Bytes&#x2F;Sec</span><br><span class="line">master       178        984        372       0.00        0.00          0</span><br><span class="line">slave        182       1529        584       0.00        0.00          0</span><br><span class="line"></span><br><span class="line">_________________________ Log Statistics _________________________</span><br><span class="line">CXN     Sequence No.  Flushed To  Last Checkpoint  IO Done  IO&#x2F;Sec</span><br><span class="line">master  1818551       1818551     1818551              207    0.00</span><br><span class="line">slave   1939016       1939016     1939016              306    0.00</span><br></pre></td></tr></table></figure><p>本文不是innotop的介绍文档，仅仅是测试一下innotop是否能在arm64平台上正常运行，是否能正常监控MariaDB数据库。所以关于inntop的其他使用方法和界面介绍, 就不再啰嗦了。有兴趣的小伙伴也可以试一下，网上也有很多innotop的资料，用命令<code>man innotop</code>也可以看到使用指导。</p><p><strong>mytop</strong></p><p>mytop也是一个用Perl写的小工具，业界应用也比较广泛。不过缺点是只能连一个数据库服务器，而且比较老了，功能也比较简单，得到的信息比较有限。</p><p>mytop可以直接从apt命令安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install mytop</span><br></pre></td></tr></table></figure><p>配置，编辑如下文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi ~/.mytop</span><br></pre></td></tr></table></figure><p>输入如下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">user&#x3D;proxysql</span><br><span class="line">pass&#x3D;proxypassword</span><br><span class="line">host&#x3D;192.168.0.204</span><br><span class="line">db&#x3D;testdb</span><br><span class="line">delay&#x3D;5</span><br><span class="line">port&#x3D;3306</span><br><span class="line">socket&#x3D;</span><br><span class="line">batchmode&#x3D;0</span><br><span class="line">header&#x3D;1</span><br><span class="line">color&#x3D;1</span><br><span class="line">idle&#x3D;1</span><br></pre></td></tr></table></figure><p>执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mytop</span><br></pre></td></tr></table></figure><p>界面如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">MySQL on 192.168.0.204 (10.1.47)                                                                                                                                             load 0.00 0.00 0.00 1&#x2F;1397 32133 up 16+23:58:33 [17:45:26]</span><br><span class="line"> Queries: 50.0     qps:    0 Slow:     0.0         Se&#x2F;In&#x2F;Up&#x2F;De(%):    12&#x2F;00&#x2F;00&#x2F;00</span><br><span class="line"> Sorts:      0 qps now:    1 Slow qps: 0.0  Threads:    5 (   2&#x2F;   7) 00&#x2F;00&#x2F;00&#x2F;00</span><br><span class="line"> Key Efficiency: 84.0%  Bps in&#x2F;out:   0.0&#x2F;  0.2   Now in&#x2F;out:  21.3&#x2F; 3.6k</span><br><span class="line"></span><br><span class="line">       Id      User         Host&#x2F;IP         DB       Time    Cmd    State Query</span><br><span class="line">       --      ----         -------         --       ----    ---    ----- ----------</span><br><span class="line">       46 replicati hadoop-arm-kae-               1466725 Binlog Master h</span><br><span class="line">       63  proxysql hadoop-arm-kae-       test          4  Sleep</span><br><span class="line">       57   monitor hadoop-arm-kae-                     3  Sleep</span><br><span class="line">      838   monitor hadoop-arm-kae-                     0  Sleep</span><br><span class="line">    24644  proxysql hadoop-arm-kae-     testdb          0  Query     init show full processlist</span><br></pre></td></tr></table></figure><p>可以看到风格和top差不多，效果也挺不错的。</p><p>关于mytop的更多信息，可以参考如下链接：<a href="http://jeremy.zawodny.com/mysql/mytop/">http://jeremy.zawodny.com/mysql/mytop/</a> 。网上相关资料也很多，用<code>man mytop</code>命令也可看到相关指导。</p><p><strong>Zabbix</strong></p><p>Zabbix是一个大型的监控告警运维工具，不仅仅只是用来监控数据库的，不过我们这里只简单用它来监控一下MariaDB。</p><p>关于它的架构，功能等等的详细介绍，官网上包括其他网站上资料已经很多，这里不再详述。这里只描述在我们的测试环境上测试的步骤。(具体的安装步骤，不同版本的Zabbix可能不太一样，不同的OS也有不同，这里安装的是Ubuntu 18.04自带的Zabbix3.0版本，仅供参考)</p><p>首先我们在测试虚拟机上安装Zabbix Server和Zabbix Frontend</p><p>zabbix-server根据自身用的内置数据库的不同，有PostgreSql和Mysql两个版本，这里为了和我们要监控的目标数据库MariaDB区分开来，我们这里安装PostgreSql版本。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install zabbix-server-pgsql zabbix-frontend-php php-pgsql</span><br></pre></td></tr></table></figure><p>在我们要监控的主从数据库节点上都安装Zabbix Agent</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install zabbix-agent</span><br></pre></td></tr></table></figure><p>Zabbix Agent安装之后就会自动启动。</p><p>Zabbix Server需要配置下，登陆Server所在节点</p><p>首先创建一个Zabbix Server自用的数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo -u postgres createuser --pwprompt zabbix</span><br><span class="line">sudo -u postgres createdb -O zabbix -E Unicode -T template0 zabbix</span><br></pre></td></tr></table></figure><p>然后导入数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zcat /usr/share/zabbix-server-pgsql/schema.sql.gz |sudo -u zabbix psql zabbix</span><br><span class="line">zcat /usr/share/zabbix-server-pgsql/images.sql.gz |sudo -u zabbix psql zabbix</span><br><span class="line">zcat /usr/share/zabbix-server-pgsql/data.sql.gz |sudo -u zabbix psql zabbix</span><br></pre></td></tr></table></figure><p>在Zabbix Server配置文件中配置刚刚设置的自用数据库的相关信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/zabbix/zabbix_server.conf</span><br></pre></td></tr></table></figure><p>确保如下配置项都配置正确</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DBHost&#x3D;localhost</span><br><span class="line">DBName&#x3D;zabbix</span><br><span class="line">DBUser&#x3D;zabbix</span><br><span class="line">DBPassword&#x3D;zabbix</span><br></pre></td></tr></table></figure><p>启动Zabbix Server</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service zabbix-server start</span><br><span class="line">update-rc.d zabbix-server enable</span><br></pre></td></tr></table></figure><p>开始配置Frontend</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/apache2/conf-enabled</span><br><span class="line">ln -s ../conf-available/zabbix-frontend-php.conf zabbix-frontend-php.conf</span><br><span class="line">vi zabbix-frontend-php.conf</span><br></pre></td></tr></table></figure><p>将里面的时区参数配置正确，默认的如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># php_value date.timezone Europe&#x2F;Riga</span><br></pre></td></tr></table></figure><p>我们改成如下设置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">php_value date.timezone Asia&#x2F;Shanghai</span><br></pre></td></tr></table></figure><p>修改成正确的时区后，继续做如下操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service apache2 restart</span><br></pre></td></tr></table></figure><p>然后从浏览器访问如下链接（注意其中的地址要替换为你环境的地址，另外如果要从公网的地址访问的话，注意配置好华为云的安全组规则，限制要登陆的地址范围。）</p><p><em>http://测试机的ip地址/zabbix</em></p><p>然后点击next，根据提示一步步配置即可。</p><p>配置数据库连接的地方，按下图配置</p><p><img src="https://i.loli.net/2020/11/20/9BM2ECHVf8X3ybA.png" alt="image-20201118144826675"></p><p>配置server的地方，直接默认值即可</p><p><img src="https://i.loli.net/2020/11/20/DvoruKhilXWFjcm.png" alt="image-20201118144923513"></p><p>最后一步，会提示下载一个配置文件，将配置文件下载下来，上传到<code>/etc/zabbix</code>目录下即可。</p><p><img src="https://i.loli.net/2020/11/20/OIWoK3hz6GBE1tx.png" alt="image-20201118150217001"></p><p>最后重启下apache2</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service apache2 restart</span><br></pre></td></tr></table></figure><p>然后重新登陆 <em>http://测试机的ip地址/zabbix</em> 网页，输入用户名Admin，密码zabbix，就可以看到Zabbix的系统页面了。</p><p>然后我们创建两个Host，一个是MariaDB主库，一个是MariaDB从库。</p><p>点击Configuration，Hosts，Create host，其中Host name，Groups和IP地址，是必填项。</p><p><img src="https://i.loli.net/2020/11/20/vzFMQmtLAkD9WZU.png" alt="image-20201119101133383"></p><p>主机创建好后，可以点击上图中的圈红的那个Templates按钮，到如下页面，输入mysql搜索模板，然后点击add按钮，把mysql监控模板关联到主机</p><p><img src="https://i.loli.net/2020/11/20/TGKsMeYZyQSUHhm.png" alt="image-20201119101336228"></p><p>Frontend上面的配置就完成了。</p><p>登陆两个agent所在虚拟机，做如下配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/share/doc/zabbix-agent/examples/userparameter_mysql.conf /etc/zabbix/zabbix_agentd.conf.d/</span><br><span class="line">mkdir -p /var/lib/zabbix</span><br></pre></td></tr></table></figure><p>编辑配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /var/lib/zabbix/.my.cnf</span><br></pre></td></tr></table></figure><p>输入如下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[mysql]</span><br><span class="line">user&#x3D;proxysql</span><br><span class="line">password&#x3D;proxypassword</span><br><span class="line">host&#x3D;127.0.0.1</span><br><span class="line"></span><br><span class="line">[mysqladmin]</span><br><span class="line">user&#x3D;proxysql</span><br><span class="line">password&#x3D;proxypassword</span><br><span class="line">host&#x3D;127.0.0.1</span><br></pre></td></tr></table></figure><p>注意输入的用户要有在本机连接MariaDB，并有查询的相关权限。</p><p>然后编辑<code>/etc/zabbix/zabbix_agentd.conf</code>文件，确保如下三项的配置是正确的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Server&#x3D;192.168.0.54</span><br><span class="line">ServerActive&#x3D;192.168.0.54</span><br><span class="line">Hostname&#x3D;mariadb_slave</span><br></pre></td></tr></table></figure><p>其中Server和ServerActive是Zabbix Server的IP地址，Hostname是在Frontend上面配置的Agent所在的Host的名字，每个Agent的配置不一样。</p><p>然后重启agent</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service zabbix-agent restart</span><br></pre></td></tr></table></figure><p>现在重新登陆前端，就可以看到监控效果了。</p><p>点击Monitoring，Latest Data，可以看到很多监控项，带宽，慢查询，select，insert，delete请求的qps等等，都有。</p><p>我们看看一个带宽的监控图，效果如下：</p><p><img src="https://i.loli.net/2020/11/20/Ie3qKTyPkN2DM1h.png" alt="image-20201119102130275"></p><p>如果觉得默认的Mysql的模板功能不够完善，感兴趣的小伙伴可以自己定制模板，也可以从Percona的网站上面下载已经定制好的功能比较完善的模板，导入进来。 这里就不详细介绍了</p><p><strong>Prometheus</strong></p><p>Prometheus 也是一个大型的监控运维工具，这次我们也只是简单测试下他的数据库监控功能，关于他的架构，功能等各方面的介绍，这里就不再多说。</p><p>我们首先来安装一下Prometheus Server。</p><p>我们从官网下载一个最新的arm64版本的Prometheus来安装。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/prometheus/prometheus/releases/download/v2.22.2/prometheus-2.22.2.linux-arm64.tar.gz</span><br><span class="line">tar -zxvf prometheus-2.22.2.linux-arm64.tar.gz</span><br><span class="line">cd prometheus-2.22.2.linux-arm64</span><br></pre></td></tr></table></figure><p>在主从数据库节点上下载最新的mysqld_exporter来安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/prometheus/mysqld_exporter/releases/download/v0.12.1/mysqld_exporter-0.12.1.linux-arm64.tar.gz</span><br><span class="line">tar -zxvf mysqld_exporter-0.12.1.linux-arm64.tar.gz</span><br><span class="line">cd mysqld_exporter-0.12.1.linux-arm64</span><br></pre></td></tr></table></figure><p>编辑文件<code>vi prometheus.yml</code>，追加如下配置，里面的IP地址是主从数据库节点的IP，端口是稍后我们会安装在主从数据库节点的相当于客户端进程的端口，我们这里先配置上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- job_name: &#39;mariadb&#39;</span><br><span class="line">  static_configs:</span><br><span class="line">  - targets: [&#39;192.168.0.204:9104&#39;,&#39;192.168.0.64:9104&#39;]</span><br></pre></td></tr></table></figure><p>启动Prometheus</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;prometheus --config.file&#x3D;prometheus.yml</span><br></pre></td></tr></table></figure><p>在主从数据库节点上都编辑如下的文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi .my.cnf</span><br></pre></td></tr></table></figure><p>输入如下内容，注意确保输入的用户有在本地连接MariaDB的权限，并有相关查询的权限：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[client]</span><br><span class="line">user&#x3D;proxysql</span><br><span class="line">password&#x3D;proxypassword</span><br></pre></td></tr></table></figure><p>启动mysqld_exporter</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup ./mysqld_exporter --config.my-cnf=./.my.cnf &amp;</span><br></pre></td></tr></table></figure><p>登陆Prometheus 页面：（注意其中的地址要替换为你环境的地址，另外如果要从公网的地址访问的话，注意配置好华为云的安全组规则，限制要登陆的地址范围。）</p><p>http://你的IP地址:9090</p><p>可以在Execute按钮旁边的下拉框里面看到很多指标，我们选择一个连接数的，调整一下监控时间，比如我们调整成过去30分钟，然后就可以看到如下图所示的监控图</p><p><img src="https://i.loli.net/2020/11/20/2RTmQVrpZGzevg1.png" alt="image-20201120143145987"></p><p><strong>总结</strong></p><p>通过以上的测试我们可以发现，这几款常用的监控工具，都可以在arm64平台上运行良好，而且都可以监控MariaDB数据库，效果都不错。而且这几款工具还都是开源免费的，有兴趣的小伙伴可以探索一下更多的功能。</p><p><strong>参考链接：</strong></p><p><a href="https://www.cnblogs.com/ivictor/p/5101506.html">https://www.cnblogs.com/ivictor/p/5101506.html</a></p><p><a href="https://www.jianshu.com/p/b8508fe10b8e">https://www.jianshu.com/p/b8508fe10b8e</a></p><p><a href="https://github.com/innotop/innotop">https://github.com/innotop/innotop</a></p><p><a href="https://github.com/jzawodn/mytop">https://github.com/jzawodn/mytop</a></p><p><a href="http://jeremy.zawodny.com/mysql/mytop/">http://jeremy.zawodny.com/mysql/mytop/</a></p><p><a href="https://www.zabbix.com/documentation/4.0/zh/manual/introduction">https://www.zabbix.com/documentation/4.0/zh/manual/introduction</a></p><p><a href="https://prometheus.io/docs/prometheus/latest/getting_started/">https://prometheus.io/docs/prometheus/latest/getting_started/</a></p><p><a href="https://www.cnblogs.com/heian99/p/12189317.html">https://www.cnblogs.com/heian99/p/12189317.html</a></p><p><a href="https://prometheus.io/download/">https://prometheus.io/download/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: zhaorenhai&lt;/p&gt;
&lt;p&gt;本篇文章尝试探索下，在arm64平台上都有哪些好用的性能监控工具，可以用来监控MariaDB数据库。&lt;/p&gt;
&lt;p&gt;测试平台仍然选用华为云的鲲鹏虚拟机，OS采用Ubuntu18.04。 并提前已经在同内网内其他虚拟机上部署好了一对MariaDB主从数据库，并建好了供远程连接的数据库用户名和密码。MariaDB采用Ubuntu自带的10.1版本。&lt;/p&gt;
&lt;p&gt;本文准备测试四个免费并且开源的工具，两个命令行工具，两个大型的图形化运维工具。&lt;/p&gt;
&lt;p&gt;两个命令行工具是innotop和mytop&lt;/p&gt;
&lt;p&gt;图形化运维工具就是Zabbix和Prometheus&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>主流开发语言如何在arm64平台上连接MariaDB数据库</title>
    <link href="https://kunpengcompute.github.io/2020/11/14/zhu-liu-kai-fa-yu-yan-ru-he-zai-arm64-ping-tai-shang-lian-jie-mariadb-shu-ju-ku/"/>
    <id>https://kunpengcompute.github.io/2020/11/14/zhu-liu-kai-fa-yu-yan-ru-he-zai-arm64-ping-tai-shang-lian-jie-mariadb-shu-ju-ku/</id>
    <published>2020-11-14T15:58:41.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者: zhaorenhai</p><p>本文尝试探索下几种主流的开发语言如何在arm64平台上连接MariaDB数据库，并测试一下增删改查各项功能是否都正常。主要包括如下几种语言：C , Java, Node.js, Python, Go, Rust, PHP</p><p>测试平台采用华为云的鲲鹏虚拟机，OS采用Ubuntu18.04。 并提前已经在同内网内的另外一台虚拟机上已经部署好了一个MariaDB数据库，并建好了供远程连接的数据库用户名和密码, 这里假设我们新建的用户名为proxysql，密码为proxypassword，端口号就用默认的3306端口号。</p><a id="more"></a><p><strong>C语言</strong></p><p>首先安装连接器库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install libmariadbclient-dev</span><br></pre></td></tr></table></figure><p>然后新建一个C语言文件<code>version.c</code>，输入如下内容：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mysql.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"MySQL client version: %s\n"</span>, mysql_get_client_info());</span><br><span class="line"></span><br><span class="line">  <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编译并执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gcc version.c -o version  `mysql_config --cflags --libs`</span><br><span class="line">./version</span><br></pre></td></tr></table></figure><p>可以看到输出如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MySQL client version: 10.1.47-MariaDB</span><br></pre></td></tr></table></figure><p>说明可以成功打印出客户端的版本。</p><p>接下来我们新建一个<code>createdb.c</code>文件，输入如下代码，测试下创建数据库的功能</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mysql.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  MYSQL *con = mysql_init(<span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (con == <span class="literal">NULL</span>)</span><br><span class="line">  &#123;</span><br><span class="line">      <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"%s\n"</span>, mysql_error(con));</span><br><span class="line">      <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (mysql_real_connect(con, <span class="string">"192.168.0.204"</span>, <span class="string">"proxysql"</span>, <span class="string">"proxypassword"</span>,</span><br><span class="line">    <span class="literal">NULL</span>, <span class="number">0</span>, <span class="literal">NULL</span>, <span class="number">0</span>) == <span class="literal">NULL</span>)</span><br><span class="line">  &#123;</span><br><span class="line">      <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"%s\n"</span>, mysql_error(con));</span><br><span class="line">      mysql_close(con);</span><br><span class="line">      <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"CREATE DATABASE testdb"</span>))</span><br><span class="line">  &#123;</span><br><span class="line">      <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"%s\n"</span>, mysql_error(con));</span><br><span class="line">      mysql_close(con);</span><br><span class="line">      <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  mysql_close(con);</span><br><span class="line">  <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编译并执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gcc createdb.c -o createdb  `mysql_config --cflags --libs`</span><br><span class="line">./createdb</span><br></pre></td></tr></table></figure><p>接下来我们新建一个<code>testdb.c</code>文件，输入如下代码，测试下新增表，插入记录，查询表，关联查询，更新，删除记录，删除表各项功能。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mysql.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">finish_with_error</span><span class="params">(MYSQL *con)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"%s\n"</span>, mysql_error(con));</span><br><span class="line">  mysql_close(con);</span><br><span class="line">  <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  MYSQL *con = mysql_init(<span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (con == <span class="literal">NULL</span>)</span><br><span class="line">  &#123;</span><br><span class="line">      <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"%s\n"</span>, mysql_error(con));</span><br><span class="line">      <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (mysql_real_connect(con, <span class="string">"192.168.0.204"</span>, <span class="string">"proxysql"</span>, <span class="string">"proxypassword"</span>,</span><br><span class="line">    <span class="string">"testdb"</span>, <span class="number">0</span>, <span class="literal">NULL</span>, <span class="number">0</span>) == <span class="literal">NULL</span>)</span><br><span class="line">  &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"DROP TABLE IF EXISTS cars;"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"DROP TABLE IF EXISTS people;"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"CREATE TABLE cars(id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(255), price INT)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;    </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"CREATE TABLE people(id INT, name VARCHAR(255), car_id INT)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;    </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO cars VALUES(1,'Audi',52642)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO cars VALUES(2,'Mercedes',57127)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO cars VALUES(3,'Skoda',9000)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO cars VALUES(4,'Volvo',29000)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO cars VALUES(5,'Bentley',350000)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO cars VALUES(6,'Citroen',21000)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO cars VALUES(7,'Hummer',41400)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO cars VALUES(8,'Volkswagen',21600)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;    </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO people VALUES(1,'Jim',7)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;  </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO people VALUES(1,'Jim',8)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;    </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"INSERT INTO people VALUES(2,'Tom',6)"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"SELECT * FROM cars"</span>))</span><br><span class="line">  &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  MYSQL_RES *result = mysql_store_result(con);</span><br><span class="line">  <span class="keyword">if</span> (result == <span class="literal">NULL</span>)</span><br><span class="line">  &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">int</span> num_fields = mysql_num_fields(result);</span><br><span class="line">  MYSQL_ROW row;</span><br><span class="line">  <span class="keyword">while</span> ((row = mysql_fetch_row(result)))</span><br><span class="line">  &#123;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_fields; i++)</span><br><span class="line">      &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%s "</span>, row[i] ? row[i] : <span class="string">"NULL"</span>);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  mysql_free_result(result);</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"update cars set price = 42400 where name = 'Hummer'"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;   </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"SELECT people.name,cars.name,cars.price FROM cars,people where cars.id = people.car_id"</span>))</span><br><span class="line">  &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  result = mysql_store_result(con);</span><br><span class="line">  <span class="keyword">if</span> (result == <span class="literal">NULL</span>)</span><br><span class="line">  &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  num_fields = mysql_num_fields(result);</span><br><span class="line">  <span class="keyword">while</span> ((row = mysql_fetch_row(result)))</span><br><span class="line">  &#123;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_fields; i++)</span><br><span class="line">      &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%s "</span>, row[i] ? row[i] : <span class="string">"NULL"</span>);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">  &#125;  </span><br><span class="line">  mysql_free_result(result);</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"delete from people where id = 1"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"drop table people;"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;  </span><br><span class="line">  <span class="keyword">if</span> (mysql_query(con, <span class="string">"drop table cars;"</span>)) &#123;</span><br><span class="line">      finish_with_error(con);</span><br><span class="line">  &#125;  </span><br><span class="line">    </span><br><span class="line">  mysql_close(con);</span><br><span class="line">  <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编译并执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gcc testdb.c -o testdb  `mysql_config --cflags --libs`</span><br><span class="line">./testdb</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1 Audi 52642</span><br><span class="line">2 Mercedes 57127</span><br><span class="line">3 Skoda 9000</span><br><span class="line">4 Volvo 29000</span><br><span class="line">5 Bentley 350000</span><br><span class="line">6 Citroen 21000</span><br><span class="line">7 Hummer 41400</span><br><span class="line">8 Volkswagen 21600</span><br><span class="line">Jim Hummer 42400</span><br><span class="line">Jim Volkswagen 21600</span><br><span class="line">Tom Citroen 21000</span><br></pre></td></tr></table></figure><p>根据以上测试可以发现，C语言连接MariaDB数据库，增删改查，创建表，删除表，等各项功能在arm64平台上都正常。</p><p>以上测试只是演示了从OS的软件源安装MariaDB的Connector，你如果想用最新版本的Connector，也可以参考官方文档，编译一个最新版本的:  <a href="https://mariadb.com/kb/en/mariadb-connector-c/">https://mariadb.com/kb/en/mariadb-connector-c/</a></p><p>另外MariaDB的C连接器项目是LGPL的License发布的，以动态链接库的方式链接，可以放心使用。</p><p><strong>Java</strong></p><p>首先要确保在测试虚拟机上已经安装了最新版本的OpenJDK和Maven。</p><p>用Maven创建一个样例项目：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mvn archetype:generate -DgroupId=com.mycompany.app -DartifactId=my-app </span><br><span class="line">-DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false</span><br></pre></td></tr></table></figure><p>然后在生成的<code>pom.xml</code>中的dependencies模块，添加如下内容：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mariadb.jdbc<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mariadb-java-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后编辑<code>src/main/java/com/mycompany/app</code>目录下的<code>App.java</code>文件，内容如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.mycompany.app;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String[] args )</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> (Connection conn = DriverManager.getConnection(<span class="string">"jdbc:mariadb://192.168.0.204/"</span>, <span class="string">"proxysql"</span>, <span class="string">"proxypassword"</span>)) &#123;</span><br><span class="line">      <span class="comment">// create a Statement</span></span><br><span class="line">      <span class="keyword">try</span> (Statement stmt = conn.createStatement()) &#123;</span><br><span class="line">        <span class="comment">//execute query</span></span><br><span class="line">        <span class="keyword">try</span> (ResultSet rs = stmt.executeQuery(<span class="string">"SELECT 'Hello World!'"</span>)) &#123;</span><br><span class="line">          <span class="comment">//position result to first</span></span><br><span class="line">          rs.first();</span><br><span class="line">          System.out.println(rs.getString(<span class="number">1</span>)); <span class="comment">//result is "Hello World!"</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后执行如下命令编译</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn install</span><br></pre></td></tr></table></figure><p>执行如下命令执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn exec:java -Dexec.mainClass="com.mycompany.app.App"</span><br></pre></td></tr></table></figure><p>输出<code>Hello Word!</code>代表正常</p><p>上面仅仅是测试了比较简单的功能，下面我们测试下连接池功能，并执行下增删改查等功能。</p><p>修改<code>App.java</code>的内容如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.mycompany.app;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String[] args )</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">    <span class="comment">//option "pool" must be set to indicate that pool has to be used</span></span><br><span class="line">    String connectionString = <span class="string">"jdbc:mariadb://192.168.0.204/testdb?user=proxysql&amp;password=proxypassword&amp;maxPoolSize=10&amp;pool"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> (Connection connection = DriverManager.getConnection(connectionString)) &#123;</span><br><span class="line">      <span class="keyword">try</span> (Statement stmt = connection.createStatement()) &#123;</span><br><span class="line">        ResultSet rs = stmt.executeQuery(<span class="string">"DROP TABLE IF EXISTS cars;"</span>);</span><br><span class="line">        stmt.executeQuery(<span class="string">"CREATE TABLE cars(id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(255), price INT)"</span>);</span><br><span class="line">        stmt.executeQuery(<span class="string">"INSERT INTO cars VALUES(1,'Audi',52642)"</span>);</span><br><span class="line">        stmt.executeQuery(<span class="string">"INSERT INTO cars VALUES(2,'Mercedes',57127)"</span>);</span><br><span class="line">        rs = stmt.executeQuery(<span class="string">"SELECT * FROM cars"</span>);</span><br><span class="line">        rs.next();</span><br><span class="line">        System.out.println(rs.getString(<span class="number">2</span>));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> (Connection connection = DriverManager.getConnection(connectionString)) &#123;</span><br><span class="line">      <span class="keyword">try</span> (Statement stmt = connection.createStatement()) &#123;</span><br><span class="line">        stmt.executeQuery(<span class="string">"update cars set name = 'VolksWagen' where id = 1"</span>);</span><br><span class="line">        stmt.executeQuery(<span class="string">"delete from cars where id = 2"</span>);</span><br><span class="line">        ResultSet rs = stmt.executeQuery(<span class="string">"SELECT * FROM cars"</span>);</span><br><span class="line">        rs.next();</span><br><span class="line">        System.out.println(rs.getString(<span class="number">2</span>));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编译并执行，成功输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Audi</span><br><span class="line">VolksWagen</span><br></pre></td></tr></table></figure><p>经过以上测试，连接池功能和增删改查功能也都正常。</p><p>Java的连接器功能比较丰富，还实现了对集群或者主从数据库的负载均衡或者读写分离功能。接下来我们测试下读写分离功能。 在此之前请确保已经搭建好了MariaDB的主从数据库环境。</p><p>将<code>App.java</code>的代码改成如下所示，注意jdbc连接串里面增加了replication关键字，代表主从复制，如果是其他负载均衡环境，也支持loadbalance等关键字，具体参考MariaDB官方文档。另外也注意下连接串中，主数据库地址在前，从数据库地址在后。当connection的属性改成了ReadOnly之后，语句就会到备库去查询了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.mycompany.app;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String[] args )</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">        String connectionString = <span class="string">"jdbc:mysql:replication://192.168.0.204,192.168.0.64/testdb?user=proxysql&amp;password=proxypassword&amp;maxPoolSize=10&amp;pool"</span>;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">try</span> (Connection connection = DriverManager.getConnection(connectionString)) &#123;</span><br><span class="line">            <span class="keyword">try</span> (Statement stmt = connection.createStatement()) &#123;</span><br><span class="line">                stmt.executeQuery(<span class="string">"INSERT INTO cars VALUES(2,'Mercedes',57127)"</span>);</span><br><span class="line">                connection.setReadOnly(<span class="keyword">true</span>);</span><br><span class="line">                ResultSet rs = stmt.executeQuery(<span class="string">"SELECT * FROM cars"</span>);</span><br><span class="line">                rs.next();</span><br><span class="line">                System.out.println(rs.getString(<span class="number">2</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编译并执行，输出如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">VolksWagen</span><br></pre></td></tr></table></figure><p>执行成功。</p><p>以上都是以Maven为例演示的相关功能，如果没使用Maven，也可以参考MariaDB官方文档里面的其他工具的指导：<a href="https://mariadb.com/kb/en/mariadb-connector-j/">https://mariadb.com/kb/en/mariadb-connector-j/</a></p><p>MariaDB Java连接器也是以LGPL协议发布的，可以放心使用。</p><p><strong>Python</strong></p><p>首先确保测试虚拟机上已经安装好了Python3和pip3。</p><p>在Python里面连接MariaDB需要先安装C语言的连接器，可以执行如下命令安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt install libmariadb-dev</span><br></pre></td></tr></table></figure><p>不过当前Ubuntu18.04的版本自带的mariadb的包版本较老，不能满足python的版本要求，我们从源码编译一个。</p><p>执行如下命令下载连接器源码，并解压</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://downloads.mariadb.org/interstitial/connector-c-3.1.11/mariadb-connector-c-3.1.11-src.zip</span><br><span class="line">unzip mariadb-connector-c-3.1.11-src.zip</span><br></pre></td></tr></table></figure><p>开始编译</p><p>新建一个单独的编译目录，在这个目录下进行编译，并安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir build-mariadb-connector-c</span><br><span class="line">cd build-mariadb-connector-c</span><br><span class="line">cmake ../mariadb-connector-c-3.1.11-src -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/usr/local</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line">echo "/usr/local/lib/mariadb/" &gt; /etc/ld.so.conf.d/mariadb.conf</span><br><span class="line">ldconfig</span><br></pre></td></tr></table></figure><p>然后执行如下命令安装Python的连接器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install mariadb</span><br></pre></td></tr></table></figure><p>接下来我们直接测试一下连接池功能，并简单测试下查询和新增的功能。</p><p>编辑如下代码，命名为<code>testmariadb.py</code>文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mariadb</span><br><span class="line"></span><br><span class="line">pool = mariadb.ConnectionPool(</span><br><span class="line">        user=<span class="string">"proxysql"</span>,</span><br><span class="line">        password=<span class="string">"proxypassword"</span>,</span><br><span class="line">        host=<span class="string">"192.168.0.204"</span>,</span><br><span class="line">        port=<span class="number">3306</span>,</span><br><span class="line">        pool_name=<span class="string">"web-app"</span>,</span><br><span class="line">        pool_size=<span class="number">20</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    pconn = pool.get_connection()</span><br><span class="line">    cur = pconn.cursor()</span><br><span class="line">    data = [(<span class="number">3</span>, <span class="string">'Skoda'</span>, <span class="number">9000</span>),(<span class="number">4</span>, <span class="string">'Volvo'</span>, <span class="number">29000</span>),(<span class="number">5</span>, <span class="string">'Bently'</span>, <span class="number">350000</span>)]</span><br><span class="line">    cur.executemany(<span class="string">"INSERT INTO testdb.cars(id, name, price) VALUES (?, ?, ?)"</span>, data)</span><br><span class="line">    cur.execute(<span class="string">"select * from testdb.cars"</span>)</span><br><span class="line">    cars = []</span><br><span class="line">    <span class="keyword">for</span> (id,name,price) <span class="keyword">in</span> cur:</span><br><span class="line">        cars.append(<span class="string">f"<span class="subst">&#123;id&#125;</span> <span class="subst">&#123;name&#125;</span> <span class="subst">&#123;price&#125;</span>"</span>)</span><br><span class="line">    print(<span class="string">"\n"</span>.join(cars))</span><br><span class="line"><span class="keyword">except</span> mariadb.PoolError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># Report Error</span></span><br><span class="line">    print(<span class="string">f"Error opening connection from pool: <span class="subst">&#123;e&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><p>执行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 testmariadb.py</span><br></pre></td></tr></table></figure><p>输出如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1 VolksWagen 52642</span><br><span class="line">2 Mercedes 57127</span><br><span class="line">3 Skoda 9000</span><br><span class="line">4 Volvo 29000</span><br><span class="line">5 Bently 350000</span><br></pre></td></tr></table></figure><p>说明成功执行。</p><p>关于Python连接器的其他功能，可以参考如下链接：<a href="https://mariadb.com/docs/appdev/connector-python/">https://mariadb.com/docs/appdev/connector-python/</a></p><p>Python连接器也是LGPL协议，可以放心使用。</p><p><strong>Node.js</strong></p><p>我们首先要先安装nodejs和npm， mariadb连接器要求至少nodejs为10.13以上版本，所以我们从官网下载一个arm64的二进制包，手动安装下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget https://nodejs.org/dist/v14.15.0/node-v14.15.0-linux-arm64.tar.xz</span><br><span class="line">xz -d node-v14.15.0-linux-arm64.tar.xz</span><br><span class="line">tar -xf node-v14.15.0-linux-arm64.tar</span><br><span class="line">cd node-v14.15.0-linux-arm64/bin</span><br><span class="line">sudo ln -s `pwd`/node /usr/local/bin/</span><br><span class="line">sudo ln -s `pwd`/npm /usr/local/bin/</span><br></pre></td></tr></table></figure><p>然后安装mariadb连接器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install mariadb</span><br></pre></td></tr></table></figure><p>然后编辑一个<code>testmariadb.js</code>文件，内容如下：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> mariadb = <span class="built_in">require</span>(<span class="string">'mariadb'</span>);</span><br><span class="line"><span class="keyword">const</span> pool = mariadb.createPool(&#123;</span><br><span class="line">     host:<span class="string">'192.168.0.204'</span>,</span><br><span class="line">     user:<span class="string">'proxysql'</span>,</span><br><span class="line">     password: <span class="string">'proxypassword'</span>,</span><br><span class="line">     connectionLimit: <span class="number">5</span></span><br><span class="line">&#125;);</span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">function</span> <span class="title">asyncFunction</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">let</span> conn;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    conn = <span class="keyword">await</span> pool.getConnection();</span><br><span class="line">    <span class="keyword">const</span> res = <span class="keyword">await</span> conn.query(<span class="string">"INSERT INTO testdb.cars value (?, ?, ?)"</span>, [<span class="number">6</span>,<span class="string">'Citroen'</span>,<span class="number">21000</span>]);</span><br><span class="line">    <span class="built_in">console</span>.log(res);</span><br><span class="line">    <span class="keyword">const</span> rows = <span class="keyword">await</span> conn.query(<span class="string">"SELECT * from testdb.cars"</span>);</span><br><span class="line">    <span class="built_in">console</span>.log(rows);</span><br><span class="line"></span><br><span class="line">  &#125; <span class="keyword">catch</span> (err) &#123;</span><br><span class="line">    <span class="keyword">throw</span> err;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (conn) <span class="keyword">return</span> conn.end();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">asyncFunction();</span><br></pre></td></tr></table></figure><p>然后执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node testmariadb.js</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">OkPacket &#123; affectedRows: 1, insertId: 6, warningStatus: 0 &#125;</span><br><span class="line">[</span><br><span class="line">  &#123; id: 1, name: &#39;VolksWagen&#39;, price: 52642 &#125;,</span><br><span class="line">  &#123; id: 2, name: &#39;Mercedes&#39;, price: 57127 &#125;,</span><br><span class="line">  &#123; id: 3, name: &#39;Skoda&#39;, price: 9000 &#125;,</span><br><span class="line">  &#123; id: 4, name: &#39;Volvo&#39;, price: 29000 &#125;,</span><br><span class="line">  &#123; id: 5, name: &#39;Bently&#39;, price: 350000 &#125;,</span><br><span class="line">  &#123; id: 6, name: &#39;Citroen&#39;, price: 21000 &#125;,</span><br><span class="line">  meta: [</span><br><span class="line">    ColumnDef &#123;</span><br><span class="line">      _parse: [StringParser],</span><br><span class="line">      collation: [Collation],</span><br><span class="line">      columnLength: 11,</span><br><span class="line">      columnType: 3,</span><br><span class="line">      flags: 16899,</span><br><span class="line">      scale: 0,</span><br><span class="line">      type: &#39;LONG&#39;</span><br><span class="line">    &#125;,</span><br><span class="line">    ColumnDef &#123;</span><br><span class="line">      _parse: [StringParser],</span><br><span class="line">      collation: [Collation],</span><br><span class="line">      columnLength: 1020,</span><br><span class="line">      columnType: 253,</span><br><span class="line">      flags: 0,</span><br><span class="line">      scale: 0,</span><br><span class="line">      type: &#39;VAR_STRING&#39;</span><br><span class="line">    &#125;,</span><br><span class="line">    ColumnDef &#123;</span><br><span class="line">      _parse: [StringParser],</span><br><span class="line">      collation: [Collation],</span><br><span class="line">      columnLength: 11,</span><br><span class="line">      columnType: 3,</span><br><span class="line">      flags: 0,</span><br><span class="line">      scale: 0,</span><br><span class="line">      type: &#39;LONG&#39;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>MariaDB Node.js的连接器也是用的LGPL协议，可以放心使用。</p><p>关于Node.js连接器的其他信息，请参考：<a href="https://mariadb.com/kb/en/nodejs-connector/">https://mariadb.com/kb/en/nodejs-connector/</a></p><p><strong>PHP</strong></p><p>PHP连接MariaDB和连接Mysql的代码是一样的。</p><p>首先在测试机上安装php和mysql驱动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install php</span><br><span class="line">sudo apt install php-mysql</span><br></pre></td></tr></table></figure><p>然后编辑一个<code>testmariadb.php</code>文件</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line">$mysqli = <span class="keyword">new</span> mysqli(<span class="string">"192.168.0.204"</span>, <span class="string">"proxysql"</span>, <span class="string">"proxypassword"</span>, <span class="string">"testdb"</span>);</span><br><span class="line">$mysqli-&gt;query(<span class="string">"insert into cars values(7,'Hummer',41400)"</span>);</span><br><span class="line">$result = $mysqli-&gt;query(<span class="string">"SELECT * FROM cars where id = 7"</span>);</span><br><span class="line">$row = $result-&gt;fetch_assoc();</span><br><span class="line"><span class="keyword">echo</span> htmlentities($row[<span class="string">'name'</span>]);</span><br><span class="line"><span class="meta">?&gt;</span></span><br></pre></td></tr></table></figure><p>执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">php testmariadb.php</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hummer</span><br></pre></td></tr></table></figure><p>对于PHP，我们仅测试了插入和查询两个简单的场景，证明PHP在arm64平台上连接MariaDB数据库可行。关于PHP连接mysql和mariadb数据库的其他信息，可以参考如下链接：</p><p><a href="https://www.php.net/manual/en/mysql.php">https://www.php.net/manual/en/mysql.php</a></p><p><strong>Go</strong></p><p>Go语言连接MariaDB数据库和连接Mysql数据库也是一样的。</p><p>首先安装Go语言的Mysql驱动, （这个驱动的协议是MPL2.0，也是一个比较宽松的协议）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go get -u github.com/go-sql-driver/mysql</span><br></pre></td></tr></table></figure><p>编辑<code>testmariadb.go</code>文件，内容如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">"fmt"</span></span><br><span class="line"><span class="keyword">import</span> <span class="string">"strings"</span></span><br><span class="line"><span class="keyword">import</span> <span class="string">"database/sql"</span></span><br><span class="line"><span class="keyword">import</span> _ <span class="string">"github.com/go-sql-driver/mysql"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">const</span> (</span><br><span class="line">        userName = <span class="string">"proxysql"</span></span><br><span class="line">        password = <span class="string">"proxypassword"</span></span><br><span class="line">        ip = <span class="string">"192.168.0.204"</span></span><br><span class="line">        port = <span class="string">"3306"</span></span><br><span class="line">        dbName = <span class="string">"testdb"</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//Db数据库连接池</span></span><br><span class="line">    <span class="keyword">var</span> DB *sql.DB</span><br><span class="line">    path := strings.Join([]<span class="keyword">string</span>&#123;userName, <span class="string">":"</span>, password, <span class="string">"@tcp("</span>,ip, <span class="string">":"</span>, port, <span class="string">")/"</span>, dbName, <span class="string">"?charset=utf8"</span>&#125;, <span class="string">""</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//打开数据库,前者是驱动名，所以要导入： _ "github.com/go-sql-driver/mysql"</span></span><br><span class="line">    DB, _ = sql.Open(<span class="string">"mysql"</span>, path)</span><br><span class="line">    <span class="comment">//设置数据库最大连接数</span></span><br><span class="line">    DB.SetConnMaxLifetime(<span class="number">100</span>)</span><br><span class="line">    <span class="comment">//设置上数据库最大闲置连接数</span></span><br><span class="line">    DB.SetMaxIdleConns(<span class="number">10</span>)</span><br><span class="line">    <span class="comment">//验证连接</span></span><br><span class="line">    <span class="keyword">if</span> err := DB.Ping(); err != <span class="literal">nil</span>&#123;</span><br><span class="line">        fmt.Println(<span class="string">"open database fail"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    fmt.Println(<span class="string">"connnect success"</span>)</span><br><span class="line">    <span class="comment">//开启事务</span></span><br><span class="line">    tx, err := DB.Begin()</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span>&#123;</span><br><span class="line">        fmt.Println(<span class="string">"tx fail"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//准备sql语句</span></span><br><span class="line">    stmt, err := tx.Prepare(<span class="string">"INSERT INTO cars VALUES (?,?,?)"</span>)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span>&#123;</span><br><span class="line">        fmt.Println(<span class="string">"Prepare fail"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//将参数传递到sql语句中并且执行</span></span><br><span class="line">    res, err := stmt.Exec(<span class="number">8</span>, <span class="string">"Mercedes"</span>, <span class="number">57127</span>)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span>&#123;</span><br><span class="line">        fmt.Println(<span class="string">"Exec fail"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//将事务提交</span></span><br><span class="line">    tx.Commit()</span><br><span class="line">    fmt.Println(res.LastInsertId())</span><br><span class="line"></span><br><span class="line">    rows, err := DB.Query(<span class="string">"SELECT * from cars"</span>)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span>&#123;</span><br><span class="line">        fmt.Println(<span class="string">"query fail"</span>)</span><br><span class="line">    &#125;    </span><br><span class="line">    <span class="comment">//循环读取结果</span></span><br><span class="line">    <span class="keyword">for</span> rows.Next()&#123;</span><br><span class="line">        <span class="keyword">var</span> id <span class="keyword">int</span></span><br><span class="line">        <span class="keyword">var</span> name <span class="keyword">string</span></span><br><span class="line">        <span class="keyword">var</span> price <span class="keyword">int</span></span><br><span class="line">        err := rows.Scan(&amp;id, &amp;name, &amp;price)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            fmt.Println(<span class="string">"rows fail"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        fmt.Printf(<span class="string">"%v %q %v \n"</span>, id, name, price)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go run testmariadb.go</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">connnect success</span><br><span class="line">8 &lt;nil&gt;</span><br><span class="line">1 &quot;VolksWagen&quot; 52642</span><br><span class="line">2 &quot;Mercedes&quot; 57127</span><br><span class="line">3 &quot;Skoda&quot; 9000</span><br><span class="line">4 &quot;Volvo&quot; 29000</span><br><span class="line">5 &quot;Bently&quot; 350000</span><br><span class="line">6 &quot;Citroen&quot; 21000</span><br><span class="line">7 &quot;Hummer&quot; 41400</span><br><span class="line">8 &quot;Mercedes&quot; 57127</span><br></pre></td></tr></table></figure><p>上面我们测试了Go语言的MariaDB连接器的连接池，查询，事务，插入等功能，都是成功的。 关于其他功能，大家可以参考go语言的官网或者其他相关网站。</p><p><strong>Rust</strong></p><p>Rust连接MariaDB的方式和连接Mysql也是一样的。</p><p>首先安装最新版本的Rust：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl --proto '=https' --tlsv1.2 https://sh.rustup.rs -sSf | sh</span><br></pre></td></tr></table></figure><p>安装完后，需要将<code>$HOME/.cargo/bin</code>添加到PATH环境变量里</p><p>对于当前环境，我们执行如下命令先临时生效：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source $HOME/.cargo/env</span><br></pre></td></tr></table></figure><p>然后执行如下命令，创建一个新工程：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cargo new mariadb_test</span><br></pre></td></tr></table></figure><p>然后</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd mariadb_test</span><br></pre></td></tr></table></figure><p>编辑<code>Cargo.toml</code>配置文件，在<code>[dependencies]</code>下面添加如下内容, 设置驱动的版本：</p><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mysql</span> = <span class="string">"20.0.1"</span></span><br></pre></td></tr></table></figure><p>然后编辑<code>src/main.rs</code>文件：</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> mysql::*;</span><br><span class="line"><span class="keyword">use</span> mysql::prelude::*;</span><br><span class="line"></span><br><span class="line"><span class="meta">#[derive(Debug, PartialEq, Eq)]</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Car</span></span> &#123;</span><br><span class="line">    id: <span class="built_in">i32</span>,</span><br><span class="line">    name: <span class="built_in">String</span>,</span><br><span class="line">    price: <span class="built_in">i32</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">testmariadb</span></span>() -&gt; std::result::<span class="built_in">Result</span>&lt;std::string::<span class="built_in">String</span>, mysql::Error&gt; &#123;</span><br><span class="line">    <span class="keyword">let</span> url = <span class="string">"mysql://proxysql:proxypassword@192.168.0.204:3306/testdb"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">let</span> pool = Pool::new(url)?;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut</span> conn = pool.get_conn()?;</span><br><span class="line">    </span><br><span class="line">    conn.exec_drop(<span class="string">r"delete from cars"</span>,())?;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">let</span> cars = <span class="built_in">vec!</span>[</span><br><span class="line">        Car &#123; id: <span class="number">1</span>, name: <span class="string">"Audi"</span>.into(), price: <span class="number">52642</span> &#125;,</span><br><span class="line">        Car &#123; id: <span class="number">2</span>, name: <span class="string">"Volkswagen"</span>.into(), price: <span class="number">21600</span> &#125;,</span><br><span class="line">        Car &#123; id: <span class="number">3</span>, name: <span class="string">"Skoda"</span>.into(), price: <span class="number">9000</span> &#125;,</span><br><span class="line">    ];</span><br><span class="line">    conn.exec_batch(</span><br><span class="line">        r<span class="string">"INSERT INTO cars (id, name, price)</span></span><br><span class="line"><span class="string">      VALUES (:id, :name, :price)"</span>,</span><br><span class="line">      cars.iter().map(|p| params! &#123;</span><br><span class="line">          <span class="string">"id"</span> =&gt; p.id,</span><br><span class="line">          <span class="string">"name"</span> =&gt; &amp;p.name,</span><br><span class="line">          <span class="string">"price"</span> =&gt; p.price,</span><br><span class="line">      &#125;)</span><br><span class="line">      )?;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">let</span> selected_cars = conn</span><br><span class="line">        .query_map(</span><br><span class="line">            <span class="string">"SELECT id, name, price from cars"</span>,</span><br><span class="line">            |(id, name, price)| &#123;</span><br><span class="line">                Car &#123; id, name, price &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            )?;</span><br><span class="line">    <span class="built_in">assert_eq!</span>(cars, selected_cars);</span><br><span class="line">    <span class="built_in">println!</span>(<span class="string">"Yay!"</span>);</span><br><span class="line">    <span class="literal">Ok</span>(<span class="string">"Yay!"</span>.into())</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">main</span></span>() &#123;</span><br><span class="line">    testmariadb();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们测试了连接池功能，还测试了，删除，插入，查询等功能，在一个批量的查询之后，将查询出来的结果放到一个列表里面，和插入之前的数据列表进行对比，如果相等，则打印<code>Yay!</code></p><p>在<code>mariadb_test</code>目录下，执行如下命令编译加运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cargo run</span><br></pre></td></tr></table></figure><p>如果执行因为update crates.io-index超时，需要更改一下下载地址，进行如下配置</p><p>编辑文件: <code>vi $HOME/.cargo/config</code> </p><p>输入如下内容，保存退出。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[source.crates-io]</span><br><span class="line">registry &#x3D; &quot;https:&#x2F;&#x2F;github.com&#x2F;rust-lang&#x2F;crates.io-index&quot;</span><br><span class="line">replace-with &#x3D; &#39;ustc&#39;</span><br><span class="line">[source.ustc]</span><br><span class="line">registry &#x3D; &quot;git:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;crates.io-index&quot;</span><br></pre></td></tr></table></figure><p>然后重新<code>cargo run</code></p><p>输出<code>Yay!</code>，成功运行。</p><p>关于Rust Mysql连接器的其他功能，请参考：</p><p><a href="https://docs.rs/mysql/20.0.1/mysql/">https://docs.rs/mysql/20.0.1/mysql/</a></p><p>另外Rust还有一个比较新的异步连接器，具体可以参考下面这两个链接：</p><p><a href="https://docs.rs/mysql_async/0.25.0/mysql_async/">https://docs.rs/mysql_async/0.25.0/mysql_async/</a></p><p><a href="https://github.com/blackbeam/mysql_async">https://github.com/blackbeam/mysql_async</a></p><p>Rust的连接器使用的都是MIT/Apache2.0协议，这两个都是比较宽松的协议。</p><p><strong>总结</strong></p><p>我们测试了C，Java，Node.js， Python，PHP，Go，Rust多种主流语言，都可以在arm64平台上成功的连接MariaDB数据库，而且用法和x86平台都是一样的。而且各个语言连接器的开源协议也都比较宽松。</p><p><strong>参考链接：</strong></p><p><a href="http://zetcode.com/db/mysqlc/">http://zetcode.com/db/mysqlc/</a></p><p><a href="https://mariadb.com/kb/en/mariadb-connector-c/">https://mariadb.com/kb/en/mariadb-connector-c/</a></p><p><a href="https://mariadb.com/kb/en/mariadb-connector-j/">https://mariadb.com/kb/en/mariadb-connector-j/</a></p><p><a href="https://mariadb.com/docs/appdev/connector-python/">https://mariadb.com/docs/appdev/connector-python/</a></p><p><a href="https://mariadb.com/kb/en/nodejs-connector/">https://mariadb.com/kb/en/nodejs-connector/</a></p><p><a href="https://www.php.net/manual/en/mysql.php">https://www.php.net/manual/en/mysql.php</a></p><p><a href="https://www.jianshu.com/p/ee87e989f149">https://www.jianshu.com/p/ee87e989f149</a></p><p><a href="https://golang.org/pkg/database/sql">https://golang.org/pkg/database/sql</a></p><p><a href="https://github.com/go-sql-driver/mysql">https://github.com/go-sql-driver/mysql</a></p><p><a href="https://doc.rust-lang.org/book/title-page.html">https://doc.rust-lang.org/book/title-page.html</a></p><p><a href="https://docs.rs/mysql/20.0.1/mysql/">https://docs.rs/mysql/20.0.1/mysql/</a></p><p><a href="https://docs.rs/mysql_async/0.25.0/mysql_async/">https://docs.rs/mysql_async/0.25.0/mysql_async/</a></p><p><a href="https://github.com/blackbeam/mysql_async">https://github.com/blackbeam/mysql_async</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: zhaorenhai&lt;/p&gt;
&lt;p&gt;本文尝试探索下几种主流的开发语言如何在arm64平台上连接MariaDB数据库，并测试一下增删改查各项功能是否都正常。主要包括如下几种语言：C , Java, Node.js, Python, Go, Rust, PHP&lt;/p&gt;
&lt;p&gt;测试平台采用华为云的鲲鹏虚拟机，OS采用Ubuntu18.04。 并提前已经在同内网内的另外一台虚拟机上已经部署好了一个MariaDB数据库，并建好了供远程连接的数据库用户名和密码, 这里假设我们新建的用户名为proxysql，密码为proxypassword，端口号就用默认的3306端口号。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Mysql Innodb cluster on ARM64概述</title>
    <link href="https://kunpengcompute.github.io/2020/11/11/mysql-innodb-cluster-on-arm64-gai-shu/"/>
    <id>https://kunpengcompute.github.io/2020/11/11/mysql-innodb-cluster-on-arm64-gai-shu/</id>
    <published>2020-11-11T06:18:02.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者: <a href="https://github.com/wangxiyuan">wangxiyuan</a></p><p>本文介绍Mysql 8.0的Innodb cluster架构，以及在arm64上的部署流程</p><a id="more"></a><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Mysql Innodb cluster是Mysql 在HA场景下推荐的一种部署模型。支持多节点集群部署，保证Mysql的高可用性。需要配套Mysql Router使用，并建议使用Mysql shell进行部署。</p><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p><img src="https://user-images.githubusercontent.com/10891919/98775479-3025af80-2428-11eb-880a-c3a663a0a4f2.png" alt="innodb_cluster_overview"></p><p>如图所以，一个Mysql Innodb cluster主要由三个部分组成：</p><ol><li><p>Mysql HA集群。集群中包含多个Mysql服务，每个服务使用Innodb后端，并配置Group Replication。</p></li><li><p>Mysql Router。提供前端Load Balance能力。</p></li><li><p>Mysql Shell以及其他Client。前端客户端，用来部署、使用Cluster。</p></li></ol><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><p>本文使用源码编译的方式部署。</p><h3 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h3><ol><li><p>3节点ARM64环境。</p></li><li><p>Mysql使用Innodb存储引擎，并开启了Group Replication功能。</p></li><li><p>Mysql Shell依赖Python。需要安装python3和python3-dev</p></li><li><p>必要的C/C++编译工具，例如gcc、gcc-c++、autoconf、cmake、make等等，不再赘述。</p></li></ol><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><ol><li><p>Mysql Server</p><p> Mysql Server 8.0编译很简单。</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;mysql&#x2F;mysql-server</span><br><span class="line">mkdir mysql-server&#x2F;bld</span><br><span class="line">cd mysql-server&#x2F;bld</span><br><span class="line">cmake .. -DDOWNLOAD_BOOST&#x3D;1 -DWITH_BOOST&#x3D;&#123;boost download folder&#125;</span><br><span class="line">make -j8</span><br></pre></td></tr></table></figure></li><li><p>Mysql-shell</p><p> Mysql官方没提供arm64的安装包，需要手动编译。</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https:&#x2F;&#x2F;github.com&#x2F;mysql&#x2F;mysql-shell</span><br><span class="line">$ mkdir mysql-shell&#x2F;bld</span><br><span class="line">$ cd mysql-shell&#x2F;bld</span><br><span class="line">$ cmake .. -DDOWNLOAD_BOOST&#x3D;1 -DENABLE_DOWNLOADS&#x3D;1 -DMYSQL_SOURCE_DIR&#x3D;&#x2F;opt&#x2F;mysql-server -DMYSQL_BUILD_DIR&#x3D;&#x2F;opt&#x2F;mysql-server&#x2F;bld -DHAVE_PYTHON&#x3D;1</span><br><span class="line">$ make -j4</span><br></pre></td></tr></table></figure><p> 这里有个问题：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Mysql-shell依赖protobuf和Mysql server，而Mysql server也依赖protobuf。这两个protobuf要版本一致。不一致的话Mysql-shell无法编译通过</span><br><span class="line"></span><br><span class="line">但是Mysql server在代码中内置了自己fork的非官方protobuf。在默认编译参数下，Mysql Server会使用这个protobuf。</span><br><span class="line"></span><br><span class="line">因此这里有两个解决方法:</span><br><span class="line"></span><br><span class="line">1. 统一使用Mysql server的非官方protobuf，注意这里的protobuf必须是静态编译的（即有libprotobuf.a文件，而不是.so），这是Mysql-shell的依赖要求。但Mysql8.0默认动态编译protobuf,因此在编译Mysql需要给cmake命令添加 &#96;-Dprotobuf_BUILD_SHARED_LIBS&#x3D;OFF&#96;参数。</span><br><span class="line"></span><br><span class="line">2. 统一使用官方protobuf，提前安装好protobuf，并在编译Mysql server时指定 &#96;-DWITH_PROTOBUF&#x3D;system&#96;，编译Mysql-shell时指定&#96;-DProtobuf_INCLUDE_DIR&#96;</span><br></pre></td></tr></table></figure></li><li><p>Mysql Router</p><p>Mysql Router官方也没有提供arm64安装包，也需要手动编译。但自Mysql 8.0以后，Mysql Router的源码已经合并到Mysql server中，因此编译Mysql server后，自带了Mysql Router。</p></li></ol><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>官方提供了两种部署方式：</p><ol><li><p>沙盒（测试）环境。用户可以在单节点上体验、测试Innodb Cluster。</p></li><li><p>生产环境。多节点部署Innodb Cluster</p></li></ol><p>本文先使用沙盒模式体验一下Innodb Cluster，然后再部署生产环境并测试Innodb Cluster。</p><h4 id="沙盒环境"><a href="#沙盒环境" class="headerlink" title="沙盒环境"></a>沙盒环境</h4><ol><li><p>进入mysql-shell的bld/bin目录，执行<code>./mysqlsh</code>。mysql-shell支持多种语言的API，我们在前面章节编译的mysql-shell使用的是python，因此执行./mysqlsh后，可以看到<code>mysql-py &gt;</code>这样的命令行提示符。Mysql官方文档使用的是mysql-js，与本文不同。以下的操作和命令都在mysql-py命令行中执行。</p></li><li><p>创建sandbox Instance</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dba.deploy_sandbox_instance(3310)</span><br></pre></td></tr></table></figure><p> 根据提示创建密码后报错，找不到mysql，这里需要把之前编译要的Mysq server加入到PATH中</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PATH&#x3D;$PATH:&#x2F;opt&#x2F;mysql-server&#x2F;bld&#x2F;bin&#x2F;</span><br></pre></td></tr></table></figure><p> 然后重复执行创建命令，成功后显示如下：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mysql-py&gt; dba.deploy_sandbox_instance(3310)</span><br><span class="line">A new MySQL sandbox instance will be created on this host in</span><br><span class="line">&#x2F;root&#x2F;mysql-sandboxes&#x2F;3310</span><br><span class="line"></span><br><span class="line">Warning: Sandbox instances are only suitable for deploying and</span><br><span class="line">running on your local machine for testing purposes and are not</span><br><span class="line">accessible from external networks.</span><br><span class="line"></span><br><span class="line">Please enter a MySQL root password for the new instance: ****</span><br><span class="line"></span><br><span class="line">Deploying new MySQL instance...</span><br><span class="line"></span><br><span class="line">Instance localhost:3310 successfully deployed and started.</span><br><span class="line">Use shell.connect(&#39;root@localhost:3310&#39;) to connect to the instance.</span><br></pre></td></tr></table></figure><p> 此时在<code>$HOME</code>目录下生成了<code>/mysql-sandboxes/3310</code>目录。并且拉起了两个如下进程。</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;mysqld --user mysql</span><br><span class="line">&#x2F;root&#x2F;mysql-sandboxes&#x2F;3310&#x2F;bin&#x2F;mysqld --defaults-file&#x3D;&#x2F;root&#x2F;mysql-sandboxes&#x2F;3310&#x2F;my.cnf --user&#x3D;root</span><br></pre></td></tr></table></figure><p> 然后在拉起两个Instance，组成Cluster</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dba.deploy_sandbox_instance(3320)</span><br><span class="line">dba.deploy_sandbox_instance(3330)</span><br></pre></td></tr></table></figure><p> 查看进程，发现多了两个sandbox进程，分别对应新创的Instance。</p></li><li><p>组成Cluster</p><p> 3个Mysql Instance创建成功后，我们把他们配置成Cluster。登录Instance并创建cluster。</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">shell.connect(&#39;root@localhost:3310&#39;)</span><br><span class="line"></span><br><span class="line">cluster&#x3D;dba.create_cluster(&#39;testCluster&#39;)</span><br></pre></td></tr></table></figure><p> 报错</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">SystemError: RuntimeError: Dba.create_cluster: error installing plugin &#39;group_replication&#39;: 127.0.0.1:3310: Can&#39;t open shared library &#39;&#x2F;opt&#x2F;mysql-server&#x2F;bld&#x2F;lib&#x2F;plugin&#x2F;group_replication.so&#39; (errno: 0 &#x2F;opt&#x2F;mysql-server&#x2F;bld&#x2F;lib&#x2F;plugin&#x2F;group_replication.so: cannot open shared object file: No such file or directory)</span><br></pre></td></tr></table></figure><p> 这是mysql编译后直接只用bld目录的问题，手动创建该软连接。</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ln -s  &#x2F;opt&#x2F;mysql-server&#x2F;bld&#x2F;plugin_output_directory&#x2F; &#x2F;opt&#x2F;mysql-server&#x2F;bld&#x2F;lib&#x2F;</span><br><span class="line">$ mv &#x2F;opt&#x2F;mysql-server&#x2F;bld&#x2F;lib&#x2F;plugin_output_directory&#x2F; &#x2F;opt&#x2F;mysql-server&#x2F;bld&#x2F;lib&#x2F;plugin&#x2F;</span><br></pre></td></tr></table></figure><p> 再次执行create命令，成功。然后给Cluster中添加Instance。</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cluster.add_instance(&#39;root@localhost:3320&#39;)</span><br><span class="line">cluster.add_instance(&#39;root@localhost:3330&#39;)</span><br></pre></td></tr></table></figure><p> 至此，sandbox Innodb Cluster部署完成，查询Cluster状态。</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">mysql-py []&gt; cluster.status()</span><br><span class="line">&#123;</span><br><span class="line">    &quot;clusterName&quot;: &quot;testCluster&quot;,</span><br><span class="line">    &quot;defaultReplicaSet&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;default&quot;,</span><br><span class="line">        &quot;primary&quot;: &quot;127.0.0.1:3310&quot;,</span><br><span class="line">        &quot;ssl&quot;: &quot;REQUIRED&quot;,</span><br><span class="line">        &quot;status&quot;: &quot;OK&quot;,</span><br><span class="line">        &quot;statusText&quot;: &quot;Cluster is ONLINE and can tolerate up to ONE failure.&quot;,</span><br><span class="line">        &quot;topology&quot;: &#123;</span><br><span class="line">            &quot;127.0.0.1:3310&quot;: &#123;</span><br><span class="line">                &quot;address&quot;: &quot;127.0.0.1:3310&quot;,</span><br><span class="line">                &quot;mode&quot;: &quot;R&#x2F;W&quot;,</span><br><span class="line">                &quot;readReplicas&quot;: &#123;&#125;,</span><br><span class="line">                &quot;replicationLag&quot;: null,</span><br><span class="line">                &quot;role&quot;: &quot;HA&quot;,</span><br><span class="line">                &quot;status&quot;: &quot;ONLINE&quot;,</span><br><span class="line">                &quot;version&quot;: &quot;8.0.21&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;127.0.0.1:3320&quot;: &#123;</span><br><span class="line">                &quot;address&quot;: &quot;127.0.0.1:3320&quot;,</span><br><span class="line">                &quot;mode&quot;: &quot;R&#x2F;O&quot;,</span><br><span class="line">                &quot;readReplicas&quot;: &#123;&#125;,</span><br><span class="line">                &quot;replicationLag&quot;: null,</span><br><span class="line">                &quot;role&quot;: &quot;HA&quot;,</span><br><span class="line">                &quot;status&quot;: &quot;ONLINE&quot;,</span><br><span class="line">                &quot;version&quot;: &quot;8.0.21&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;127.0.0.1:3330&quot;: &#123;</span><br><span class="line">                &quot;address&quot;: &quot;127.0.0.1:3330&quot;,</span><br><span class="line">                &quot;mode&quot;: &quot;R&#x2F;O&quot;,</span><br><span class="line">                &quot;readReplicas&quot;: &#123;&#125;,</span><br><span class="line">                &quot;replicationLag&quot;: null,</span><br><span class="line">                &quot;role&quot;: &quot;HA&quot;,</span><br><span class="line">                &quot;status&quot;: &quot;ONLINE&quot;,</span><br><span class="line">                &quot;version&quot;: &quot;8.0.21&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;topologyMode&quot;: &quot;Single-Primary&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;groupInformationSourceMember&quot;: &quot;127.0.0.1:3310&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>配置Mysql Router</p><p> Mysql Server 8.0自带了Mysql Router，命令是<code>mysqlrouter</code>，在bld的bin目录下。执行初始化命令：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqlrouter --bootstrap root@localhost:3310 --user&#x3D;root</span><br></pre></td></tr></table></figure><p> 返回如下信息：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">- Creating account(s) (only those that are needed, if any)</span><br><span class="line">- Verifying account (using it to run SQL queries that would be run by Router)</span><br><span class="line">- Storing account in keyring</span><br><span class="line">- Adjusting permissions of generated files</span><br><span class="line">- Creating configuration &#x2F;opt&#x2F;mysql-server&#x2F;bld&#x2F;mysqlrouter.conf</span><br><span class="line"></span><br><span class="line"># MySQL Router configured for the InnoDB Cluster &#39;testCluster&#39;</span><br><span class="line"></span><br><span class="line">After this MySQL Router has been started with the generated configuration</span><br><span class="line"></span><br><span class="line">   $ &#x2F;etc&#x2F;init.d&#x2F;mysqlrouter restart</span><br><span class="line">or</span><br><span class="line">   $ systemctl start mysqlrouter</span><br><span class="line">or</span><br><span class="line">   $ mysqlrouter -c &#x2F;opt&#x2F;mysql-server&#x2F;bld&#x2F;mysqlrouter.conf</span><br><span class="line"></span><br><span class="line">the cluster &#39;testCluster&#39; can be reached by connecting to:</span><br><span class="line"></span><br><span class="line">## MySQL Classic protocol</span><br><span class="line"></span><br><span class="line">- Read&#x2F;Write Connections: localhost:6446</span><br><span class="line">- Read&#x2F;Only Connections:  localhost:6447</span><br><span class="line"></span><br><span class="line">## MySQL X protocol</span><br><span class="line"></span><br><span class="line">- Read&#x2F;Write Connections: localhost:64460</span><br><span class="line">- Read&#x2F;Only Connections:  localhost:64470</span><br></pre></td></tr></table></figure><p> 该命令会在<code>mysql-server/bld/</code>生成<code>mysqlrouter.conf</code>配置文件。</p><p> 根据返回提示信息，启动mysql router：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqlrouter -c &#x2F;opt&#x2F;mysql-server&#x2F;bld&#x2F;mysqlrouter.conf</span><br></pre></td></tr></table></figure><p> 又报错了：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error: Loading plugin for config-section &#39;[metadata_cache:testCluster]&#39; failed: &#x2F;opt&#x2F;mysql-server&#x2F;bld&#x2F;runtime_output_directory&#x2F;..&#x2F;lib&#x2F;mysqlrouter&#x2F;metadata_cache.so: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure><p> 这个错误和之前group_replication.so的问题一样，添加新的软链接即可：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ln -s  &#x2F;opt&#x2F;mysql-server&#x2F;bld&#x2F;plugin_output_directory&#x2F; &#x2F;opt&#x2F;mysql-server&#x2F;bld&#x2F;lib&#x2F;</span><br><span class="line"></span><br><span class="line">$ mv &#x2F;opt&#x2F;mysql-server&#x2F;bld&#x2F;lib&#x2F;plugin_output_directory &#x2F;opt&#x2F;mysql-server&#x2F;bld&#x2F;lib&#x2F;mysqlrouter</span><br></pre></td></tr></table></figure><p> 再次启动，成功。这时就可以通过Mysql Router的6446端口访问Mysql Innodb Cluster了。在Mysql-shell中：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql-py&gt; \connect root@localhost:6446</span><br><span class="line">mysql-py []&gt; \sql</span><br><span class="line">mysql-sql []&gt; select @@port;</span><br><span class="line">+--------+</span><br><span class="line">| @@port |</span><br><span class="line">+--------+</span><br><span class="line">|   3310 |</span><br><span class="line">+--------+</span><br></pre></td></tr></table></figure><p> 成功登入</p></li></ol><h4 id="生产环境"><a href="#生产环境" class="headerlink" title="生产环境"></a>生产环境</h4><p>这里我手里没有3个物理环境，因此采用容器化方式模拟三节点，刚好可以测试一下msyql的容器化能力。</p><h5 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h5><p>提前准备三个<code>my.cnf</code>，命名为<code>my1.cnf</code> <code>my2.cnf</code> <code>my3.cnf</code>, 注意配置项<code>report-host</code>依次为<code>172.8.0.100</code> <code>172.8.0.101</code> <code>172.8.0.102</code>, <code>server_id</code>依次为<code>1</code> <code>2</code> <code>3</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># For advice on how to change settings please see</span><br><span class="line"># http:&#x2F;&#x2F;dev.mysql.com&#x2F;doc&#x2F;refman&#x2F;8.0&#x2F;en&#x2F;server-configuration-defaults.html</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">#</span><br><span class="line"># Remove leading # and set to the amount of RAM for the most important data</span><br><span class="line"># cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.</span><br><span class="line"># innodb_buffer_pool_size &#x3D; 128M</span><br><span class="line">#</span><br><span class="line"># Remove leading # to turn on a very important data integrity option: logging</span><br><span class="line"># changes to the binary log between backups.</span><br><span class="line"># log_bin</span><br><span class="line">#</span><br><span class="line"># Remove leading # to set options mainly useful for reporting servers.</span><br><span class="line"># The server defaults are faster for transactions and fast SELECTs.</span><br><span class="line"># Adjust sizes as needed, experiment to find the optimal values.</span><br><span class="line"># join_buffer_size &#x3D; 128M</span><br><span class="line"># sort_buffer_size &#x3D; 2M</span><br><span class="line"># read_rnd_buffer_size &#x3D; 2M</span><br><span class="line"></span><br><span class="line"># Remove leading # to revert to previous value for default_authentication_plugin,</span><br><span class="line"># this will increase compatibility with older clients. For background, see:</span><br><span class="line"># https:&#x2F;&#x2F;dev.mysql.com&#x2F;doc&#x2F;refman&#x2F;8.0&#x2F;en&#x2F;server-system-variables.html#sysvar_default_authentication_plugin</span><br><span class="line"># default-authentication-plugin&#x3D;mysql_native_password</span><br><span class="line">skip-host-cache</span><br><span class="line">skip-name-resolve</span><br><span class="line">datadir&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql</span><br><span class="line">socket&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql.sock</span><br><span class="line">secure-file-priv&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql-files</span><br><span class="line">user&#x3D;mysql</span><br><span class="line"></span><br><span class="line">pid-file&#x3D;&#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld.pid</span><br><span class="line"></span><br><span class="line">enforce_gtid_consistency&#x3D;ON</span><br><span class="line">gtid_mode&#x3D;ON</span><br><span class="line">report-host &#x3D; 172.8.0.100</span><br><span class="line">server_id&#x3D;1</span><br></pre></td></tr></table></figure><p>再创建自定义docker网络</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker network create --subnet&#x3D;172.18.0.0&#x2F;24 my_bridge</span><br></pre></td></tr></table></figure><h5 id="部署-1"><a href="#部署-1" class="headerlink" title="部署"></a>部署</h5><p>使用docker启动3个mysql容器,我这里准备的<code>my.cnf</code>文件在<code>root</code>目录下.另外，由于在使用mysql-shell部署集群中，需要重启mysql，为了保证容器不退出，这里在mysql2和mysql3上手动执行死循环命令。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">docker</span> <span class="string">run</span> <span class="string">--name=mysql1</span> <span class="string">--network=my_bridge</span> <span class="string">--ip</span> <span class="number">172.18</span><span class="number">.0</span><span class="number">.100</span> <span class="string">-v</span> <span class="string">/root/my1.cnf:/etc/my.cnf</span> <span class="string">-d</span> <span class="string">mysql/mysql-server:8.0.22</span></span><br><span class="line"><span class="string">docker</span> <span class="string">run</span> <span class="string">--name=mysql2</span> <span class="string">--network=my_bridge</span> <span class="string">--ip</span> <span class="number">172.18</span><span class="number">.0</span><span class="number">.101</span> <span class="string">-v</span> <span class="string">/root/my2.cnf:/etc/my.cnf</span> <span class="string">-d</span> <span class="string">mysql/mysql-server:8.0.22</span> <span class="string">/bin/bash</span> <span class="string">-c</span> <span class="string">"while true;do sleep 10;done"</span></span><br><span class="line"><span class="string">docker</span> <span class="string">run</span> <span class="string">--name=mysql3</span> <span class="string">--network=my_bridge</span> <span class="string">--ip</span> <span class="number">172.18</span><span class="number">.0</span><span class="number">.102</span> <span class="string">-v</span> <span class="string">/root/my3.cnf:/etc/my.cnf</span> <span class="string">-d</span> <span class="string">mysql/mysql-server:8.0.22</span> <span class="string">/bin/bash</span> <span class="string">-c</span> <span class="string">"while true;do sleep 10;done"</span></span><br></pre></td></tr></table></figure><p>由于改写了mysql2和3的容器命令，这里我们要手动拉起其中的myql，以mysql2为例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker exec -it mysql2 bash</span><br><span class="line">$ &#x2F;entrypoint.sh mysqld&amp;</span><br><span class="line">$ exit</span><br></pre></td></tr></table></figure><h5 id="初始化配置"><a href="#初始化配置" class="headerlink" title="初始化配置"></a>初始化配置</h5><p>查看mysql初始密码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker logs mysql1 2&gt;&amp;1 | grep GENERATED</span><br><span class="line">mysql2和mysql3的秘密在上一步手动拉起的屏显中</span><br></pre></td></tr></table></figure><p>依次登录mysql容器并配置。以mysql1为例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it mysql1 bash</span><br><span class="line"></span><br><span class="line">mysql -uroot -p&#123;初始密码&#125;</span><br><span class="line"></span><br><span class="line">alter user &#39;root&#39;@&#39;localhost&#39; identified by &#39;root&#39;;</span><br><span class="line">create user &#39;root&#39;@&#39;%&#39; identified by &#39;root&#39;;</span><br><span class="line">grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; with grant option;</span><br></pre></td></tr></table></figure><p>验证，登录mysql：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -h172.18.0.100 -uroot -proot</span><br></pre></td></tr></table></figure><h5 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h5><p>使用mysql-shell进行配置，由于我的环境没有mysql-shell，直接进入mysql1容器使用自带的mysql-shell</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker exec -it mysql1 bash</span><br><span class="line">$ mysqlsh</span><br><span class="line">mysqlsh默认是JS模式，可以使用\py切换到python模式。之前的sandbox章节使用的是python，这是我们就使用JavaScript吧</span><br></pre></td></tr></table></figure><ol><li>检查配置项<br>以此执行<code>dba.checkInstanceConfiguration(&quot;root@{三个容器的IP}:3306&quot;)</code>，返回OK</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;status&quot;: &quot;ok&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>连接主节点，创建集群</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">shell.connect(&#39;root@172.18.0.100:3306&#39;)</span><br><span class="line">var cluster &#x3D; dba.createCluster(&#39;testCluster&#39;)</span><br></pre></td></tr></table></figure><ol start="3"><li>新增节点</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cluster.addInstance(&#39;root@172.18.0.101:3306&#39;)</span><br><span class="line">cluster.addInstance(&#39;root@172.18.0.102:3306&#39;)</span><br></pre></td></tr></table></figure><p>注意，在添加节点过程中，mysql会重启，但mysql-shell会拉起服务失败，需要手动上去拉起，另开一个bash：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker exec -ti mysql2 bash</span><br><span class="line">$ mysqld&amp;</span><br><span class="line">$ exit</span><br></pre></td></tr></table></figure><ol start="4"><li>检查状态</li></ol><p>此时，3节点Innodb Cluster已经部署完成，检查集群状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">cluster.status()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;clusterName&quot;: &quot;testCluster&quot;, </span><br><span class="line">    &quot;defaultReplicaSet&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;default&quot;, </span><br><span class="line">        &quot;primary&quot;: &quot;172.18.0.100:3306&quot;, </span><br><span class="line">        &quot;ssl&quot;: &quot;REQUIRED&quot;, </span><br><span class="line">        &quot;status&quot;: &quot;OK&quot;, </span><br><span class="line">        &quot;statusText&quot;: &quot;Cluster is ONLINE and can tolerate up to ONE failure.&quot;, </span><br><span class="line">        &quot;topology&quot;: &#123;</span><br><span class="line">            &quot;172.18.0.100:3306&quot;: &#123;</span><br><span class="line">                &quot;address&quot;: &quot;172.18.0.100:3306&quot;, </span><br><span class="line">                &quot;mode&quot;: &quot;R&#x2F;W&quot;, </span><br><span class="line">                &quot;readReplicas&quot;: &#123;&#125;, </span><br><span class="line">                &quot;replicationLag&quot;: null, </span><br><span class="line">                &quot;role&quot;: &quot;HA&quot;, </span><br><span class="line">                &quot;status&quot;: &quot;ONLINE&quot;, </span><br><span class="line">                &quot;version&quot;: &quot;8.0.22&quot;</span><br><span class="line">            &#125;, </span><br><span class="line">            &quot;172.18.0.101:3306&quot;: &#123;</span><br><span class="line">                &quot;address&quot;: &quot;172.18.0.101:3306&quot;, </span><br><span class="line">                &quot;mode&quot;: &quot;R&#x2F;O&quot;, </span><br><span class="line">                &quot;readReplicas&quot;: &#123;&#125;, </span><br><span class="line">                &quot;replicationLag&quot;: null, </span><br><span class="line">                &quot;role&quot;: &quot;HA&quot;, </span><br><span class="line">                &quot;status&quot;: &quot;ONLINE&quot;, </span><br><span class="line">                &quot;version&quot;: &quot;8.0.22&quot;</span><br><span class="line">            &#125;, </span><br><span class="line">            &quot;172.18.0.102:3306&quot;: &#123;</span><br><span class="line">                &quot;address&quot;: &quot;172.18.0.102:3306&quot;, </span><br><span class="line">                &quot;mode&quot;: &quot;R&#x2F;O&quot;, </span><br><span class="line">                &quot;readReplicas&quot;: &#123;&#125;, </span><br><span class="line">                &quot;replicationLag&quot;: null, </span><br><span class="line">                &quot;role&quot;: &quot;HA&quot;, </span><br><span class="line">                &quot;status&quot;: &quot;ONLINE&quot;, </span><br><span class="line">                &quot;version&quot;: &quot;8.0.22&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;, </span><br><span class="line">        &quot;topologyMode&quot;: &quot;Single-Primary&quot;</span><br><span class="line">    &#125;, </span><br><span class="line">    &quot;groupInformationSourceMember&quot;: &quot;172.18.0.100:3306&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="5"><li>Mysql router</li></ol><p>mysql官方没有提供mysql router的arm64版本，mysql-server镜像中也没有集成mysql router。因此只能使用非容器化方式，步骤与上一章沙箱方式一样，不再赘述。</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>Mysql-shell主要提供了两大类命令：dba.xxx和cluster.xxxx。使用cluster的命令要先获取cluster对象，使用<code>dba.get_cluster()</code>命令。通过命令<code>\help dba</code>和<code>\help cluster</code>查询命令详情。使用Mysql-shell可以配置、使用Mysql innodb cluster。 当然直接使用mysql client也是可以的。</p><p>普通用户直接访问mysql router对外暴露的统一端口即可。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: &lt;a href=&quot;https://github.com/wangxiyuan&quot;&gt;wangxiyuan&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文介绍Mysql 8.0的Innodb cluster架构，以及在arm64上的部署流程&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>在arm64平台上创建MariaDB的Docker镜像并在K8S环境上部署</title>
    <link href="https://kunpengcompute.github.io/2020/11/09/zai-arm64-ping-tai-shang-chuang-jian-mariadb-de-docker-jing-xiang-bing-zai-k8s-huan-jing-shang-bu-shu/"/>
    <id>https://kunpengcompute.github.io/2020/11/09/zai-arm64-ping-tai-shang-chuang-jian-mariadb-de-docker-jing-xiang-bing-zai-k8s-huan-jing-shang-bu-shu/</id>
    <published>2020-11-09T12:55:16.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者:  zhaorenhai</p><p>本文计划在arm64平台上创建MariaDB的Docker镜像，并在K8S环境上部署起来。测试一下整个流程，以验证下MariaDB在arm64平台上的容器相关的功能也都是正常的。</p><p>测试平台选择华为云鲲鹏虚拟机，OS采用openEuler20.03版本，MariaDB就用openEuler自带的10.3.9版本，镜像版本我们计划采用openEuler20.09的OS镜像作为基础镜像。</p><a id="more"></a><p>首先确保在虚拟机上安装好了docker</p><p>然后下载openEuler基础镜像，并用docker加载：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -L -O https:&#x2F;&#x2F;repo.openeuler.org&#x2F;openEuler-20.09&#x2F;docker_img&#x2F;aarch64&#x2F;openEuler-docker.aarch64.tar.xz</span><br><span class="line">docker load --input openEuler-docker.aarch64.tar.xz</span><br></pre></td></tr></table></figure><p>然后在同一目录下编辑一个Dockerfile文件和entrypoint.sh文件</p><p>内容分别如下（本文主要演示使用，内容较简单，仅仅是在OS镜像的基础上安装了一个数据库，并自动拉起，简单等待了10s，然后创建了一个默认用户，再把数据库重新在前台启动而已。各位有兴趣的小伙伴可以自行添加其他较复杂的功能）：</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> openeuler-<span class="number">20.09</span>:latest</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">set</span> -ex; \</span></span><br><span class="line"><span class="bash">        yum update -y; \</span></span><br><span class="line"><span class="bash">        yum install -y mariadb-server;\</span></span><br><span class="line"><span class="bash">        yum install -y expect</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> mysql_install_db --user=mysql --skip-test-db</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> entrypoint.sh /opt/entrypoint.sh</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> chmod +x /opt/entrypoint.sh</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">3306</span></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [<span class="string">"/opt/entrypoint.sh"</span>]</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">mysqld_safe &amp;</span><br><span class="line">sleep 10</span><br><span class="line">if [ ! -f /opt/mypasswordfile ]; then</span><br><span class="line">  mkpasswd -s 0 &gt; /opt/mypasswordfile</span><br><span class="line">fi</span><br><span class="line">MYPASSWORD=$(cat /opt/mypasswordfile|head -1)</span><br><span class="line">mysql -uroot -e "Create user 'openeulermariadb'@'%' identified by '$MYPASSWORD' ;"</span><br><span class="line">mysql -uroot -e "grant all privileges on *.* to 'openeulermariadb'@'%';"</span><br><span class="line">mysqladmin -uroot shutdown</span><br><span class="line">mysqld_safe</span><br></pre></td></tr></table></figure><p>然后在这两个文件所在的目录下执行如下命令，创建一个docker镜像：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t openeulermariadb:latest .</span><br></pre></td></tr></table></figure><p>如果没有错误，用<code>docker images</code>命令可以看到刚刚创建的镜像，找到其中的镜像ID。</p><p>然后执行如下命令运行一个容器，这样MariaDB服务就运行起来了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd 253e90ce4419 /bin/bash</span><br></pre></td></tr></table></figure><p>然后执行<code>docker  ps  -a</code> 可以看到刚刚起来的容器，找到其中的容器ID。</p><p>然后执行如下命令，进入容器，并测试相关数据库功能，是否都正常。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it 44d3c508436e /bin/bash</span><br></pre></td></tr></table></figure><p>如果一切正常，就可以参考如下链接：<a href="https://support.huaweicloud.com/usermanual-swr/swr_01_0011.html">https://support.huaweicloud.com/usermanual-swr/swr_01_0011.html</a></p><p>将镜像上传到华为云的SWR镜像服务里面去。</p><p>并在SWR镜像管理页面，将私有镜像改为公开的。</p><p>然后我们尝试将此镜像在K8S里面部署。</p><p>首先确保K8S相关环境已安装，并正常运行。本文不是K8S指导文档，部署步骤在此略过。</p><p>然后创建一个内容如下的openeulermariadb.yaml文件，注意其中的镜像的地址就是刚刚上传到SWR的镜像地址，另外特意加了nodeSelector，选择arm64架构的节点，确保此容器只在arm64的节点上运行：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Namespace</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mariadb</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mariadb-svc</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">mariadb</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">mariadb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">3306</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">3306</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">mariadb3306</span></span><br><span class="line">      <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">mariadb</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mariadb</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">mariadb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">mariadb</span></span><br><span class="line">  <span class="attr">strategy:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">Recreate</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">mariadb</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">nodeSelector:</span></span><br><span class="line">        <span class="attr">kubernetes.io/arch:</span> <span class="string">arm64</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line"></span><br><span class="line">   <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">swr.cn-north-4.myhuaweicloud.com/kunpengcompute/openeulermariadb:latest</span></span><br><span class="line">     <span class="attr">name:</span> <span class="string">mariadb</span></span><br><span class="line">     <span class="attr">ports:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">3306</span></span><br><span class="line">       <span class="attr">name:</span> <span class="string">mariadb3306</span></span><br></pre></td></tr></table></figure><p>然后执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f  openeulermariadb.yaml</span><br></pre></td></tr></table></figure><p>在K8S里面部署，提示如下代表部署成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">namespace&#x2F;mariadb created</span><br><span class="line">service&#x2F;mariadb-svc created</span><br><span class="line">deployment.apps&#x2F;mariadb created</span><br></pre></td></tr></table></figure><p>然后用如下命令可以查看部署成功的服务和POD</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get services -n mariadb</span><br><span class="line">kubectl get pods -n mariadb</span><br></pre></td></tr></table></figure><p>输出分别如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NAME          TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">mariadb-svc   NodePort   10.106.88.47   &lt;none&gt;        3306:30977&#x2F;TCP   2m5s</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NAME                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">mariadb-6cf778c658-wphnq   1&#x2F;1     Running   0          2m33s</span><br></pre></td></tr></table></figure><p>如果在K8S的容器内部网络，可以通过IP 10.106.88.47，端口3306访问数据库，如果是在容器外部网络，可以通过K8S的Master的IP地址和端口号30977来访问数据库。</p><p>也可以执行如下命令，进入容器，测试各项功能。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it mariadb-6cf778c658-wphnq -n mariadb -- /bin/bash</span><br></pre></td></tr></table></figure><p>至此整个流程基本完毕。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者:  zhaorenhai&lt;/p&gt;
&lt;p&gt;本文计划在arm64平台上创建MariaDB的Docker镜像，并在K8S环境上部署起来。测试一下整个流程，以验证下MariaDB在arm64平台上的容器相关的功能也都是正常的。&lt;/p&gt;
&lt;p&gt;测试平台选择华为云鲲鹏虚拟机，OS采用openEuler20.03版本，MariaDB就用openEuler自带的10.3.9版本，镜像版本我们计划采用openEuler20.09的OS镜像作为基础镜像。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>在arm64平台上对MariaDB进行备份恢复测试</title>
    <link href="https://kunpengcompute.github.io/2020/11/04/zai-arm64-ping-tai-shang-dui-mariadb-jin-xing-bei-fen-hui-fu-ce-shi/"/>
    <id>https://kunpengcompute.github.io/2020/11/04/zai-arm64-ping-tai-shang-dui-mariadb-jin-xing-bei-fen-hui-fu-ce-shi/</id>
    <published>2020-11-04T02:48:57.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者: zhaorenhai</p><p>本文计划测试下MariaDB的备份恢复功能在arm64平台上的运行情况，给有兴趣在arm64平台上部署MariaDB的小伙伴提供个参考。</p><p>测试平台选择华为云上8C16G的鲲鹏虚拟机，OS为Ubuntu18.04， MariaDB我们计划选择github上的最新版本来进行测试。</p><a id="more"></a><p>首先从<a href="https://github.com:/MariaDB/server">https://github.com:/MariaDB/server</a> fork一份最新的代码到自己的仓库。</p><p>然后登陆我们的虚拟机，创建一个用户：</p><p><code>adduser mariadb</code><br>后续所有的工作都在这个用户下进行。</p><p>切换到这个用户，并下载刚刚fork的代码，并进行编译.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">su - mariadb</span><br><span class="line">git clone https://github.com/zhaorenhai/server</span><br><span class="line">sudo apt-get install build-essential libncurses5-dev gnutls-dev bison zlib1g-dev ccache libnuma-dev libxml2-dev cmake</span><br><span class="line">mkdir build-mariadb-server</span><br><span class="line">cd build-mariadb-server</span><br><span class="line"><span class="meta">#</span><span class="bash">加入RelWithDebInfo选项，是为了后续如果需要进行对性能分析的话，可以用perf工具看到代码</span></span><br><span class="line">cmake ../server -DCMAKE_BUILD_TYPE=RelWithDebInfo</span><br><span class="line">cmake --build .</span><br></pre></td></tr></table></figure><p>现在我们有了一个最新版本的数据库程序，下面我们继续创建数据库的配置文件，并将数据库运行起来</p><p>创建一个数据库参数文件：<br><code>vi ~/mariadb.cnf</code></p><p>设置如下参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[mariadb]</span><br><span class="line"><span class="meta">#</span><span class="bash">数据文件目录</span></span><br><span class="line">datadir = /home/mariadb/data/dir</span><br><span class="line"><span class="meta">#</span><span class="bash">path to <span class="built_in">source</span> dir + sql/share</span></span><br><span class="line">lc_messages_dir = /home/mariadb/server/sql/share</span><br><span class="line"><span class="meta">#</span><span class="bash">buffer pool，配置成8G，我们预留一部分内存给OS和其他工具</span></span><br><span class="line">innodb_buffer_pool_size = 8G</span><br><span class="line"><span class="meta">#</span><span class="bash">redo <span class="built_in">log</span> 配置成1G</span></span><br><span class="line">innodb_log_file_size = 1G</span><br><span class="line"><span class="meta">#</span><span class="bash">flush_method配置成o_direct,可以减少文件系统的cache占用内存</span></span><br><span class="line">innodb_flush_method = O_DIRECT</span><br><span class="line"><span class="meta">#</span><span class="bash">跳过root用户登陆权限验证</span></span><br><span class="line">skip-grant-tables</span><br><span class="line"><span class="meta">#</span><span class="bash">开启binlog</span></span><br><span class="line">log-bin</span><br></pre></td></tr></table></figure><p>运行数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /home/mariadb/data/dir</span><br><span class="line">./scripts/mysql_install_db --srcdir=../server --defaults-file=~/mariadb.cnf</span><br><span class="line">sql/mysqld --defaults-file=~/mariadb.cnf</span><br></pre></td></tr></table></figure><p>现在一个数据库就运行起来了，我们本文的主要目的是测试备份和恢复功能，在此之前我们还要加载一些数据，我们尽量加载多一点数据。加载数据的工具我们用tpcc-mysql工具，只用他的数据加载功能。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install libmariadbclient-dev</span><br><span class="line">git clone https://github.com/Percona-Lab/tpcc-mysql</span><br><span class="line">cd tpcc-mysql/src</span><br><span class="line">make</span><br><span class="line">cd ..</span><br><span class="line">export PATH=~/build-mariadb-server/client:$PATH  </span><br><span class="line">mysqladmin create tpcc100</span><br><span class="line">mysql tpcc100 &lt; create_table.sql</span><br><span class="line">./tpcc_load -h127.0.0.1 -d tpcc100  -u root -p "" -w 100</span><br><span class="line">mysql tpcc100 &lt; add_fkey_idx.sql</span><br></pre></td></tr></table></figure><p>下面开始测试数据库的备份功能。</p><p>MariaDB数据库备份分为了逻辑备份和物理备份。</p><p>逻辑备份的优点是备份的文件都是sql或者是格式化的文本文件，可以用于其他类型的数据库，也方便导入或者数据迁移，缺点就是备份的时候对数据库性能影响较大,而且恢复的时候比较慢。MariaDB用于逻辑备份的工具就是mysqldump。</p><p>物理备份其实就是对于数据库物理文件的备份，因此对数据库影响相对较小，但是备份出的文件不能用于其他类型的数据库。仅能用于备份恢复使用。 MariaDB用于物理备份的工具是mariabackup。</p><p>MariaDB的逻辑备份和物理备份都是对于某一个时间点数据库的备份，并不能做到完全的无损恢复，如果要做到无损恢复，还需要用到对于binlog日志的重放功能，MariaDB实现这个的工具主要是mysqlbinlog。</p><p>下面我们逐个来测试下这三个工具。</p><p>首先测试下逻辑备份。</p><p><code>mysqldump --all-databases --master-data=2 --single-transaction &gt; all_databases.sql</code></p><p>上面命令中的–master-data=2选项的意思是在备份文件中记录备份时的binlog位置，如果后续做无损恢复的话，可以给mysqlbinlog工具提供开始恢复的位置。–single-transaction的意思是备份在一个事务里执行，这样可以不影响其他会话的正常运行，也可以保证备份数据的一致性。</p><p>我们的data目录一共8.6G，只花了3分钟就备份完毕，备份文件7.2个G，性能还是可以的。当然这和所用的磁盘也是有关系的。我们这里用的是华为云的SSD，速度会快一点。</p><p>逻辑备份的恢复也很简单，直接执行导出的sql文件即可。我们这里只是测试，所以就直接在原库上执行了。</p><p><code>mysql -u root &lt; all_databases.sql</code></p><p>上面7.2G的sql大概执行了21分钟左右，性能也还可以，如果是在一个新的数据库上恢复，估计会更快一点。</p><p>我们继续测试下物理备份。</p><p>先创建一个备份文件存放的目录：</p><p><code>mkdir -p /home/mariadb/data/backup</code></p><p>然后备份的时候不能有skip-grant-tables选项，我们先将备份文件中的这个配置项删除，重启数据库，然后执行<code>mysql</code>进入数据库，修改root用户密码：</p><p><code>alter user &#39;root&#39;@&#39;localhost&#39; identified by &#39;backuptest&#39;;</code></p><p>我们编译出来的mariabackup在编译目录的extra目录下，我们进入到那个目录执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd ~/build-mariadb-server/extra/mariabackup</span><br><span class="line">./mariabackup --backup --target-dir=/home/mariadb/data/backup -uroot -pbackuptest </span><br><span class="line">./mariabackup --prepare --target-dir=/home/mariadb/data/backup</span><br></pre></td></tr></table></figure><p>整个备份两分钟左右就完成了，速度还是比较快的。</p><p>我们继续测试一下恢复：</p><p>恢复的时候要停止数据库。</p><p>然后我们将原数据目录备份一下，创建一个新目录，因为恢复的时候数据文件目录必须是空的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /home/mariadb/data</span><br><span class="line">mv dir dirbak</span><br><span class="line">mkdir dir</span><br></pre></td></tr></table></figure><p>然后开始恢复：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~/build-mariadb-server/extra/mariabackup</span><br><span class="line">./mariabackup --copy-back --target-dir=/home/mariadb/data/backup --datadir=/home/mariadb/data/dir/</span><br></pre></td></tr></table></figure><p>恢复只用了一分半钟</p><p>下面我继续测试下无损恢复。</p><p>假设如下场景：做了数据库备份以后，又新建了一些表，插入了一些数据，但是还没有来得及备份，这个时候数据库数据全被人删除了，但是备份文件和binlog还在。我们试试用备份文件和binlog来将数据库完全恢复出来。</p><p>我们先和之前一样，用mariabackup工具先备份一下数据库，然后再执行如下sql先创建一些表，插入一些数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">test</span>;</span><br><span class="line"><span class="keyword">use</span> <span class="keyword">test</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> binlogtest(<span class="keyword">id</span> <span class="built_in">int</span>, descs <span class="built_in">varchar</span>(<span class="number">10</span>));</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> binlogtest <span class="keyword">values</span>(<span class="number">1</span>, <span class="string">'test1'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> binlogtest <span class="keyword">values</span>(<span class="number">2</span>, <span class="string">'test2'</span>);</span><br><span class="line"><span class="keyword">commit</span>;</span><br></pre></td></tr></table></figure><p>然后执行如下命令，模拟数据文件全被人清除。我们这里用了mv，主要也是为了保留binlog文件，毕竟我们测试环境，binlog没有做多份复制。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /home/mariadb/data</span><br><span class="line">mv dir dirbak2</span><br><span class="line">mkdir dir</span><br></pre></td></tr></table></figure><p>现在数据文件全被清除了，我们先用mariadbbackup恢复备份时的文件，先把数据库停掉，然后执行如下命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~/build-mariadb-server/extra/mariabackup</span><br><span class="line">./mariabackup --copy-back --target-dir=/home/mariadb/data/backup --datadir=/home/mariadb/data/dir/</span><br></pre></td></tr></table></figure><p>然后将数据库启动，登陆数据库</p><p>用如下sql去查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> test.binlogtest;</span><br></pre></td></tr></table></figure><p>会得到错误提示，表不存在。</p><p>下面我们尝试用binlog来恢复</p><p>首先查看备份文件的xtrabackup_binlog_info文件，找到开始恢复的pos点</p><p><code>cat xtrabackup_binlog_info</code></p><p>结果如下</p><p><code>mariadb-arm-perf-test-bin.000012        358     0-1-7541</code></p><p>说明开始恢复的pos点应该是358</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /home/mariadb/data/dirbak2</span><br><span class="line">mysqlbinlog mariadb-arm-perf-test-bin.000001 --start-position=344| mysql -uroot -pbackuptest</span><br></pre></td></tr></table></figure><p>然后再次登陆数据库查询之前的binlogtest表，就会发现数据已经恢复。 </p><p>至此经过测试，我们可以发现MariaDB的各项备份恢复功能在arm64平台上都能完美运行，而且性能也不错。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: zhaorenhai&lt;/p&gt;
&lt;p&gt;本文计划测试下MariaDB的备份恢复功能在arm64平台上的运行情况，给有兴趣在arm64平台上部署MariaDB的小伙伴提供个参考。&lt;/p&gt;
&lt;p&gt;测试平台选择华为云上8C16G的鲲鹏虚拟机，OS为Ubuntu18.04， MariaDB我们计划选择github上的最新版本来进行测试。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Arm64版本的ProxySQL功能测试</title>
    <link href="https://kunpengcompute.github.io/2020/10/31/arm64-ban-ben-de-proxysql-gong-neng-ce-shi/"/>
    <id>https://kunpengcompute.github.io/2020/10/31/arm64-ban-ben-de-proxysql-gong-neng-ce-shi/</id>
    <published>2020-10-31T11:25:20.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者: zhaorenhai</p><p>ProxySQL是业界著名的数据库代理层软件，可以实现数据库的读写分离，负载均衡等高可用功能。可对接的数据库有MySQL，MariaDB，Percona。 最近ProxySQL 2.0.15版本开始支持arm64平台，见如下链接：<a href="https://github.com/sysown/proxysql/releases/tag/v2.0.15">https://github.com/sysown/proxysql/releases/tag/v2.0.15</a> ， 其中centos，ubuntu，debian等系统都有对应的arm64的版本。 本文准备在MariaDB数据库上测试一下最新的ProxySQL arm64版本，也给其他有兴趣的小伙伴提供个参考。</p><a id="more"></a><p>测试环境，还是采用华为云上的云服务器，共三台，一台用来装ProxySQL，另外两台一台装MariaDB主库，另外一台装MariaDB从库。OS我们采用Ubuntu 18.04版本。MariaDB就用OS自带的10.1版本。</p><p>首先搭建一个MariaDB的主从复制高可用环境，具体步骤可以参考这篇博客：<a href="https://kunpengcompute.github.io/2020/10/26/mariadb-ji-qun-zai-x86-he-arm64-shang-de-hun-he-bu-shu/">https://kunpengcompute.github.io/2020/10/26/mariadb-ji-qun-zai-x86-he-arm64-shang-de-hun-he-bu-shu/</a>  虽然这篇博客的里面的OS是用的openEuler，不过除了软件安装的命令和配置文件的路径有些许差异外，其他基本都一样。</p><p>在主数据库上创建两个数据库账户，一个是ProxySQL用来监控数据库的，一个是ProxySQL用来调度业务请求的</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">user</span> <span class="string">'monitor'</span>@<span class="string">'192.168.%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'monitorpassword'</span>;</span><br><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> <span class="string">'monitor'</span>@<span class="string">'192.168.%'</span> <span class="keyword">with</span> <span class="keyword">grant</span> <span class="keyword">option</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">user</span> <span class="string">'proxysql'</span>@<span class="string">'192.168.%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'proxypassword'</span>;</span><br><span class="line"><span class="keyword">grant</span> <span class="keyword">all</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">to</span> <span class="string">'proxysql'</span>@<span class="string">'192.168.%'</span> <span class="keyword">with</span> <span class="keyword">grant</span> <span class="keyword">option</span>;</span><br><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span><br></pre></td></tr></table></figure><p>  然后在调度节点上安装并启动ProxySQL：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/sysown/proxysql/releases/download/v2.0.15/proxysql_2.0.15-ubuntu18_arm64.deb</span><br><span class="line">dpkg -i proxysql_2.0.15-ubuntu18_arm64.deb</span><br><span class="line">apt install mariadb-client</span><br><span class="line">systemctl start proxysql</span><br></pre></td></tr></table></figure><p>登陆ProxySQL：</p><p><code>mysql -uadmin -padmin -h 127.0.0.1 -P 6032</code></p><p>ProxySQL上的配置，包括创建组，创建用户，创建数据库服务器，创建读写分离的调度规则，并将这些配置持久化：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> mysql_replication_hostgroups ( writer_hostgroup, reader_hostgroup, <span class="keyword">comment</span>) <span class="keyword">values</span> (<span class="number">10</span>,<span class="number">20</span>,<span class="string">'proxy'</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> mysql_servers(hostgroup_id,hostname,port) <span class="keyword">values</span> (<span class="number">10</span>,<span class="string">'192.168.0.204'</span>,<span class="number">3306</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> mysql_servers(hostgroup_id,hostname,port) <span class="keyword">values</span> (<span class="number">20</span>,<span class="string">'192.168.0.64'</span>,<span class="number">3306</span>);</span><br><span class="line"><span class="keyword">UPDATE</span> global_variables <span class="keyword">SET</span> variable_value=<span class="string">'monitor'</span> <span class="keyword">WHERE</span> variable_name=<span class="string">'mysql-monitor_username'</span>;</span><br><span class="line"><span class="keyword">UPDATE</span> global_variables <span class="keyword">SET</span> variable_value=<span class="string">'monitorpassword'</span> <span class="keyword">WHERE</span> variable_name=<span class="string">'mysql-monitor_password'</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> mysql_users (username,<span class="keyword">password</span>,default_hostgroup) <span class="keyword">values</span> (<span class="string">'proxysql'</span>,<span class="string">'proxypassword'</span>,<span class="number">10</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> mysql_query_rules(rule_id,active,match_pattern,destination_hostgroup,<span class="keyword">apply</span>) <span class="keyword">values</span> (<span class="number">1</span>,<span class="number">1</span>,<span class="string">'^select.*for update$'</span>,<span class="number">10</span>,<span class="number">1</span>); </span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> mysql_query_rules(rule_id,active,match_pattern,destination_hostgroup,<span class="keyword">apply</span>) <span class="keyword">values</span> (<span class="number">2</span>,<span class="number">1</span>,<span class="string">'^select'</span>,<span class="number">20</span>,<span class="number">1</span>);</span><br><span class="line"><span class="keyword">commit</span>;</span><br><span class="line"><span class="keyword">load</span> mysql servers <span class="keyword">to</span> runtime;</span><br><span class="line">save mysql servers to disk;</span><br><span class="line"><span class="keyword">load</span> mysql <span class="keyword">variables</span> <span class="keyword">to</span> runtime;</span><br><span class="line">save mysql variables to disk;</span><br><span class="line"><span class="keyword">load</span> mysql <span class="keyword">users</span> <span class="keyword">to</span> runtime;</span><br><span class="line">save mysql users to disk;</span><br><span class="line"><span class="keyword">load</span> mysql <span class="keyword">query</span> <span class="keyword">rules</span> <span class="keyword">to</span> runtime;</span><br><span class="line">save mysql query rules to disk;</span><br></pre></td></tr></table></figure><p>用proxysql用户来登陆，注意端口号这次是6033：</p><p><code>mysql -uproxysql -pproxypassword -h 127.0.0.1 -P 6033</code></p><p>执行一些sql，比如创建数据库，创建表，进行一些插入，查询之类的：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">test</span>;</span><br><span class="line"><span class="keyword">use</span> <span class="keyword">test</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_table(col1 <span class="built_in">varchar</span>(<span class="number">10</span>), col2 <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_table <span class="keyword">values</span> (<span class="string">'hello'</span>, <span class="number">999</span>);</span><br><span class="line"><span class="keyword">commit</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> test_table;</span><br></pre></td></tr></table></figure><p>然后再用admin用户登陆ProxySQL管理端：</p><p><code>mysql -uadmin -padmin -h 127.0.0.1 -P 6032</code></p><p>查询一些调度记录，数据库监控情况等等：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> monitor.mysql_server_connect_log;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> mysql_server_ping_log <span class="keyword">limit</span> <span class="number">10</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> mysql_server_read_only_log <span class="keyword">limit</span> <span class="number">10</span>;</span><br><span class="line"><span class="keyword">select</span> hostgroup,schemaname,username,digest_text,count_star <span class="keyword">from</span>  stats_mysql_query_digest;</span><br></pre></td></tr></table></figure><p>根据查询的结果可以看出来，的确读写已经被分离到了不同的数据库。</p><p>参考文档：</p><p><a href="https://www.cnblogs.com/keme/p/12290977.html#1proxysql-%E4%BB%8B%E7%BB%8D">https://www.cnblogs.com/keme/p/12290977.html#1proxysql-%E4%BB%8B%E7%BB%8D</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: zhaorenhai&lt;/p&gt;
&lt;p&gt;ProxySQL是业界著名的数据库代理层软件，可以实现数据库的读写分离，负载均衡等高可用功能。可对接的数据库有MySQL，MariaDB，Percona。 最近ProxySQL 2.0.15版本开始支持arm64平台，见如下链接：&lt;a href=&quot;https://github.com/sysown/proxysql/releases/tag/v2.0.15&quot;&gt;https://github.com/sysown/proxysql/releases/tag/v2.0.15&lt;/a&gt; ， 其中centos，ubuntu，debian等系统都有对应的arm64的版本。 本文准备在MariaDB数据库上测试一下最新的ProxySQL arm64版本，也给其他有兴趣的小伙伴提供个参考。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>基于Bucardo建立PostgreSQL 主主混合集群(X86 + ARM)部署</title>
    <link href="https://kunpengcompute.github.io/2020/10/26/ji-yu-bucardo-jian-li-postgresql-zhu-zhu-hun-he-ji-qun-x86-arm-bu-shu/"/>
    <id>https://kunpengcompute.github.io/2020/10/26/ji-yu-bucardo-jian-li-postgresql-zhu-zhu-hun-he-ji-qun-x86-arm-bu-shu/</id>
    <published>2020-10-26T03:53:26.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者: bzhaoopenstack(<a href="https://github.com/bzhaoopenstack">https://github.com/bzhaoopenstack</a>)</p><p>PG在混步跨硬件平台的首次試水，此次基於Bucardo部署PG跨硬件平台集群。</p><a id="more"></a><h2 id="基于Bucardo建立PostgreSQL-主主混合集群部署"><a href="#基于Bucardo建立PostgreSQL-主主混合集群部署" class="headerlink" title="基于Bucardo建立PostgreSQL 主主混合集群部署"></a>基于Bucardo建立PostgreSQL 主主混合集群部署</h2><hr><p>PostgreSQL集群化部署，有很多种主流方案，包括Bucardo, repmgr, pglogical等方式，并且大致分为两种方式：</p><blockquote><ol><li>基于物理复制的集群，其中主从节点耦合紧密，在这种情况下，它们应该运行在相同的PG版本和相同的硬件平台上。在这种方案下，需要依赖底层硬件功能，并且跨硬件平台的复制可能存在问题，需要验证。</li><li>基于非物理复制的集群(binlog复制等等)，主备节点之间没有耦合关系，所以这种部署方式往往性能比较差，因为其不使用底层PG的复制功能，而是使用外围的形式进行复制，比如基于语句或触发器的复制。另外，由于缺少硬件耦合，PG服务器不能提供数据一致性保证，因此部分部署工具必须确保没有数据丢失。</li></ol></blockquote><p>那么Bucardo在这里是属于第二种部署形式，而repmgr是使用第一种部署形式，因为它使用了PG内建的复制支持。</p><p>Bucardo是一个异步的PostgreSQL复制系统，支持主主、主从方案。它是一款能在PG中实现双向同步的软件，可以实现更多的源数据库(主数据库)以及更多的目标数据库(备份数据库)之间的同步，还可以复制到其他类型的目标数据库，包括Mysql, MariaDB, Oracle, SQLite, MongoDB和Redis等。基于 <a href="https://blog.thyhates.com/2018/08/24/postgresql-replication-bucardo/h">BSD</a>的开源协议。本文中，主要是描述主主模式的搭建过程。</p><p>Bucardo是异步同步，因此实现多主方案时，只能做到数据的最终一致，Bucardo的同步通过触发器来记录变化，并利用PG中的<code>Notify</code>消息事件通知机制实现高效同步，并且同步相当灵活，可以只同步数据库中选定的几张表或者几列。</p><p>由于资源限制，我们当前仅有一台X86虚机，有一台装有Docker的ARM虚机。考虑这种形式：</p><blockquote><p>ARM: 运行在Docker环境中，用容器起PG，并尝试连向对端PG</p><p>X86: 运行在比较常规的虚机中，PG服务直接起在虚机中，尝试连向对端PG</p></blockquote><p>那么我们现有的测试环境为：</p><blockquote><p>ARM PG container:</p><blockquote><p>HostOS version: ubuntu 1804</p><p>ARCH: aarch64</p><p>Docker image: ubuntu 1804 latest in Dockerhub</p><p>Host Intranet IP: 192.168.0.111</p><p>Host Public IP: 1.1.1.1</p><p>Container Intranet IP: 172.17.0.111</p></blockquote><p>X86 VM:</p><blockquote><p>Host OS version: ubuntu 1804</p><p>ARCH: X86</p><p>Host Intranet IP: 192.168.0.222</p><p>Host Public IP: 2.2.2.2</p></blockquote><p>PG version : PG-10 稳定版</p><p>Barcudo version: Master Branch</p><p>Perl version: v5.26.1</p><p>注意： 这里PG version之所以选用稳定版，是因为os库里有现成的PG包提供下载，为了简化部署流程，我们选用apt来安装。</p></blockquote><hr><h2 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h2><p>在ARM容器和X86虚机</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 更新库</span><br><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line"># 安装依赖库</span><br><span class="line">sudo apt-get install zlib1g zlib1g-dev bzip2 libbz2-dev readline-common libreadline-dev bison libgmp-dev libmpfr-dev libmpc-dev -y</span><br><span class="line"></span><br><span class="line"># 安装工具软件</span><br><span class="line">sudo apt-get -q install -y --no-install-recommends build-essential autoconf automake libtool cmake zlib1g-dev pkg-config libssl-dev libssl1.0.0 libsasl2-dev bats curl sudo git wget</span><br><span class="line"></span><br><span class="line"># Barcudo 依赖的perl安装</span><br><span class="line">sudo apt-get install perl -y</span><br></pre></td></tr></table></figure><p>安装PG及bucardo依赖</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 取PG版本为10</span><br><span class="line">PGVERSION&#x3D;10</span><br><span class="line"></span><br><span class="line"># 安装PG server&#x2F;client 以及perl依赖</span><br><span class="line">sudo apt-get install -y postgresql-$&#123;PGVERSION&#125; postgresql-client-$&#123;PGVERSION&#125;     postgresql-$&#123;PGVERSION&#125;-pgtap postgresql-server-dev-$&#123;PGVERSION&#125; postgresql-server-dev-all postgresql-plperl-$&#123;PGVERSION&#125; debhelper fakeroot libdbd-pg-perl libtap-parser-sourcehandler-pgtap-perl</span><br><span class="line"></span><br><span class="line"># 导入PATH</span><br><span class="line">export PATH&#x3D;&#x2F;usr&#x2F;lib&#x2F;postgresql&#x2F;$&#123;PGVERSION&#125;&#x2F;bin:$&#123;PATH&#125;</span><br></pre></td></tr></table></figure><p>为PG服务创建单独用户，后续切换为新建用户执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo useradd -m -d &#x2F;home&#x2F;pgtest -s &#x2F;bin&#x2F;bash pgtest &amp;&amp; echo pgtest:pgtest | chpasswd &amp;&amp; adduser pgtest sudo</span><br><span class="line">sudo echo &quot;pgtest ALL&#x3D;(ALL) NOPASSWD: ALL&quot; &gt;&gt; &#x2F;etc&#x2F;sudoers</span><br></pre></td></tr></table></figure><p>用新建用户下载bucardo源码，编译并安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">export PATH&#x3D;&#x2F;usr&#x2F;lib&#x2F;postgresql&#x2F;$&#123;PGVERSION&#125;&#x2F;bin:$&#123;PATH&#125;</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;bucardo&#x2F;bucardo</span><br><span class="line">cd bucardo&#x2F;</span><br><span class="line"></span><br><span class="line"># 安装缺少的Perl modules</span><br><span class="line">sudo cpan install CGI</span><br><span class="line">sudo cpan install DBIx::Safe</span><br><span class="line"></span><br><span class="line"># 编译安装bucardo</span><br><span class="line">perl Makefile.PL</span><br><span class="line">export USER&#x3D;pgtest</span><br><span class="line">BUCARDO_LOG_ERROR_CONTEXT&#x3D;1 PATH&#x3D;$PATH:&#x2F;usr&#x2F;lib&#x2F;postgresql&#x2F;$&#123;PGVERSION&#125;&#x2F;bin make test</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure><p>另外，bucardo共依赖perl模块为：</p><blockquote><ul><li>DBI</li><li>DBIx::Safe</li><li>DBD::Pg</li><li>Test::Simple</li><li>boolean</li></ul></blockquote><p>可根据缺少的模块进行安装。</p><p>此时，所有的安转工作就绪。开始双主配置。</p><hr><h2 id="2-配置"><a href="#2-配置" class="headerlink" title="2. 配置"></a>2. 配置</h2><p>对于双方都是虚机，都有公网IP，且ARM使用的是docker环境，所以对于HostOS上的配置，需要在云平台或者防火墙放通PG的TCP:5432默认端口。</p><p>在ARM容器HostOS上，添加一条对于容器内部5432端口的端口转发。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A PREROUTING -d 192.168.0.111&#x2F;32 -p tcp -m tcp --dport 5432 -j DNAT --to-destination 172.17.0.111:5432</span><br><span class="line"></span><br><span class="line"># 由于docker在Forward链默认规则是DROP，我们需要针对放通</span><br><span class="line">iptables -A FORWARD -d 172.17.0.111&#x2F;32 -p tcp -m tcp --dport 5432 -j ACCEPT</span><br></pre></td></tr></table></figure><p>此时，ARM虚机上的5432端口会转发给运行在其上的PG容器，对应的容器出公网的流量会走Docker配置的MASQUERADE SNAT出去。OK，ARM hostOS 网络配置完毕。</p><p>同样对于X86，由于PG是直接运行在上面，没有再加任何虚拟化层，所以只要保证云平台安全组或者防火墙放通即可。</p><p>下面我们先配置PG，让它能够暴露在公网。</p><p>修改PG 配置文件postgresql.conf :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">listen_addresses &#x3D; &#39;*&#39;</span><br></pre></td></tr></table></figure><p>修改PG 配置文件 pg_hba.conf:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># TYPE  DATABASE        USER            ADDRESS                 METHOD</span><br><span class="line"></span><br><span class="line"># &quot;local&quot; is for Unix domain socket connections only</span><br><span class="line">local   all             all                                     trust</span><br><span class="line"># IPv4 local connections:</span><br><span class="line">host    all             all             127.0.0.1&#x2F;32            trust</span><br><span class="line"># IPv6 local connections:</span><br><span class="line">host    all             all             ::1&#x2F;128                 trust</span><br><span class="line"># Allow replication connections from localhost, by a user with the</span><br><span class="line"># replication privilege.</span><br><span class="line">local   replication     all                                     trust</span><br><span class="line">host    replication     all             127.0.0.1&#x2F;32            trust</span><br><span class="line">host    replication     all             ::1&#x2F;128                 trust</span><br><span class="line">host    replication     all             1.1.1.1&#x2F;32                 trust</span><br><span class="line">host    replication     all             2.2.2.2&#x2F;32                 trust</span><br><span class="line">host    all             all             1.1.1.1&#x2F;32               md5</span><br><span class="line">host    all             all             2.2.2.2&#x2F;32               md5</span><br></pre></td></tr></table></figure><p>其他PG配置保持默认即可。配置完成后就可以启动PG服务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 导入PG到环境变量</span><br><span class="line">PGVERSION&#x3D;10</span><br><span class="line">export PATH&#x3D;&#x2F;usr&#x2F;lib&#x2F;postgresql&#x2F;$&#123;PGVERSION&#125;&#x2F;bin:$&#123;PATH&#125;</span><br><span class="line"></span><br><span class="line"># 初始化PG数据库服务，注意PG_DATA_DIR 和 PG_LOG_DIR变量需要自行设置，保证当前运行用户有权限访问</span><br><span class="line">initdb --pgdata&#x3D;$PG_DATA_DIR --encoding&#x3D;UTF8</span><br><span class="line">pg_ctl -D $PG_DATA_DIR -l $PG_LOG_DIR start</span><br><span class="line"></span><br><span class="line"># 创建测试数据库 testdb</span><br><span class="line">createdb -O pgtest testdb</span><br></pre></td></tr></table></figure><p>然后，创建同步用户和要同步的表，注意要同步的对象一定要有<strong><em>主键</em></strong>，如果没有主键，在创建同步队列时会失败。虽然可以通过一些办法强行对无主键表进行同步，但会在同步发生数据冲突时产生不可预测的错误。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">pgtest@pg-x86:~&#x2F;bucardo$ psql -d testdb</span><br><span class="line">psql (10.14 (Ubuntu 10.14-0ubuntu0.18.04.1))</span><br><span class="line">Type &quot;help&quot; for help.</span><br><span class="line"></span><br><span class="line">testdb&#x3D;# create user dbuser with password &#39;123456&#39;;</span><br><span class="line">CREATE ROLE</span><br><span class="line">testdb&#x3D;# create table t1 (col1 numeric NOT NULL, col2 numeric, CONSTRAINT pk_1 PRIMARY KEY (col1));</span><br><span class="line">CREATE TABLE</span><br><span class="line">testdb&#x3D;# \q</span><br><span class="line">pgtest@pg-x86:~&#x2F;bucardo$ psql -U dbuser -d testdb</span><br><span class="line">psql (10.14 (Ubuntu 10.14-0ubuntu0.18.04.1))</span><br><span class="line">Type &quot;help&quot; for help.</span><br><span class="line"></span><br><span class="line">testdb&#x3D;&gt; \q</span><br></pre></td></tr></table></figure><p>OK，开始在两个节点(ARM容器和X86虚机)上部署Bucardo辅助对象，其中PG_PID_DIR需要自行指定：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;bucardo install</span><br><span class="line">Enter a number to change it, P to proceed, or Q to quit: 3</span><br><span class="line"></span><br><span class="line">Change the user to: greenplum</span><br><span class="line"></span><br><span class="line">Changed user to: greenplum</span><br><span class="line">Current connection settings:</span><br><span class="line">1. Host:           &lt;none&gt;</span><br><span class="line">2. Port:           5432</span><br><span class="line">3. User:           pgtest</span><br><span class="line">4. Database:       bucardo</span><br><span class="line">5. PID directory:  $PG_PID_DIR</span><br><span class="line">Enter a number to change it, P to proceed, or Q to quit: P</span><br><span class="line"></span><br><span class="line">Creating superuser &#39;bucardo&#39;</span><br><span class="line">Attempting to create and populate the bucardo database and schema</span><br><span class="line">Database creation is complete</span><br><span class="line"></span><br><span class="line">Updated configuration setting &quot;piddir&quot;</span><br><span class="line">Installation is now complete.</span><br></pre></td></tr></table></figure><p>现在可以利用bucardo配置双向同步，以下展示的仅为单向(ARM容器 to X86虚机)，要保证双向的话，反过来再做一次即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 添加源数据库</span><br><span class="line">bucardo add database pg51 dbname&#x3D;testdb port 5432 host&#x3D;127.0.0.1 user&#x3D;dbuser pass&#x3D;123456</span><br><span class="line"></span><br><span class="line"># 添加目标数据库</span><br><span class="line">bucardo add database pg52 dbname&#x3D;testdb port 5432 host&#x3D;2.2.2.2 user&#x3D;dbuser pass&#x3D;123456</span><br><span class="line"></span><br><span class="line"># 添加数据库组</span><br><span class="line">bucardo add dbgroup grp1 pg51:source pg52:target</span><br><span class="line"></span><br><span class="line"># 添加表集群, 注意：同步的表需要有主键</span><br><span class="line">bucardo add table public.t1 herd&#x3D;herd_test</span><br><span class="line"></span><br><span class="line"># 添加同步信息, conflict_strategy为解决冲突的方式共有6种方式，source,target,skip,random,latest,abort</span><br><span class="line">bucardo add sync sync51to52 herd&#x3D;herd_test dbs&#x3D;grp1 conflict_strategy&#x3D;latest</span><br></pre></td></tr></table></figure><p>然后按着相同的配置反向在X86虚机 to ARM容器再做一次配置。这样所有的同步配置就完成了。</p><p>最后两个节点启动bucardo</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bucardo start</span><br></pre></td></tr></table></figure><p>下面是其他的管理命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 停止同步</span><br><span class="line">bucardo stop</span><br><span class="line"></span><br><span class="line"># 暂停 &#x2F; 恢复某一组同步</span><br><span class="line">bucardo pause&#x2F;resume sync51to52</span><br><span class="line"></span><br><span class="line"># 查看同步状态</span><br><span class="line">bucardo status</span><br></pre></td></tr></table></figure><p>这样，所有对两个节点上dbuser用户的public.t1表的操作都会双向同步到各自运行的PG数据库服务上。</p><hr><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h2><p>对于外围同步的Bucardo来部署不同硬件平台的PG服务，是能够运行的。但是在部署过程中，发现不管X86还是ARM平台在运行Bucardo的相关测试用例时都会有失败，包括社区上游，可见该社区似乎不太活跃，导致正常的CI测试都无人问津。这个部署过程有以下几个看法：</p><blockquote><ol><li>PG数据库本身的底层复制模块需要支持跨平台。</li><li>对于DEMO或者性能要求不高的场景可以使用Bucardo，对于性能要求较高的仍然不足。</li><li>对于跨平台的问题跟踪还需要继续深究，同样缺少真实的商用试点样板，这样才能给PG用户足够的信心。</li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: bzhaoopenstack(&lt;a href=&quot;https://github.com/bzhaoopenstack&quot;&gt;https://github.com/bzhaoopenstack&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;PG在混步跨硬件平台的首次試水，此次基於Bucardo部署PG跨硬件平台集群。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>MariaDB集群在x86和arm64上的混合部署</title>
    <link href="https://kunpengcompute.github.io/2020/10/26/mariadb-ji-qun-zai-x86-he-arm64-shang-de-hun-he-bu-shu/"/>
    <id>https://kunpengcompute.github.io/2020/10/26/mariadb-ji-qun-zai-x86-he-arm64-shang-de-hun-he-bu-shu/</id>
    <published>2020-10-26T02:27:33.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者: zhaorenhai</p><p>本文尝试在x86和arm64平台上混合部署MariaDB集群，看看是否可以部署成功。<br>部署环境计划在华为云上进行，OS采用openEuler20.03版本，MariaDB采用10.3.9版本。<br>MariaDB官方文档里面，高可用环境部署方法一共两种类型，比较简单的是复制，然后是Galera集群。<br>我们分别尝试下这两种高可用环境，是否支持在x86和arm64平台上混合部署。</p><a id="more"></a><p><strong>复制</strong><br>复制，顾名思义，就是一个或者多个从数据库从一个主数据库实时的复制数据，从而保证两个数据库的数据是一样的。（当然复制也支持多个主数据库，这种情况较复杂，而且容易出问题，而且现实场景应用不多，本文先主要尝试一主一从的场景）<br>复制的用途：<br>•    可以将读请求分布在多个数据库上，从而分散主数据库的压力，最常用的场景是读多写少的场景。<br>•    数据分析场景。分析数据库中的数据一般需要比较复杂的sql，如果在主数据库上执行会对主数据库造成不少的压力，因为主数据库一般都设计成OLTP类型的数据库，也不太适合分析型的需求。如果将数据复制到从数据库，数据分析就可以只在从数据库执行，减小主数据库的压力。<br>•    备份场景。备份本身也会对数据库造成比较大的性能影响，可以将数据复制到从库，在从库上进行备份就可以避免对主库的影响。更新和插入比较频繁的数据库，可以备份时，将从库和主库断开，这样备份更快，备份完再恢复。<br>•    数据分布，可以通过复制，将数据分布到各个不同的地域。<br>复制的技术原理：首先将主库数据备份一份，然后在从库上恢复，保证两个数据库的基础数据一致，然后通过配置，让从库实时应用主库的binlog，来保证两个数据库数据的一致性。</p><p><strong>复制的部署步骤</strong><br>首先在华为云同一个网络区域购买两个云服务器，一个x86的，一个arm64的，确保内网能够互通，OS镜像都选择openEuler。<br>然后在x86上部署主库：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb-server</span><br></pre></td></tr></table></figure><p>创建数据文件目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /var/db/mariadbdatadir</span><br></pre></td></tr></table></figure><p>配置数据库配置文件：<br>修改/etc/my.cnf.d/mariadb-server.cnf，在[mariadb]下面添加如下配置(具体参数可以根据自己环境调整)：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">datadir = /var/db/mariadbdatadir</span><br><span class="line">innodb_buffer_pool_size = 6G</span><br><span class="line">innodb_log_file_size = 1G</span><br><span class="line">innodb_flush_method = O_DIRECT</span><br><span class="line">bind-address = 0.0.0.0</span><br><span class="line">log-bin</span><br><span class="line">server_id = 1</span><br><span class="line">log-basename = primary1</span><br><span class="line">binlog-format = mixed</span><br></pre></td></tr></table></figure><p>启动数据库：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start mariadb.service</span><br></pre></td></tr></table></figure><p>查看数据库是否启动成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl status mariadb.service</span><br></pre></td></tr></table></figure><p>加载测试数据：<br>为了简单起见，我们就用tpcc-mysql工具来加载一些测试用的tpcc数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb-devel</span><br><span class="line">git clone https://github.com/Percona-Lab/tpcc-mysql</span><br><span class="line">cd tpcc-mysql/src</span><br><span class="line">make</span><br><span class="line">cd ..</span><br><span class="line">mysqladmin create tpcc10</span><br><span class="line">mysql tpcc10 &lt; create_table.sql</span><br><span class="line">./tpcc_load -h127.0.0.1 -d tpcc10  -u root -p "" -w 10</span><br></pre></td></tr></table></figure><p>配置一个用来让从库复制数据的用户</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root</span><br><span class="line">Create user 'replication_user'@'%' identified by 'bigs3cret';</span><br><span class="line">grant replication slave on *.* to 'replication_user'@'%';</span><br></pre></td></tr></table></figure><p>现在一个正常运行的有一些数据的主数据库就配置好了。</p><p>接下来我们在arm机器上配置一个从库。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb-server</span><br></pre></td></tr></table></figure><p>创建数据文件目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /var/db/mariadbdatadir</span><br></pre></td></tr></table></figure><p>配置数据库配置文件：<br>修改/etc/my.cnf.d/mariadb-server.cnf，在[mariadb]下面添加如下配置(具体参数可以根据自己环境调整)：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">datadir = /var/db/mariadbdatadir</span><br><span class="line">innodb_buffer_pool_size = 6G</span><br><span class="line">innodb_log_file_size = 1G</span><br><span class="line">innodb_flush_method = O_DIRECT</span><br><span class="line">server_id = 2</span><br></pre></td></tr></table></figure><p>接下来将主库上的基础数据先复制到从库。<br>登陆主库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root</span><br></pre></td></tr></table></figure><p>执行如下sql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">flush tables with read lock;</span><br><span class="line">show master status;</span><br><span class="line">+---------------------+----------+--------------+------------------+</span><br><span class="line">| File                | Position | Binlog_Do_DB | Binlog_Ignore_DB |</span><br><span class="line">+---------------------+----------+--------------+------------------+</span><br><span class="line">| primary1-bin.000011 |      687 |              |                  |</span><br><span class="line">+---------------------+----------+--------------+------------------+</span><br></pre></td></tr></table></figure><p>记录下以上输出中的File和Position的结果<br>注意这个会话不能中断，直到下面主库的数据备份完成。<br>在主库上创建一个备份目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /var/db/mariadbbackup</span><br></pre></td></tr></table></figure><p>执行备份</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mariabackup --backup --target-dir=/var/db/mariadbbackup/ --user=root --password=""</span><br><span class="line">mariabackup --prepare --target-dir=/var/db/mariadbbackup/</span><br></pre></td></tr></table></figure><p>然后现在就可以将上面的会话解锁了,在上面那个会话中执行如下sql：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unlock tables;</span><br></pre></td></tr></table></figure><p>将备份的数据复制到从库：<br>在主库上：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /var/db</span><br><span class="line">tar -czvf mariadbbackup.tar.gz mariadbbackup</span><br></pre></td></tr></table></figure><p>在从库上的操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scp root@192.168.0.196:/var/db/mariadbbackup.tar.gz /var/db/</span><br><span class="line">cd /var/db/</span><br><span class="line">tar -zxvf mariadbbackup.tar.gz </span><br><span class="line">mariabackup --copy-back --target-dir=/var/db/mariadbbackup/ --datadir=/var/db/mariadbdatadir/</span><br><span class="line">cd mariadbdatadir</span><br><span class="line">chown -R mysql:mysql ./*</span><br><span class="line">systemctl start mariadb.service</span><br></pre></td></tr></table></figure><p>配置实时复制：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /var/db/mariadbbackup</span><br><span class="line">cat xtrabackup_binlog_info</span><br></pre></td></tr></table></figure><p>输出如下，记住这些值，后面从库上的操作要用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">primary1-bin.000011     687     0-1-437</span><br></pre></td></tr></table></figure><p>从库上操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root</span><br><span class="line">set global gtid_slave_pos = '0-1-437';</span><br><span class="line">CHANGE MASTER TO</span><br><span class="line">  MASTER_HOST='192.168.0.196',</span><br><span class="line">  MASTER_USER='replication_user',</span><br><span class="line">  MASTER_PASSWORD='bigs3cret',</span><br><span class="line">  MASTER_PORT=3306,</span><br><span class="line">  MASTER_LOG_FILE='primary1-bin.000011 ',</span><br><span class="line">  MASTER_LOG_POS=687,</span><br><span class="line">  MASTER_CONNECT_RETRY=10,</span><br><span class="line">  MASTER_USE_GTID = slave_pos;</span><br><span class="line">START SLAVE;</span><br></pre></td></tr></table></figure><p>使用如下命令查看复制状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show slave status \G</span><br></pre></td></tr></table></figure><p>如果下面两个项的值都是Yes</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Slave_IO_Running: Yes</span><br><span class="line">Slave_SQL_Running: Yes</span><br></pre></td></tr></table></figure><p>就是正常的</p><p>在主库上，再加载一个数据库，进行一下测试：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd tpcc-mysql/</span><br><span class="line">mysql tpcc-test &lt; create_table.sql</span><br><span class="line">./tpcc_load -h127.0.0.1 -d tpcc-test  -u root -p "" -w 10</span><br></pre></td></tr></table></figure><p>在从库上查询</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root</span><br><span class="line">show databases;</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| mysql              |</span><br><span class="line">| performance_schema |</span><br><span class="line">| tpcc-test          |</span><br><span class="line">| tpcc10             |</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure><p>可以看到tpcc-test数据库也正常生成了<br>可以执行如下sql进行进一步的测试：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">use database tpcc-test;</span><br><span class="line">show tables;</span><br><span class="line">select count(*) from customer;</span><br></pre></td></tr></table></figure><p>如果都是正常的，代表数据都正常同步过来了</p><p>经过以上测试，基本可以证明，复制是可以在x86和arm64环境上混合部署的。</p><p><strong>Galera集群</strong><br>Galera集群是一个多主，多活集群，每一个节点都可以是主节点，数据在多个节点间同步，保证多个数据库节点的数据是一致的，和Oracle的RAC集群不同的是，RAC是共享存储的，Galera集群并不共享存储，每个数据库都是一个独立的存储。<br>这样有多个好处，任意一个数据库节点挂了都不会影响业务，对于读请求比较多的应用，还可以很好的分担压力，当然对于写请求，由于同步请求的存在，会比单机数据库性能下降一点。<br>Galera集群多个节点之间的数据同步是同步的，不是异步的，这样就能保证数据库在多个节点的一致性，多个节点之间无数据分歧，为了性能考虑，采用的并不是实时的同步技术，采用的是一种虚拟同步，虚拟同步技术的原理较复杂，可以参考如下文章：<br><a href="https://blog.csdn.net/wzy0623/article/details/102522268">https://blog.csdn.net/wzy0623/article/details/102522268</a></p><p><strong>Galera集群部署步骤</strong><br>我们计划部署一个三节点的x86 Galera集群，然后前端部署一个GLB负载均衡器。</p><p>首先在华为云上购买四个x86和一个arm的云服务器实例，规格：4vCPUs 16GB内存，CPU频率2.6GHz。OS还是采用openEuler20.03版本。<br>四个x86云服务器，一个用来部署GLB，另外三个用来部署MariaDB Galera集群。一个arm云服务器用来部署后来加入集群的MariaDB节点。</p><p>在三台x86集群节点上进行如下操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb-server</span><br></pre></td></tr></table></figure><p>创建数据文件目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /var/db/mariadbdatadir</span><br></pre></td></tr></table></figure><p>配置数据库配置文件：<br>修改/etc/my.cnf.d/mariadb-server.cnf，在[mariadb]下面添加如下配置(具体参数可以根据自己环境调整)：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">datadir = /var/db/mariadbdatadir</span><br><span class="line">innodb_buffer_pool_size = 6G</span><br><span class="line">innodb_log_file_size = 1G</span><br><span class="line">innodb_flush_method = O_DIRECT</span><br><span class="line">bind-address = 0.0.0.0</span><br></pre></td></tr></table></figure><p>然后执行systemctl start mariadb，先把数据库启动起来</p><p>每个节点再执行如下操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb-server-galera</span><br></pre></td></tr></table></figure><p>修改/etc/my.cnf.d/galera.cnf<br>wsrep_cluster_address配置为如下值，其中各个IP地址的值为三个集群节点的内网IP</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsrep_cluster_address="gcomm://192.168.0.159,192.168.0.78,192.168.0.23"</span><br></pre></td></tr></table></figure><p>在第一个节点上，把数据库停掉</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop mariadb</span><br></pre></td></tr></table></figure><p>在第一个节点上执行如下命令初始化集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">galera_new_cluster</span><br></pre></td></tr></table></figure><p>然后在其他两个节点上重启数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart mariadb</span><br></pre></td></tr></table></figure><p>可以登陆数据库，用如下命令查看集群状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show status like 'wsrep%';</span><br></pre></td></tr></table></figure><p>现在集群搭建成功了，我们下一步准备搭建一个GLB负载均衡器。<br>GLB是一个纯TCP层的负载均衡器，简单易用。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">yum install gcc* libtool</span><br><span class="line">git clone https://github.com/codership/glb</span><br><span class="line">cd glb/</span><br><span class="line">./bootstrap.sh</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line">cp files/glbd.cfg /etc/default/glbd</span><br></pre></td></tr></table></figure><p>编辑/etc/default/glbd<br>配置如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">LISTEN_ADDR="8010"</span><br><span class="line">CONTROL_ADDR="127.0.0.1:8011"</span><br><span class="line">CONTROL_FIFO="/var/run/glbd.fifo"</span><br><span class="line">THREADS="16"</span><br><span class="line">MAX_CONN=256</span><br><span class="line">DEFAULT_TARGETS="192.168.0.159:3306 192.168.0.78:3306 192.168.0.23:3306"</span><br></pre></td></tr></table></figure><p>然后启动负载均衡器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd files</span><br><span class="line">./glbd.sh</span><br></pre></td></tr></table></figure><p>然后登陆集群节点任意一数据库，执行如下sql，建一个用户</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create user 'tpcc'@'192.168.0.%' identified by '123456';</span><br><span class="line">grant all privileges on *.* to 'tpcc'@'192.168.0.%' with grant option;</span><br></pre></td></tr></table></figure><p>下面开始加载一些测试数据。<br>在负载均衡器所在的节点上进行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb</span><br><span class="line">yum install mariadb-devel</span><br><span class="line">git clone https://github.com/Percona-Lab/tpcc-mysql</span><br><span class="line">cd tpcc-mysql/src</span><br><span class="line">make</span><br><span class="line">cd ..</span><br></pre></td></tr></table></figure><p>加载测试数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mysqladmin -h 127.0.0.1 -P 8010 -u tpcc -p123456 create tpcc10</span><br><span class="line">mysql -h 127.0.0.1 -P 8010 -u tpcc -p123456 tpcc10 &lt; create_table.sql</span><br><span class="line">./tpcc_load -h127.0.0.1 -P8010 -d tpcc10  -u tpcc -p "123456" -w 10</span><br><span class="line">mysql -h 127.0.0.1 -P 8010 -u tpcc -p123456 tpcc10 &lt; add_fkey_idx.sql</span><br></pre></td></tr></table></figure><p>接下来把其中一个X86集群节点停掉，准备换成arm64节点。<br>登陆其中一个集群节点<br>systemctl stop mariadb<br>然后登陆另外两个集群节点<br>把/etc/my.cnf.d/galera.cnf里面的wsrep_cluster_address的值里面停掉节点的IP改为am64节点的IP</p><p>在arm64节点上进行如下操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb-server</span><br></pre></td></tr></table></figure><p>创建数据文件目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /var/db/mariadbdatadir</span><br></pre></td></tr></table></figure><p>配置数据库配置文件：<br>修改/etc/my.cnf.d/mariadb-server.cnf，在[mariadb]下面添加如下配置(具体参数可以根据自己环境调整)：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">datadir = /var/db/mariadbdatadir</span><br><span class="line">innodb_buffer_pool_size = 6G</span><br><span class="line">innodb_log_file_size = 1G</span><br><span class="line">innodb_flush_method = O_DIRECT</span><br><span class="line">bind-address = 0.0.0.0</span><br></pre></td></tr></table></figure><p>然后执行systemctl start mariadb，先把数据库启动起来</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb-server-galera</span><br></pre></td></tr></table></figure><p>修改/etc/my.cnf.d/galera.cnf<br>wsrep_cluster_address配置为如下值，就是和另外两个X86节点保持一致。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsrep_cluster_address="gcomm://192.168.0.159,192.168.0.78,192.168.0.173"</span><br></pre></td></tr></table></figure><p>然后执行systemctl restart mariadb<br>等待重启成功，可能会比较慢。<br>然后登陆数据库，输入以下命令，查看集群状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show status like 'wsrep%';</span><br></pre></td></tr></table></figure><p>然后登录arm64节点上的数据库，查询数据是否都正常同步过来。<br>如果一切正常的话，我们登陆到负载均衡器节点，可以将之前的数据卸载了，再重新加载数据，测试一下大规模删除和新增数据场景。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd tpcc-mysql</span><br><span class="line">mysqladmin -h 127.0.0.1 -P 8010 -u tpcc -p123456 drop tpcc10</span><br><span class="line">mysqladmin -h 127.0.0.1 -P 8010 -u tpcc -p123456 create tpcc10</span><br><span class="line">mysql -h 127.0.0.1 -P 8010 -u tpcc -p123456 tpcc10 &lt; create_table.sql</span><br><span class="line">./tpcc_load -h127.0.0.1 -P8010 -d tpcc10  -u tpcc -p "123456" -w 10</span><br><span class="line">mysql -h 127.0.0.1 -P 8010 -u tpcc -p123456 tpcc10 &lt; add_fkey_idx.sql</span><br></pre></td></tr></table></figure><p>然后登陆各个数据库查询，数据是否一致。<br>经过以上测试可以证明Galera集群也是可以在x86和arm64上混合部署的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: zhaorenhai&lt;/p&gt;
&lt;p&gt;本文尝试在x86和arm64平台上混合部署MariaDB集群，看看是否可以部署成功。&lt;br&gt;部署环境计划在华为云上进行，OS采用openEuler20.03版本，MariaDB采用10.3.9版本。&lt;br&gt;MariaDB官方文档里面，高可用环境部署方法一共两种类型，比较简单的是复制，然后是Galera集群。&lt;br&gt;我们分别尝试下这两种高可用环境，是否支持在x86和arm64平台上混合部署。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>ARM64版本的Ubuntu上安装perf</title>
    <link href="https://kunpengcompute.github.io/2020/10/22/arm64-ban-ben-de-ubuntu-shang-an-zhuang-perf/"/>
    <id>https://kunpengcompute.github.io/2020/10/22/arm64-ban-ben-de-ubuntu-shang-an-zhuang-perf/</id>
    <published>2020-10-22T11:29:57.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者: zhaorenhai</p><p>perf是linux下一个很好用的性能分析，性能调优工具。在x86版本的Ubuntu上面，使用perf时，如果默认没有安装，会提示你apt install linux-tools-common，然后再安装apt install linux-tools-<em>x.x.x</em>-generic linux-cloud-tools-<em>x.x.x</em>-generic 一般就可以正常使用了。<br>但是当前ARM64版本的Ubuntu上面还不支持这么直接安装perf。 接下来我们就介绍下如何在ARM64版本的Ubuntu上安装perf</p><a id="more"></a><p>首先查看系统是否已经安装了linux源码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/src</span><br></pre></td></tr></table></figure><p>查看是否有linux-source-x.x.x 目录，如果没有，要先安装linux源码，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt install linux-source</span><br></pre></td></tr></table></figure><p>安装完毕之后，一般安装的格式是一个bz2包，还要先解压：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -jxf linux-source-x.x.x.tar.bz2</span><br></pre></td></tr></table></figure><p>另外编译perf需要flex，先安装一下flex</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt install flex</span><br></pre></td></tr></table></figure><p>然后进入如下目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd linux-source-x.x.x/tools/perf</span><br></pre></td></tr></table></figure><p>直接执行<code>make</code>，这是第一遍make，即使make成功了，也会提示很多包没安装，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">Auto-detecting system features:</span><br><span class="line">...                         dwarf: [ OFF ]</span><br><span class="line">...            dwarf_getlocations: [ OFF ]</span><br><span class="line">...                         glibc: [ on  ]</span><br><span class="line">...                          gtk2: [ OFF ]</span><br><span class="line">...                      libaudit: [ OFF ]</span><br><span class="line">...                        libbfd: [ OFF ]</span><br><span class="line">...                        libcap: [ OFF ]</span><br><span class="line">...                        libelf: [ OFF ]</span><br><span class="line">...                       libnuma: [ on  ]</span><br><span class="line">...        numa_num_possible_cpus: [ on  ]</span><br><span class="line">...                       libperl: [ OFF ]</span><br><span class="line">...                     libpython: [ OFF ]</span><br><span class="line">...                     libcrypto: [ on  ]</span><br><span class="line">...                     libunwind: [ OFF ]</span><br><span class="line">...            libdw-dwarf-unwind: [ OFF ]</span><br><span class="line">...                          zlib: [ on  ]</span><br><span class="line">...                          lzma: [ OFF ]</span><br><span class="line">...                     get_cpuid: [ OFF ]</span><br><span class="line">...                           bpf: [ on  ]</span><br><span class="line">...                        libaio: [ on  ]</span><br><span class="line">...                       libzstd: [ OFF ]</span><br><span class="line">...        disassembler-four-args: [ OFF ]</span><br><span class="line"></span><br><span class="line">Makefile.config:369: No libelf found. Disables &#39;probe&#39; tool, jvmti and BPF support in &#39;perf record&#39;. Please install libelf-dev, libelf-devel or elfutils-libelf-devel</span><br><span class="line">Makefile.config:506: No sys&#x2F;sdt.h found, no SDT events are defined, please install systemtap-sdt-devel or systemtap-sdt-dev</span><br><span class="line">Makefile.config:581: Disabling post unwind, no support found.</span><br><span class="line">Makefile.config:662: slang not found, disables TUI support. Please install slang-devel, libslang-dev or libslang2-dev</span><br><span class="line">Makefile.config:679: GTK2 not found, disables GTK2 support. Please install gtk2-devel or libgtk2.0-dev</span><br><span class="line">Makefile.config:706: Missing perl devel files. Disabling perl scripting support, please install perl-ExtUtils-Embed&#x2F;libperl-dev</span><br><span class="line">Makefile.config:733: No python interpreter was found: disables Python support - please install python-devel&#x2F;python-dev</span><br><span class="line">Makefile.config:829: No liblzma found, disables xz kernel module decompression, please install xz-devel&#x2F;liblzma-dev</span><br><span class="line">Makefile.config:842: No libzstd found, disables trace compression, please install libzstd-dev[el] and&#x2F;or set LIBZSTD_DIR</span><br><span class="line">Makefile.config:853: No libcap found, disables capability support, please install libcap-devel&#x2F;libcap-dev</span><br><span class="line">Makefile.config:921: No libbabeltrace found, disables &#39;perf data&#39; CTF format support, please install libbabeltrace-dev[el]&#x2F;libbabeltrace-ctf-dev</span><br></pre></td></tr></table></figure><p>根据上面的提示，把需要装的包，用<code>apt install</code>命令都安装了，<br>然后</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make clean</span><br></pre></td></tr></table></figure><p>再执行一遍</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make</span><br></pre></td></tr></table></figure><p>如果还有提示某些包没装，建议尽量还是装一下，不然perf的某些功能可能就不正常，重复上面这三个步骤：make clean，装包，make，直到输出的提示里的包，你确定不装对你需要的功能没有影响，就可以了。<br>最后再</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make install</span><br></pre></td></tr></table></figure><p> 这时，perf目录下应该会生成一个perf可执行文件。<br>然后再执行一下如下命令，就可以正常使用perf了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s  /usr/src/linux-source-x.x.x/tools/perf/perf /usr/local/bin/perf</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: zhaorenhai&lt;/p&gt;
&lt;p&gt;perf是linux下一个很好用的性能分析，性能调优工具。在x86版本的Ubuntu上面，使用perf时，如果默认没有安装，会提示你apt install linux-tools-common，然后再安装apt install linux-tools-&lt;em&gt;x.x.x&lt;/em&gt;-generic linux-cloud-tools-&lt;em&gt;x.x.x&lt;/em&gt;-generic 一般就可以正常使用了。&lt;br&gt;但是当前ARM64版本的Ubuntu上面还不支持这么直接安装perf。 接下来我们就介绍下如何在ARM64版本的Ubuntu上安装perf&lt;/p&gt;
    
    </summary>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/categories/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/tags/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>关于原子操作和弱内存序</title>
    <link href="https://kunpengcompute.github.io/2020/09/20/guan-yu-yuan-zi-cao-zuo-he-ruo-nei-cun-xu/"/>
    <id>https://kunpengcompute.github.io/2020/09/20/guan-yu-yuan-zi-cao-zuo-he-ruo-nei-cun-xu/</id>
    <published>2020-09-20T12:02:59.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者：赵仁海</p><h3 id="什么是原子操作"><a href="#什么是原子操作" class="headerlink" title="什么是原子操作"></a>什么是原子操作</h3><p>大家都知道在多核系统上，可以多个CPU核并行执行，即使是在单核系统上，也可以通过中断的方式模拟并行执行。但是内存只有一个，或者确切的说，某一个地址上的数据在内存里只有一个，当有可能出现多个线程对某一个内存地址上的数据同时进行操作的时候，由于这个操作一般会被翻译成CPU的多个指令，当你想实现： 让这多个指令的执行不能被中断，或者同一个内存地址当当前线程在操作的时候其他CPU核不能访问，即使被中断了，或者被其他核访问了，对当前线程也没有任何影响的操作，就可以称之为原子操作。</p><a id="more"></a><p>上面一段话有点绕，我们举个具体的例子：<br>对一个全局变量i的加一操作，比如i++这行代码，在ARM上，实际编译为底层指令之后，是三个指令，ldr，add，str，也就是首先从内存把i这个变量的值加载到寄存器里面，然后在寄存器上加1，然后再存储到内存里面去。 假设i的值初始为0，这时候一个线程执行了前面两个指令，将寄存器的值加为了1，这个时候另外一个核也执行了同样的代码，或者说当前线程被中断了，被调度给了另外一个执行同样代码的线程，另外一个线程执行完毕之后，i的值为1，当前线程获得执行机会继续执行，但是由于当前线程的前面两个指令执行完了，最后一个指令还没执行，当前线程就会把寄存器里面的值1给存储到内存里面去，i的值仍然是1， 这样就产生了错误。本来两个线程的目的都是加1，结果应该是2，但是执行完结果却是1， 所以在这种时候，就需要原子操作，来保证这三个指令不能被中断，或者说即使被中断了，也不会对结果有什么影响。</p><h3 id="原子操作是怎么实现的"><a href="#原子操作是怎么实现的" class="headerlink" title="原子操作是怎么实现的"></a>原子操作是怎么实现的</h3><p>上面简单举了个例子供大家来初步认识下什么是原子操作。具体的实现还是比较复杂的。<br>我们先来看看X86是怎么实现原子操作的，首先我们来看看单核系统的情况，单核系统比较简单，而且在X86上，由于是CISC指令集，是有单独的对内存上的数据直接加1的指令的，也就是说在X86的单核系统上，i++是可以直接被编译成一个原子的指令的，不需要什么额外的操作。但是在多核系统上，由于多个核有可能执行同样的代码，同时访问内存，这个时候也会有可能出现混乱的情况，所以每个核在执行前就需要先锁住内存上的数据，保证在这个指令完成之前，其他CPU不能访问内存这个地址上的数据。 所以如果是在多核的X86系统上，对一个全局变量i++，最后翻译成的汇编指令通常类似于这样的代码：LOCK “incl %0”，其中LOCK的意思就是锁住内存的意思。（最新的X86的CPU实现不是锁内存，而是锁Cache，然后由MESI协议来保证Cache一致性，可以参考这篇文章<a href="https://cloud.tencent.com/developer/article/1367365">https://cloud.tencent.com/developer/article/1367365</a> ，关于MESI协议，是一个更复杂的话题，本文暂不涉及）<br>再来看看在ARM上，ARM是RISC指令集，一开始我们举得例子也说了，同样的一行代码，ARM要翻译成3个指令，这就说明了，即使在单核的ARM系统上，线程在执行这行代码的时候也有可能被中断，所以如果要实现原子指令，在单核系统上最简单的方法就是关闭中断。<br>原子操作开始前，先把中断关闭，执行完后再打开，就实现了原子操作。 但是这种方法在多核系统上也不好用了，ARM在多核系统上采用了另外一种方法来实现原子操作。<br>（下面这段关于多核系统上ARM实现原子指令的描述来自于知乎兰新宇大神的文章，因为写的比较好，我就不重复写了，为了保持叙述的完整性，我把这段copy了过来，原文在这里<a href="https://zhuanlan.zhihu.com/p/89299392">https://zhuanlan.zhihu.com/p/89299392</a>  ）<br>在ARM V8.1之前，为实现RMW的原子操作采用的方法主要是LL/SC(Load-Link/Store-Conditional)。ARMv7中实现LL/SC的指令是LDREX/STREX，其实就是比基础的LDR和STR指令多了一个”EX”，”EX”表示exclusive（独占）。具体说来就是，当用LDREX指令从内存某个地址取出数据放到寄存器后，一个硬件的monitor会将此地址标记为exclusive。<br>假设CPU A先进行load操作，并标记了变量v所在的内存地址为exclusive，在CPU A进行下一步的store操作之前，CPU B也进行了对变量v的load操作，那么这个内存地址的exclusive就成了CPU B标记的了。<br>之后CPU A使用STREX进行store操作，它会测试store的目标地址的exclusive是不是自己标记的（是否为自己独占），结果不是，那么store失败。接下来CPU B也执行STREX，因为exclusive是自己标记的，所以可以store成功，exclusive标记也同步失效。此时CPU A会再次尝试一轮LL/SC的操作，直到store成功。<br>ARM64使用LL/SC模式实现原子操作的相关代码位于”/arch/arm64/include/asm/atomic_ll_sc.h”，它通过对”##”粘合符的运用，将不同原子操作的实现放进了同一段代码中。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">__LL_SC_PREFIX(arch_atomic_##op(<span class="keyword">int</span> i, <span class="keyword">atomic_t</span> *v))</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">long</span> tmp;</span><br><span class="line"><span class="keyword">int</span> result;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">asm</span> <span class="title">volatile</span><span class="params">(<span class="string">"// atomic_"</span> #op <span class="string">"\n"</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"prfmpstl1strm, %2\n"</span>    <span class="comment">// prefetch memory</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"1:ldxr%w0, %2\n"</span>    <span class="comment">// w0 = %2(v-&gt;counter)</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">""</span> #asm_op <span class="string">"%w0, %w0, %w3\n"</span>     <span class="comment">// w0 = w0 + w3 （假设是add）</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"stxr%w1, %w0, %2\n"</span>     <span class="comment">// %2(v-&gt;counter) = w0, 执行结果存储在w1</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"cbnz%w1, 1b"</span>   <span class="comment">// 若w1 != 0，说明store失败，跳转到标号1重试      </span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">"=&amp;r"</span> (result), <span class="string">"=&amp;r"</span> (tmp), <span class="string">"+Q"</span> (v-&gt;counter)<span class="comment">// input+output</span></span></span></span><br><span class="line"><span class="function"><span class="params">: <span class="string">"Ir"</span> (i))</span></span>; <span class="comment">// input</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里的”ldxr”和”stxr”就是前面讲的”ldrex”和”strex”，只不过到了ARMv8.0中被改了个名字而已。可以看到，代码实现中会有个比较和循环，失败则会重试。<br>重试一次还好，如果CPU之间竞争比较激烈，可能导致重试的次数较多，所以从2014年的ARMv8.1开始，ARM推出了用于原子操作的LSE(Large System Extension)指令集扩展，新增的指令包括CAS, SWP和LD<OP>, ST<OP>等，其中<OP>可以是ADD, CLR, EOR, SET等。这些指令也类似于X86上，可以直接对内存上的数据进行原子计算。ARM64使用LSE指令实现原子操作的相关代码位于”/arch/arm64/include/asm/atomic_lse.h”。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">void</span> arch_atomic_#<span class="meta">#op(int i, atomic_t *v)</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="function"><span class="keyword">register</span> <span class="keyword">int</span> w0 <span class="title">asm</span> <span class="params">(<span class="string">"w0"</span>)</span> </span>= i;</span><br><span class="line"><span class="function"><span class="keyword">register</span> <span class="keyword">atomic_t</span> *x1 <span class="title">asm</span> <span class="params">(<span class="string">"x1"</span>)</span> </span>= v;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">asm</span> <span class="title">volatile</span><span class="params">(ARM64_LSE_ATOMIC_INSN(__LL_SC_ATOMIC(op),<span class="string">" "</span> #asm_op <span class="string">"%w[i], %[v]\n"</span>)</span></span></span><br><span class="line"><span class="function"><span class="params">: [i] <span class="string">"+r"</span> (w0), [v] <span class="string">"+Q"</span> (v-&gt;counter)   <span class="comment">// nput+output</span></span></span></span><br><span class="line"><span class="function"><span class="params">: <span class="string">"r"</span> (x1)      <span class="comment">// input</span></span></span></span><br><span class="line"><span class="function"><span class="params">: <span class="string">"x16"</span>, <span class="string">"x17"</span>, <span class="string">"x30"</span>)</span></span>;    <span class="comment">// clobber list  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>确实比LL/SC的实现方式简洁了很多，一条指令就可以搞定，比如实现arch_atomic_add()，只用LDADD指令即可，load操作和store操作合二为一，比较类似于x86的实现（用add指令）。<br>既然load和store都二合一了，那为啥还分别有LD<OP>和ST<OP>呢？其实，两者的效果是一样的，比如STADD就可以看做是LDADD的别名(alias)。<br><code>STADD &lt;Xs&gt;, [&lt;Xn|SP&gt;]</code><br>等同于<br><code>LDADD &lt;Xs&gt;, XZR, [&lt;Xn|SP&gt;]</code><br>（Copy兰新宇大神文章的内容到此为止）</p><p>关于ARMv8.1新的原子指令，这里多说一下，只有在很高的并发下才能显示出性能优势，一般情况下和老的原子指令性能差不多，在一般系统的性能测试下显示不出来差别。关于如何启用新版本的原子指令的方法，可以参考这篇文章：<br><a href="https://kunpengcompute.github.io/2020/08/29/aarch64-fu-wu-qi-ying-yong-ruan-jian-kai-fa-xu-yao-tian-jia-de-bian-yi-can-shu/">https://kunpengcompute.github.io/2020/08/29/aarch64-fu-wu-qi-ying-yong-ruan-jian-kai-fa-xu-yao-tian-jia-de-bian-yi-can-shu/</a></p><h3 id="原子操作有什么用"><a href="#原子操作有什么用" class="headerlink" title="原子操作有什么用"></a>原子操作有什么用</h3><p>原子操作的用途通过之前的描述，大家应该也清楚了，最简单的，需要全局计数器的场景。<br>还有像实现自旋锁，信号量等多线程同步机制的地方，都会用到。比如对信号量的count值进行增加，都必须要是原子的。<br>另外有时候无锁编程的时候也需要原子变量，我们来看一个简单的例子，什么是无锁编程。<br>还是一开始那个对变量加一的操作例子，假设有两种线程，一种是读线程，负责读取这个变量的值并进行打印，一种是写线程，对这个变量，进行自增。我们现在知道写线程本质上不是原子的，因为它由三个不同的步骤组成，读取值，将其递增，然后将新值存储回去。如果使用锁，伪代码大概是这个样子的：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mutex = initialize_mutex()</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line">reader_thread()</span><br><span class="line">    mutex.lock()</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line">    mutex.unlock()</span><br><span class="line"></span><br><span class="line">writer_thread()</span><br><span class="line">    mutex.lock()</span><br><span class="line">    i++</span><br><span class="line">    mutex.unlock()</span><br></pre></td></tr></table></figure><p>获取锁的第一个线程可以执行，而其他线程必须排队等待。</p><p>相反，无锁方法引入了另一种模式：通过采用原子操作，线程可以不受任何阻碍地自由运行。例如：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">i = <span class="number">0</span></span><br><span class="line">reader_thread()</span><br><span class="line">    <span class="built_in">print</span>(load(i))</span><br><span class="line"></span><br><span class="line">writer_thread()</span><br><span class="line">    fetch_and_add(i, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>这里fetch_and_add()和load()是基于相应硬件指令的原子操作伪代码。这里没有任何东西被锁定。原子性load()确保没有读线程将读取共享值的一半完成，并且没有写线程会由于造成部分写入而损坏共享值。</p><h3 id="什么是弱内存序"><a href="#什么是弱内存序" class="headerlink" title="什么是弱内存序"></a>什么是弱内存序</h3><p>内存序，顾名思义就是CPU访问内存的顺序。<br>举个简单的例子，如下代码</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="number">1</span></span><br><span class="line">b = <span class="number">2</span></span><br><span class="line">c = a + b</span><br></pre></td></tr></table></figure><p>强内存序的机器，就会按顺序执行，a =1 ，b=2， c=a+b<br>但是弱内存序的机器，有可能会执行成，b=2， a=1，c=a+b，因为a =1和b=2之间先后没什么逻辑依赖，先执行哪个，都不会导致结果有问题，编译器或者CPU就会自动的做一些优化，因为根据现代CPU结构的设计，指令执行流水线的重新排序或者乱序执行可能会带来性能的提升。这就是弱内存序。<br>那有人会问，那c = a+b会不会先执行？有可能还真会，但是在ARM和X86上都不会，在Alpha上会。上面说的强内存序和弱内存序也都是为了方便理解，简单的说法，实际上有多种内存序，不同的CPU架构上有不同的内存序。我们可以看看下表（转自<a href="https://zh.wikipedia.org/wiki/%E5%86%85%E5%AD%98%E6%8E%92%E5%BA%8F">https://zh.wikipedia.org/wiki/%E5%86%85%E5%AD%98%E6%8E%92%E5%BA%8F</a>   考虑到国内有人无法访问维基，把表截过来）</p><table><thead><tr><th>类型</th><th>Alpha</th><th>ARMv7</th><th>MIPS</th><th>LoongISA</th><th>PA-RISC</th><th>POWER</th><th>SPARC RMO</th><th>SPARC PSO</th><th>SPARC TSO</th><th>x86</th><th>x86 oostore</th><th>AMD64</th><th>IA-64</th><th>z/Architecture</th></tr></thead><tbody><tr><td>Loads reordered after loads</td><td>Y</td><td>Y</td><td>架构本身不规定</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td></td><td></td><td></td><td>Y</td><td></td><td>Y</td><td></td></tr><tr><td>Loads reordered after stores</td><td>Y</td><td>Y</td><td>微架构/芯片的实现决定</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td></td><td></td><td></td><td>Y</td><td></td><td>Y</td><td></td></tr><tr><td>Stores reordered after stores</td><td>Y</td><td>Y</td><td></td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td></td><td></td><td>Y</td><td></td><td>Y</td><td></td></tr><tr><td>Stores reordered after loads</td><td>Y</td><td>Y</td><td></td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td></tr><tr><td>Atomic reordered with loads</td><td>Y</td><td>Y</td><td></td><td></td><td></td><td>Y</td><td>Y</td><td></td><td></td><td></td><td></td><td></td><td>Y</td><td></td></tr><tr><td>Atomic reordered with stores</td><td>Y</td><td>Y</td><td></td><td></td><td></td><td>Y</td><td>Y</td><td>Y</td><td></td><td></td><td></td><td></td><td>Y</td><td></td></tr><tr><td>Dependent loads reordered</td><td>Y</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Incoherent instruction cache pipeline</td><td>Y</td><td>Y</td><td></td><td></td><td></td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td></td><td>Y</td><td></td></tr></tbody></table><p>可以看出来，内存序分了读读是否会乱序，读写是否会乱序，写写是否会乱序，写读是否会乱序，依赖是否会乱序等多种，在不同的架构上支持各种不同的内存访问顺序。 可以看出来，ARM对于上下文有依赖的指令，还是不会乱序执行的。<br>为了简单理解起见，后文我们就简单认为ARM是弱内存序模型，X86是强内存序模型。</p><h3 id="弱内存序会导致什么问题"><a href="#弱内存序会导致什么问题" class="headerlink" title="弱内存序会导致什么问题"></a>弱内存序会导致什么问题</h3><p>如果所有代码都是单线程的，当然不会有什么问题，但是如果代码是多线程的，就可能会了。考虑如下代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   a = <span class="number">1</span>;</span><br><span class="line">   b = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bar</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="keyword">while</span> (b == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">   assert(a == <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>假如一个线程执行foo，另外一个线程执行bar，在强内存序机器上，不会有问题，肯定可以确保，当b的值不为0以后，a的值肯定是1。但是在弱内存序机器上就会出现问题，因为b=1有可能先于a=1执行，因为在执行foo的线程来看，这两个并没有什么逻辑关系。 这个时候执行bar的线程如果在b=1执行完之后得到了执行的机会，这个时候，assert（a==1）就会返回错误了。</p><h3 id="怎么避免弱内存序带来的问题"><a href="#怎么避免弱内存序带来的问题" class="headerlink" title="怎么避免弱内存序带来的问题"></a>怎么避免弱内存序带来的问题</h3><p>内存屏障可以避免弱内存序带来的问题。<br>内存屏障有两种，一种是编译器的内存屏障，这种内存屏障对避免弱内存序的问题，其实没什么用，这种内存屏障只是保证了编译器编译出来的汇编指令，b=1肯定会在a=1后面，但是实际CPU执行的时候到底谁在先，谁在后就不一定了。（编译器的内存屏障，在ARM上就是这么一行代码，<strong>asm</strong> <strong>volatile</strong>(“”: : :”memory”)，在一些特殊的场景有用，比如明确需要避免编译器自动优化的场景）</p><p>另外一种就是CPU的内存屏障，这种可以解决弱内存序的问题。<br>CPU内存屏障分为三种，读内存屏障，写内存屏障，还有就是读写内存屏障。<br>读内存屏障的意思就是本线程所有后续的读操作均在本条指令以后执行，写内存屏障就是本线程所有之前的写操作均在本条指令以前执行。<br>X86上的CPU内存屏障指令要简单一些，就分了三种：lfence，sfence，mfence，分别对应读屏障，写屏障，读写屏障。<br>(有人会问X86既然是强内存序，为什么还需要内存屏障，其实X86，并不是完全有序的，根据上面的表格，X86在某些场景也会乱序执行，可以参考这篇文章 <a href="https://bartoszmilewski.com/2008/11/05/who-ordered-memory-fences-on-an-x86/">https://bartoszmilewski.com/2008/11/05/who-ordered-memory-fences-on-an-x86/</a>  ）<br>ARM上的稍微有点复杂，又分了几种情况：<br>DMB<br>全称 Data Memory Barrier，仅当所有在它前面的存储器访问都执行完毕后，才执行它后面的存储器访问动作（注意只对存储器访问敏感），其它非内存访问指令依然可以乱序执行。<br>DSB<br>全称Data Synchronous Barrier，比DMB严格：仅当所有在它前面的存储器访问都执行完毕后，才执行它在后面的指令（亦即任何指令都要等待）。<br>ISB<br>全称 Instruction Synchronous Barrier，该指令将刷新CPU pipeline和prefetch buffer，ISB之后的指令需要重新从cache或memory取指，以保证所有它前面的指令都执行完毕之后，才执行它后面的指令。<br>其中每种指令，根据内存和Cache共享域又分了这么些参数：</p><table><thead><tr><th>参数</th><th>访问控制(before-after)</th><th>共享域</th></tr></thead><tbody><tr><td>OSHLD</td><td>Load - Load,   Load - Store</td><td>Outer   shareable</td></tr><tr><td>OSHST</td><td>Store - Store</td><td>Outer   shareable</td></tr><tr><td>OSH</td><td>Any - Any</td><td>Outer   shareable</td></tr><tr><td>NSHLD</td><td>Load - Load,   Load - Store</td><td>Non-shareable</td></tr><tr><td>NSHST</td><td>Store - Store</td><td>Non-shareable</td></tr><tr><td>NSH</td><td>Any - Any</td><td>Non-shareable</td></tr><tr><td>ISHLD</td><td>Load -Load,   Load - Store</td><td>Inner   shareable</td></tr><tr><td>ISHST</td><td>Store - Store</td><td>Inner   shareable</td></tr><tr><td>ISH</td><td>Any - Any</td><td>Inner   shareable</td></tr><tr><td>LD</td><td>Load -Load,   Load - Store</td><td>Full system</td></tr><tr><td>ST</td><td>Store - Store</td><td>Full system</td></tr><tr><td>SY</td><td>Any - Any</td><td>Full system</td></tr></tbody></table><p>其中共享域的含义如下：<br>Non-shareable：core独享区域<br>Inner shareable：可被多核共享，但不需要都可访问。一个系统中可能有多个Inner shareable区域<br>Outer shareable：影响Outer Shareable的操作，隐性的会影响其中的所有Inner Shareable区域。反过来不成立<br>Full System：影响系统中的所有observer</p><p>如果考虑到X86到ARM的移植，一般对应关系是这样的：</p><table><thead><tr><th>屏障名称</th><th>x86</th><th>arm</th></tr></thead><tbody><tr><td>读屏障</td><td>asm   volatile(“lfence” ::: “memory”)</td><td>asm   volatile(“dmb ishld” ::: “memory”)</td></tr><tr><td>写屏障</td><td>asm   volatile(“sfence” ::: “memory”)</td><td>asm   volatile(“dmb ishst” ::: “memory”)</td></tr><tr><td>内存屏障</td><td>asm   volatile(“mfence” ::: “memory”)</td><td>asm   volatile(“dmb ish” ::: “memory”)</td></tr></tbody></table><p>建议平时如果新写的代码的话，尽量用linux提供的API，列表如下：</p><table><thead><tr><th>接口名称</th><th>作用</th></tr></thead><tbody><tr><td>barrier()</td><td>优化屏障，阻止编译器为了进行性能优化而进行的memory access reorder</td></tr><tr><td>mb()</td><td>内存屏障（包括读和写），用于SMP和UP</td></tr><tr><td>rmb()</td><td>读内存屏障，用于SMP和UP</td></tr><tr><td>wmb()</td><td>写内存屏障，用于SMP和UP</td></tr><tr><td>smp_mb()</td><td>用于SMP场合的内存屏障，对于UP不存在memory order的问题（对汇编指令），因此，在UP上就是一个优化屏障，确保汇编和c代码的memory order是一致的</td></tr><tr><td>smp_rmb()</td><td>用于SMP场合的读内存屏障</td></tr><tr><td>smp_wmb()</td><td>用于SMP场合的写内存屏障</td></tr></tbody></table><p>其中barrier就是我们所说的编译器屏障，其余都是CPU屏障<br>在内核中，tools/arch/arm64/include/asm/barrier.h文件给出了常用的内存屏障相关的宏定义，实际上和上面x86和arm对应表格里面的内容一样的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#define smp_mb()    asm volatile(&quot;dmb ish&quot; ::: &quot;memory&quot;)</span><br><span class="line">#define smp_wmb()   asm volatile(&quot;dmb ishst&quot; ::: &quot;memory&quot;)</span><br><span class="line">#define smp_rmb()   asm volatile(&quot;dmb ishld&quot; ::: &quot;memory&quot;)</span><br></pre></td></tr></table></figure><h3 id="原子操作怎么会和弱内存序扯到一块"><a href="#原子操作怎么会和弱内存序扯到一块" class="headerlink" title="原子操作怎么会和弱内存序扯到一块"></a>原子操作怎么会和弱内存序扯到一块</h3><p>在ARM平台上，从ARMv8开始，在很多指令里面就直接加上了内存屏障的语义，比如ldxr/stxr汇编指令 和ldaxr/stlxr汇编指令，如果在需要内存屏障的场景，用ldxr或者stxr的话，还需要再加上单独的内存屏障的指令，但是如果用ldaxr和stlxr的话，就不再需要内存屏障了。实测效果，后者要比前者性能更好。从ARMv8.1开始，新加入的原子指令，也有实现了内存屏障语义的版本，可以参考这篇文章：<br><a href="https://blog.csdn.net/Roland_Sun/article/details/107552574">https://blog.csdn.net/Roland_Sun/article/details/107552574</a><br>当然上面用汇编指令讲解，只是为了方便大家理解，平时如果是新开发代码的话，建议不要直接使用汇编指令，建议使用gcc内置的函数，这样平台通用性更强，具体函数列表可以参考这个链接：<br><a href="https://gcc.gnu.org/onlinedocs/gcc/_005f_005fatomic-Builtins.html">https://gcc.gnu.org/onlinedocs/gcc/_005f_005fatomic-Builtins.html</a><br>里面的内存屏障参数语义含义如下：</p><table><thead><tr><th>Value</th><th>Explanation</th></tr></thead><tbody><tr><td>memory_order_relaxed</td><td>不对执行顺序做任何保障</td></tr><tr><td>memory_order_consume</td><td>本线程中，所有后续的有关本原子类型的操作，必须在本条原子操作完成后执行</td></tr><tr><td>memory_order_acquire</td><td>本线程中，所有后续的读操作均在本条原子操作完成后执行</td></tr><tr><td>memory_order_release</td><td>本线程中，所有之前的写操作完成后才能执行本操作</td></tr><tr><td>memory_order_acq_rel</td><td>同时包含以上两条的语义</td></tr><tr><td>memory_order_seq_cst</td><td>全部按顺序执行</td></tr></tbody></table><h3 id="我们为什么要学习原子操作和弱内存序"><a href="#我们为什么要学习原子操作和弱内存序" class="headerlink" title="我们为什么要学习原子操作和弱内存序"></a>我们为什么要学习原子操作和弱内存序</h3><p>有人说这些东西是不是太底层了，平时是不是用不到这些知识。其实不是，除了上面所说的无锁编程场景之外，至少以下几种场景还是要用到的。<br>1 C和C++多线程编程，如果不了解弱内存序的知识，还是很可能会产生一些bug的，而且这些bug都很难定位。<br>2 定位由于弱内存序产生的bug，即使是业界知名的软件，比如mysql等一些软件，在兼容ARM的过程中，由于弱内存序的问题，也会有很多bug，定位这些bug，也需要这方面的相关知识。<br>3 如果一些软件一开始只支持了X86，并且使用了底层的原子操作，你如果想要将这些软件兼容ARM，并进行相关开发工作的话，你就要对这些原子操作汇编指令等等进行对应的移植，还要优化。这个时候也需要这方面的知识。比如我们团队在移植Impala过程中的相关开发工作：<a href="https://gerrit.cloudera.org/#/c/15300/">https://gerrit.cloudera.org/#/c/15300/</a><br>4 有时候有些软件的内存序使用过于严格，也需要进行优化，会对软件的性能有一定的提升。比如我们团队大牛在mysql上的一些优化工作，就是针对这方面的，比如有些原子变量的自增，并不需要依赖上下文，所以就可以不需要严格的内存屏障语义。可以参考：<br><a href="https://bugs.mysql.com/bug.php?id=99432">https://bugs.mysql.com/bug.php?id=99432</a><br><a href="https://bugs.mysql.com/bug.php?id=100119">https://bugs.mysql.com/bug.php?id=100119</a><br><a href="https://bugs.mysql.com/bug.php?id=100432">https://bugs.mysql.com/bug.php?id=100432</a><br><a href="https://bugs.mysql.com/bug.php?id=100060">https://bugs.mysql.com/bug.php?id=100060</a><br><a href="https://bugs.mysql.com/bug.php?id=100132">https://bugs.mysql.com/bug.php?id=100132</a></p><p>参考资料：<br><a href="https://blog.csdn.net/jus3ve/article/details/81294505">https://blog.csdn.net/jus3ve/article/details/81294505</a><br><a href="http://blog.sina.com.cn/s/blog_70441c8e0102vrcs.html">http://blog.sina.com.cn/s/blog_70441c8e0102vrcs.html</a><br><a href="https://zhuanlan.zhihu.com/p/89299392">https://zhuanlan.zhihu.com/p/89299392</a><br><a href="https://gcc.gnu.org/onlinedocs/gcc/_005f_005fatomic-Builtins.html">https://gcc.gnu.org/onlinedocs/gcc/_005f_005fatomic-Builtins.html</a><br><a href="https://www.cnblogs.com/fanzhidongyzby/p/3654855.html">https://www.cnblogs.com/fanzhidongyzby/p/3654855.html</a><br><a href="https://labs.supinfochina.com/%E5%85%B7%E6%9C%89%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%97%A0%E9%94%81%E5%A4%9A%E7%BA%BF%E7%A8%8B/">https://labs.supinfochina.com/%E5%85%B7%E6%9C%89%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%97%A0%E9%94%81%E5%A4%9A%E7%BA%BF%E7%A8%8B/</a><br><a href="https://zh.wikipedia.org/wiki/%E4%B9%B1%E5%BA%8F%E6%89%A7%E8%A1%8C">https://zh.wikipedia.org/wiki/%E4%B9%B1%E5%BA%8F%E6%89%A7%E8%A1%8C</a><br><a href="http://www.wowotech.net/memory_management/456.html">http://www.wowotech.net/memory_management/456.html</a><br><a href="http://www.wowotech.net/kernel_synchronization/Why-Memory-Barriers.html">http://www.wowotech.net/kernel_synchronization/Why-Memory-Barriers.html</a><br><a href="https://zhuanlan.zhihu.com/p/94421667">https://zhuanlan.zhihu.com/p/94421667</a><br><a href="https://zh.wikipedia.org/wiki/%E5%86%85%E5%AD%98%E6%8E%92%E5%BA%8F">https://zh.wikipedia.org/wiki/%E5%86%85%E5%AD%98%E6%8E%92%E5%BA%8F</a><br><a href="https://zh.wikipedia.org/wiki/%E5%86%85%E5%AD%98%E5%B1%8F%E9%9A%9C">https://zh.wikipedia.org/wiki/%E5%86%85%E5%AD%98%E5%B1%8F%E9%9A%9C</a><br><a href="http://www.wowotech.net/kernel_synchronization/memory-barrier.html">http://www.wowotech.net/kernel_synchronization/memory-barrier.html</a><br><a href="http://www.wowotech.net/armv8a_arch/Observability.html">http://www.wowotech.net/armv8a_arch/Observability.html</a><br><a href="https://stackoverflow.com/questions/21535058/arm64-ldxr-stxr-vs-ldaxr-stlxr">https://stackoverflow.com/questions/21535058/arm64-ldxr-stxr-vs-ldaxr-stlxr</a><br><a href="https://bartoszmilewski.com/2008/11/05/who-ordered-memory-fences-on-an-x86/">https://bartoszmilewski.com/2008/11/05/who-ordered-memory-fences-on-an-x86/</a><br><a href="https://cloud.tencent.com/developer/article/1367365">https://cloud.tencent.com/developer/article/1367365</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：赵仁海&lt;/p&gt;
&lt;h3 id=&quot;什么是原子操作&quot;&gt;&lt;a href=&quot;#什么是原子操作&quot; class=&quot;headerlink&quot; title=&quot;什么是原子操作&quot;&gt;&lt;/a&gt;什么是原子操作&lt;/h3&gt;&lt;p&gt;大家都知道在多核系统上，可以多个CPU核并行执行，即使是在单核系统上，也可以通过中断的方式模拟并行执行。但是内存只有一个，或者确切的说，某一个地址上的数据在内存里只有一个，当有可能出现多个线程对某一个内存地址上的数据同时进行操作的时候，由于这个操作一般会被翻译成CPU的多个指令，当你想实现： 让这多个指令的执行不能被中断，或者同一个内存地址当当前线程在操作的时候其他CPU核不能访问，即使被中断了，或者被其他核访问了，对当前线程也没有任何影响的操作，就可以称之为原子操作。&lt;/p&gt;
    
    </summary>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/categories/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>PostgreSQL对外部压缩方法的诉求</title>
    <link href="https://kunpengcompute.github.io/2020/08/31/postgresql-dui-wai-bu-ya-suo-fang-fa-de-su-qiu/"/>
    <id>https://kunpengcompute.github.io/2020/08/31/postgresql-dui-wai-bu-ya-suo-fang-fa-de-su-qiu/</id>
    <published>2020-08-31T01:20:24.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Amit Dattatray Khandekar<br>原文链接: <a href="https://amitdkhan-pg.blogspot.com/2020/08/need-for-external-compression-methods.html">https://amitdkhan-pg.blogspot.com/2020/08/need-for-external-compression-methods.html</a></p><p>Amit PSQL专家分析压缩库在PostgreSQL中的迫切诉求。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>现今的每个数据库系统都有一定程度的数据压缩方法。很明显，这是为了减少数据库的大小，特别是在当今数据呈指数增长的时代。另外的原因是为了提高查询性能; 其思想是: 更小的数据大小意味着需要扫描的数据页面更少，这意味着更少的磁盘 i/o 和更快的数据访问。因此，无论如何，数据解压缩的速度应该足够快，以免影响查询性能(如果不能提高的话)。</p><p>压缩提供了不同的层次: 页面压缩、行压缩、列压缩等。柱型数据库的优点是它的列压缩比很高，因为在一个列中存在连续数据的重复区域。另一种情况是，在面向行的数据库中，列值非常大，因此有必要压缩列的单个值。如果这些值不适合放在单个页面中，甚至可以单独保留它们，并且该行具有指向行外压缩数据的指针。在 PostgreSQL 中，这种技术被称为 TOAST (超大型属性存储技术) ，对于可以包含可变长度数据的列，数据被明显地压缩并存储在同一行中，或者如果数据仍然太大，则将数据以较小的块形式单独的存储在称为 TOAST table表的行中，这些块本身可能被压缩，也可能不被压缩。</p><p>压缩为不同的数据操作提供了可能性。它可能不会被限制只有几秒数据压缩。例如，在容灾系统中，把redo logs从主服务器到从服务器的传输可能成为一个巨大的网络瓶颈，因此许多 RDBMS 提供压缩redo logs的功能。</p><p>然后是 RDBMS 使用或提供选项可选的压缩算法。这一点尤其适用于数据压缩。由于数据是用户的数据，用户数据中的特定格式可能适合特定的压缩算法，而不同的存储格式可能适合另一种压缩算法。此外，这意味着，如果 RDBMS 提供一个选项，为特定列选择特定的压缩算法，或者从众所周知的标准压缩库列表(如 zlib、 lz4、 zstd、 snappy、 gzip 等)中选择特定的用户定义类型，那么这种方法将更加有益。或者，库算法都可以是完全定制的。</p><p>并且提供了与 CPU 内核紧密耦合的压缩、加密和 SIMD 硬件加速器，这些硬件加速器可以通过压缩或加密算法加以利用。其中一个例子是<a href="https://github.com/kunpengcompute/KAEzip">Kunpeng Zlib Acceleration Engine</a>, 它提供了一个支持硬件的基础设施，用于在“ Kunpeng 920” ARM64处理器上进行压缩。我还没有机会测试这种能力，但它听起来很有希望。</p><p>此外，压缩/加密算法在数据上执行重复的任务，这是利用 SIMD 向量化的自然选择。已经有一些独立的项目在 ARM64和 Intel 上进行，以便在 zlib、 lz4等著名的压缩库中进行这种特定于平台的增强。参看<a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/neon-programmers-guide-for-armv8-a/neon-intrinsics-chromium-case-study/adler-32">NEON Intrinsics case study</a> 关于优化 zlib 的 adler-32算法的 NEON intrinsic 案例研究。</p><p>所有这些都直接表明，RDBMS 服务器迫切需要为用户提供针对特定表或特定列的本地压缩算法/库的选择。在写这篇文章的时候，PostgreSQL 使用基于 LZ<a href="https://doxygen.postgresql.org/pg__lzcompress_8c_source.html">它自己的内建压缩算法</a> 来压缩Toast 表。想象一下，如果有一个用于选择 zlib 的接口，而不是内置的算法。进一步，选择 zlib 压缩级别。更进一步，为用户添加一个界面来创建一个扩展，该扩展使用特定平台的自定义算法，该平台使用硬件加速。</p><p>OK，我们正在实现一个这样的特性。在 PostgreSQL 黑客社区中查看这个 <a href="https://www.postgresql.org/message-id/flat/CAFiTN-uUpX3ck%3DK0mLEk-G_kUQY%3DSNOTeqdaNRR9FMdQrHKebw%40mail.gmail.com#81b25677aea9423d8ebb3feebcd1af46">讨论主题</a>。这个特性可能还有很长的路要走(截至本文撰写之时) ，但是我对这个特性充满希望，因为如上所示，用例足够强大，对这个功能没有反对意见，并且提交了work-in-progress的补丁。</p><p>我查看了这个补丁，玩了一下。粗略地说，下面是操作界面的样子。在补丁集完全具体化之后，接口可能会有所不同，但我认为它的本质或多或少会保持不变。下面是我的测试结果; 请注意，这只是为了通过例子强调这个功能是多么的酷和有用，并且使我在这个博客中解释的任何东西都有意义。</p><p>CREATE TABLE zlibtab(t TEXT COMPRESSION zlib WITH (level ‘4’));<br>CREATE TABLE lztab(t TEXT);<br>ALTER TABLE lztab ALTER COLUMN t SET COMPRESSION pglz;              </p><p>pgg:s2:pg$ time psql -c “\copy zlibtab from text.data”<br>COPY 13050                                    </p><p>real  0m1.344s<br>user  0m0.031s<br>sys   0m0.026s                                 </p><p>pgg:s2:pg$ time psql -c “\copy lztab from text.data”<br>COPY 13050                                    </p><p>real  0m2.088s<br>user  0m0.008s<br>sys   0m0.050s                                 </p><p>pgg:s2:pg$ time psql -c “select pg_table_size(‘zlibtab’::regclass), pg_table_size(‘lztab’::regclass)”<br> pg_table_size | pg_table_size<br>—————+—————<br>    1261568 |    1687552                          </p><p>pgg:s2:pg$ time psql -c “select NULL from zlibtab where t like ‘0000’” &gt; /dev/null</p><p>real  0m0.127s<br>user  0m0.000s<br>sys   0m0.002s</p><p>pgg:s2:pg$ time psql -c “select NULL from lztab where t like ‘0000’” &gt; /dev/null</p><p>real  0m0.050s<br>user  0m0.002s<br>sys   0m0.000s</p><p>注意两种不同的压缩算法在压缩大小、插入数据(压缩)和选择数据(解压)的速度上是如何不同的。</p><p>你甚至可以创建一个新的压缩访问函数，就像我们创建一个新的索引一样:</p><p>CREATE ACCESS METHOD pglz1 TYPE COMPRESSION HANDLER my_compression_handler;<br>其中my_compression_handler 应该是一个 PostgreSQL C 函数，可以使用 PostgreSQL 扩展创建。这个函数为一组预定义的钩子分配它自己的实现函数，这些钩子定义了 PostgreSQL 核心使用压缩访问方法所需要知道的一切: </p><p>Datum<br>my_compression_handler(PG_FUNCTION_ARGS)<br>{<br>    CompressionAmRoutine *routine = makeNode(CompressionAmRoutine);</p><p>​    routine-&gt;cmcheck = my_cmcheck;<br>​    routine-&gt;cminitstate = my_cminitstate;<br>​    routine-&gt;cmcompress = my_cmcompress;<br>​    routine-&gt;cmdecompress = my_cmdecompress;<br>​    routine-&gt;cmdecompress_slice = NULL;</p><p>​    PG_RETURN_POINTER(routine);<br>}</p><p>这是 PostgreSQL 高度可扩展的方式: 允许用户使用内置方法，但也为用户提供了一种方法来定义他/她自己的方法来完成相同的工作。上面的所有函数都在一个 PostgreSQL 扩展中，可以使用:<br>CREATE EXTENSION my_compression;</p></div><div id="English" class="tab-content"><p> Every modern database system has some way to compress its data at some level. The obvious reason for this feature is to reduce the size of it’s database, especially in today’s world where the data is growing exponentially. The less obvious reason is to improve query performance; the idea is: smaller data size means less data pages to scan, which means lesser disk i/o and faster data access. So, in any case, data de-compression should be fast enough so as not to hamper the query performance, if not improve it.</p><p>Compression is offered at different levels : page compression, row compression, column compression, etc. Columnar databases have the advantage of a very high compression ratio of its column because of presence of a repetetive pattern of contiguous data in a column. Another case is when, in a row oriented database, the column values are so large that it makes sense to compress individual values of the column. Such values can even be kept separately if they do not fit in a single page. And the row has pointers to the out-of-line compressed data. In PostgreSQL, such technique is called TOAST (The Oversized-Attribute Storage Technique), where, for columns that can contain variable-length data, the data is transparently compressed and stored in the same row, or else if it is still too large, it is stored in smaller chunks as rows in a separate table called a toast table, where these chunks themselves may or may not be compressed.</p><p>Compression is offered for different purposes. It may not be restricted for only data compression. E.g. in a replication system, the transfer of redo logs from the master to slave can become a huge network bottleneck, so many RDBMS offer to compress redo logs.</p><p>And then comes the compression algorithms that the RDBMS uses or gives options to choose. This applies especially more to data compression. Since data is user’s data, a specific pattern in the user data might suit a particular compression algorithm, while a different pattern might be suitable for another compression algorithm. Moreover, this implies that it would be far more beneficial if the RDBMS gives an option to choose a specific compression algorithm for a specific column or a specific user-defined type out of a list of well-known standard compression libraries such as zlib, lz4, ztd, snappy, gzip, etc. Or, the library algorithm may very well be a completely customized one.</p><p>Secondly, there has been a lot of advancements to optimize compression algorithms for specific platforms, and provide hardware accelerators for Compression, Encryption and SIMD that are closely coupled to CPU cores, which can then be levergaed by compression or encryption algorithms. One such example is the <a href="https://github.com/kunpengcompute/KAEzip">Kunpeng Zlib Acceleration Engine</a>, which offers a hardware-enabled infrastructure for compression on a “Kunpeng 920” ARM64 processor. I haven’t got a chance to test this capability, but it does sound promising.</p><p>Furthermore, the compression/encryption algorithms inherently do repetitive tasks over the data, which is a natural fit for leveraging SIMD vectorization. There has been independent projects going on on both ARM64 and Intel to do such platform-specific enhancements in well known libraries like zlib, lz4 etc. Check out this <a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/neon-programmers-guide-for-armv8-a/neon-intrinsics-chromium-case-study/adler-32">NEON Intrinsics case study</a> that optimizes zlib’s adler-32 algorithm using NEON intrinsics.</p><p>All this directly points to an urgent need for RDBMS servers to give users a choice for specific native compression algorithms/libraries for specific tables or specific columns. As of this writing, PostgreSQL uses <a href="https://doxygen.postgresql.org/pg__lzcompress_8c_source.html">its own built-in compression algorithm</a> based on LZ for toast table compression. Imagine if there were an interface to select zlib instead of the built-in algorithm. Further, select the zlib compression level. Still further, add an interface for users to create an extension that uses a customized algorithm native to a specific platform that uses hardware acceleration.</p><p>Well, there is exactly such a proposed feature in the making. Check out this <a href="https://www.postgresql.org/message-id/flat/CAFiTN-uUpX3ck%3DK0mLEk-G_kUQY%3DSNOTeqdaNRR9FMdQrHKebw%40mail.gmail.com#81b25677aea9423d8ebb3feebcd1af46">discussion thread</a> in the PostgreSQL hackers community. It may be a long way to go (as of this writing), but I am very hopeful of this feature going in, because the use-cases are strong enough as shown above, there are no fundamental objections to this functionality, and there are work-in-progress patches submitted.</p><p>I went ahead and applied this patch, and played around it. Roughly, below is how the interface looks like. After the patch-set fully materializes, the interface might be different, but I think the essence of it would remain more or less the same. Below is the output of my tests; please note that it is just to emphasize with examples how cool and useful this feature would be, and to make sense of whatever I explained above in this blog.</p><p>CREATE TABLE zlibtab(t TEXT COMPRESSION zlib WITH (level ‘4’));<br>CREATE TABLE lztab(t TEXT);<br>ALTER TABLE lztab ALTER COLUMN t SET COMPRESSION pglz;              </p><p>pgg:s2:pg$ time psql -c “\copy zlibtab from text.data”<br>COPY 13050                                    </p><p>real  0m1.344s<br>user  0m0.031s<br>sys   0m0.026s                                 </p><p>pgg:s2:pg$ time psql -c “\copy lztab from text.data”<br>COPY 13050                                    </p><p>real  0m2.088s<br>user  0m0.008s<br>sys   0m0.050s                                 </p><p>pgg:s2:pg$ time psql -c “select pg_table_size(‘zlibtab’::regclass), pg_table_size(‘lztab’::regclass)”<br> pg_table_size | pg_table_size<br>—————+—————<br>    1261568 |    1687552                          </p><p>pgg:s2:pg$ time psql -c “select NULL from zlibtab where t like ‘0000’” &gt; /dev/null</p><p>real  0m0.127s<br>user  0m0.000s<br>sys   0m0.002s</p><p>pgg:s2:pg$ time psql -c “select NULL from lztab where t like ‘0000’” &gt; /dev/null</p><p>real  0m0.050s<br>user  0m0.002s<br>sys   0m0.000s</p><p>Notice how two different compression algorithms differ in the compressed size, and the speed of inserting data (compression) and selecting data (decompression).</p><p>You would even be able to create a new compression access method using the same way as we do for creating a new index :<br>CREATE ACCESS METHOD pglz1 TYPE COMPRESSION HANDLER my_compression_handler;<br>where my_compression_handler should be a PostgreSQL C function that could be created using a PostgreSQL extension. This function assigns its own implementation functions for a set of pre-defined hooks that define everything that the PostgreSQL core needs to know to make use of the compression access method :</p><p>Datum<br>my_compression_handler(PG_FUNCTION_ARGS)<br>{<br>    CompressionAmRoutine *routine = makeNode(CompressionAmRoutine);</p><p>​    routine-&gt;cmcheck = my_cmcheck;<br>​    routine-&gt;cminitstate = my_cminitstate;<br>​    routine-&gt;cmcompress = my_cmcompress;<br>​    routine-&gt;cmdecompress = my_cmdecompress;<br>​    routine-&gt;cmdecompress_slice = NULL;</p><p>​    PG_RETURN_POINTER(routine);<br>}</p><p>This is PostgreSQL’s way of being highly extensible : Allow user to use built-in methods, but also provide a way for the user to define his/her own methods for doing the same job. All the above functions would be inside an PostgreSQL extension, that could be created using:<br>CREATE EXTENSION my_compression;</p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Amit Dattatray Khandekar&lt;br&gt;原文链接: &lt;a href=&quot;https://amitdkhan-pg.blogspot.com/2020/08/need-for-external-compression-methods.html&quot;&gt;https://amitdkhan-pg.blogspot.com/2020/08/need-for-external-compression-methods.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Amit PSQL专家分析压缩库在PostgreSQL中的迫切诉求。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Aarch64服务器应用软件开发需要添加的编译参数</title>
    <link href="https://kunpengcompute.github.io/2020/08/29/aarch64-fu-wu-qi-ying-yong-ruan-jian-kai-fa-xu-yao-tian-jia-de-bian-yi-can-shu/"/>
    <id>https://kunpengcompute.github.io/2020/08/29/aarch64-fu-wu-qi-ying-yong-ruan-jian-kai-fa-xu-yao-tian-jia-de-bian-yi-can-shu/</id>
    <published>2020-08-29T05:00:55.000Z</published>
    <updated>2021-07-08T02:56:40.868Z</updated>
    
    <content type="html"><![CDATA[<p>作者： <a href="https://github.com/zhaorenhai">zhaorenhai</a></p><p>   本文简单总结了一下，在aarch64服务器平台进行应用软件开发或者移植工作，编译代码时，编译器应该添加哪些选项。网上类似文章不少，但是由于arm平台涉及了移动开发，嵌入式开发，服务器开发各个领域，编译方式也有交叉编译，本地编译等，而且编译器也有gcc，armcc，armclang，clang等等多种，再加上arm平台历史版本众多，又分了32位，64位，网上这些文档一般都不明确说明文档涉及的开发平台，对应指令集版本，需要的编译器之类的情况，让人看上去比较头疼。 </p><a id="more"></a><p>本文涉及的范围：aarch64 Linux 服务器平台应用软件开发，不涉及交叉编译，只讨论本地编译情况， 只讨论可以免费使用的开源gcc编译器（clang也是可以免费可以使用的开源编译器，但是编译参数全面兼容gcc，所以本文只讨论gcc）。 另外本文也只讨论和aarch64平台强相关的参数，通用的编译参数讨论的文章比较多，本文不再赘述。<br>   最主要的参考资料来自于这篇文档：<a href="https://gcc.gnu.org/onlinedocs/gcc/AArch64-Options.html。">https://gcc.gnu.org/onlinedocs/gcc/AArch64-Options.html。</a> 本文主要讨论最主要的也是最容易引起混淆的-march，-mtune，-mcpu参数。其余参数一般都是在一些极特殊的场景下才用到，比如需要改变代码模型，需要改变数据模型，需要指定字节序为大端字节序等等，或者开发特殊的软件比如gcc编译器之类的时候才用到，一般服务器应用软件开发很少用到，所以一般保持默认值即可，后续如果有实际遇到的场景，会再补充到本文中。</p><p>  先看-march参数，原文描述：<br>-march=name<br>Specify the name of the target architecture and, optionally, one or more feature modifiers. This option has the form -march=arch{+[no]feature}*.<br>The table below summarizes the permissible values for arch and the features that they enable by default:</p><table><thead><tr><th>arch value</th><th>Architecture</th><th>Includes by default</th></tr></thead><tbody><tr><td>‘armv8-a’</td><td>Armv8-A</td><td>‘+fp’, ‘+simd’</td></tr><tr><td>‘armv8.1-a’</td><td>Armv8.1-A</td><td>‘armv8-a’, ‘+crc’, ‘+lse’, ‘+rdma’</td></tr><tr><td>‘armv8.2-a’</td><td>Armv8.2-A</td><td>‘armv8.1-a’</td></tr><tr><td>‘armv8.3-a’</td><td>Armv8.3-A</td><td>‘armv8.2-a’</td></tr><tr><td>‘armv8.4-a’</td><td>Armv8.4-A</td><td>‘armv8.3-a’, ‘+fp16fml’, ‘+dotprod’</td></tr><tr><td>‘armv8.5-a’</td><td>Armv8.5-A</td><td>‘armv8.4-a’, ‘+sb’, ‘+ssbs’, ‘+predres’</td></tr><tr><td>‘armv8.6-a’</td><td>Armv8.6-A</td><td>‘armv8.5-a’, ‘+bf16’, ‘+i8mm’</td></tr></tbody></table><p>The value ‘native’ is available on native AArch64 GNU/Linux and causes the compiler to pick the architecture of the host system. This option has no effect if the compiler is unable to recognize the architecture of the host system,<br>The permissible values for feature are listed in the sub-section on -march and -mcpu Feature Modifiers. Where conflicting feature modifiers are specified, the right-most feature is used.<br>GCC uses name to determine what kind of instructions it can emit when generating assembly code. If -march is specified without either of -mtune or -mcpu also being specified, the code is tuned to perform well across a range of target processors implementing the target architecture.</p><p>这个参数可以指定编译的目标架构，这里的架构指得是ARM CPU的指令集架构，那我们应该指定哪个版本的指令集架构，才能做到最好的兼容性，能兼容市场上所有的ARM CPU，或者既能兼顾兼容性，又能兼顾一部分性能，毕竟新版本的指令集架构能用上更新的具有特定功能的指令集。 这个需要看一下目前市场上所有的ARM服务器CPU的情况，网上目前已有的相关文档要么比较老，信息不全，要么就是出现把移动CPU和服务器CPU都放到一起的情况，比较混乱。所以本文重新搜集了目前所有ARM服务器CPU的信息，如下表（资料来源于互联网）：</p><table><thead><tr><th>厂商</th><th>型号</th><th>技术指标</th><th>微架构</th><th>指令集架构</th><th>发布时间</th></tr></thead><tbody><tr><td>Ampere</td><td>X-gene-1</td><td>8c   2.4GHz   40nm</td><td>Storm</td><td>ARMv8.0-A</td><td>2012</td></tr><tr><td>Ampere</td><td>X-gene-2</td><td>8c   2.4GHz   28nm</td><td>Shadowcat</td><td>ARMv8.0-A</td><td>2015</td></tr><tr><td>Ampere</td><td>eMag8180   (X-gene-3)</td><td>32c   2.8GHz-3.3GHz   16nm</td><td>Skylark</td><td>ARMv8.0-A</td><td>2018</td></tr><tr><td>Ampere</td><td>Altra</td><td>32-80c   1.7-3.3GHz</td><td>Quicksilver   (Neoverse N1)</td><td>ARMv8.2-A</td><td>2020年3月</td></tr><tr><td>Ampere</td><td>Altra Max</td><td>128c   7nm</td><td>Quicksilver   (Neoverse N1)</td><td>ARMv8.2-A</td><td>2020年6月</td></tr><tr><td>Cavium   (被Marvell收购)</td><td>ThunderX   (CN8890)</td><td>48c   1.9GHz   28nm</td><td>ThunderX1</td><td>ARMv8.1-A</td><td>2016</td></tr><tr><td>Cavium   (被Marvell收购)</td><td>ThunderX2   (CN99xx系列)</td><td>32-54c   1.6-2.5GHz   14nm 16nm</td><td>Vulcan   (从Broadcom收购)</td><td>ARMv8.1-A</td><td>2018年5月</td></tr><tr><td>Cavium   (被Marvell收购)</td><td>ThunderX3</td><td>96c   3.1GHz   7nm</td><td>Triton</td><td>ARMv8.3-A</td><td>2020</td></tr><tr><td>Qualcomm</td><td>Centriq 2400系列</td><td>40-48c   2.2-2.6GHz   10nm</td><td>Falkor</td><td>ARMv8.0-A</td><td>2017</td></tr><tr><td>Phytium</td><td>FT1500</td><td>16c   1.6GHz   28nm</td><td>Earth</td><td>ARMv8.0-A</td><td>2017</td></tr><tr><td>Phytium</td><td>FT2000+</td><td>64c   2.3GHz   16nm</td><td>Mars</td><td>ARMv8.0-A</td><td>2017</td></tr><tr><td>Phytium</td><td>S2500</td><td>64c   2.0-2.2GHz   16nm</td><td></td><td>ARMv8.0-A</td><td>2020</td></tr><tr><td>HiSilicon</td><td>Hi1616</td><td>32c   2.4GHz   16nm</td><td>Cortex-A72</td><td>ARMv8.0-A</td><td>2017</td></tr><tr><td>HiSilicon</td><td>Kunpeng 920   (Hi1620)</td><td>48c   3.0GHz   7nm</td><td>Taishan V110</td><td>ARMv8.2-A</td><td>2019</td></tr><tr><td>Annapurna Labs   （被AWS收购）</td><td>Gravtion</td><td>16c   2.3GHz   16nm</td><td>Cortex-A72</td><td>ARMv8.0-A</td><td>2018年11月</td></tr><tr><td>Annapurna Labs   （被AWS收购）</td><td>Gravtion2</td><td>64c   2.5GHz   7nm</td><td>Neoverse N1</td><td>ARMv8.2-A</td><td>2019年12月</td></tr><tr><td>Nuvia</td><td>Phoenix</td><td></td><td></td><td></td><td></td></tr></tbody></table><p>备注1：此表仅搜集了64位的arm cpu相关信息，32位的arm服务器cpu比较老，实际应用也不多，可以不用再考虑兼容工作。<br>备注2: 此表仅包含通用arm服务器cpu相关信息，不包含移动平台的cpu以及超算cpu，更不包含嵌入式以及实时计算cpu。</p><p>可以看出来市场上还是有不少ARMv8.0-A指令集架构的CPU，所以如果要做到最佳兼容性，-march参数的name的值就要指定为armv8-a。但是也要看到，高通实际上已经退出服务器CPU市场，Cavium，Ampere，Phytium各有一部分市场，但都不大，Nuvia是新晋厂商，还没有产品问世，市场规模最大的应该主要是海思的Kunpeng920和亚马逊的Gravition2，依托于华为云和亚马逊云，应用较广泛。而且从上图中可以看出各厂商最近两年新出的CPU，基本都已经支持了ARMv8.2-A指令集，考虑到近两年ARM服务器应用刚刚开始大规模普及，特别是华为云和亚马逊云上刚刚开始大规模上线ARM的云服务器实例，所以一些新开发的应用软件或者新迁移的应用软件，甚至可以指定为armv8.2-a，这样可以做到兼容性和性能兼顾。<br>另外-march参数还支持指定一些扩展选项，下面我们来逐个看下这些扩展选项：</p><table><thead><tr><th>扩展选项</th><th>描述</th></tr></thead><tbody><tr><td>crc</td><td>从ARMv8.1-a之后就默认包含了此扩展选项，所以当-march参数的name值为armv8-a的时候，才需要添加此选项，如果代码中涉及crc相关功能和指令，建议加上此选项</td></tr><tr><td>crypto</td><td>加密选项，这个是个额外扩展，没有哪个版本的指令集默认包含此选项，所以如果代码中涉及加密相关功能和指令，都要加上此选项。一般的应用开发涉及这个的比较少，所以这个看实际应用情况来决定要不要加</td></tr><tr><td>fp</td><td>浮点指令，从ARMv8-a就开始默认支持，可以忽略</td></tr><tr><td>simd</td><td>高级单指令多数据流指令，从ARMv8-a就开始默认支持，可以忽略</td></tr><tr><td>sve</td><td>目前已出的cpu支持此功能的较少，当前还不能加</td></tr><tr><td>lse</td><td>最新的原子指令集需要此选项，从ARMv8.1-a之后就默认包含，当-march参数的name值为armv8-a的时候，而且确定CPU支持此扩展选项的情况下可以添加。</td></tr><tr><td>rdma</td><td>乘积累加指令，在一些特殊场景会用到，使用较少。不过从ARMv8.1-a之后就默认包含。</td></tr><tr><td>fp16</td><td>半精度浮点指令，在一些图形软件中会用到，用途较少，建议视使用情况添加。</td></tr></tbody></table><p>还有一些其他的扩展选项，不过都是在更新版本的指令集中才支持，当前已经问世的CPU都不支持，这里就不再讨论。</p><p>再来看-mtune参数，原文描述：<br>-mtune=name<br>Specify the name of the target processor for which GCC should tune the performance of the code. Permissible values for this option are: ‘generic’, ‘cortex-a35’, ‘cortex-a53’, ‘cortex-a55’, ‘cortex-a57’, ‘cortex-a72’, ‘cortex-a73’, ‘cortex-a75’, ‘cortex-a76’, ‘cortex-a76ae’, ‘cortex-a77’, ‘cortex-a65’, ‘cortex-a65ae’, ‘cortex-a34’, ‘ares’, ‘exynos-m1’, ‘emag’, ‘falkor’, ‘neoverse-e1’,‘neoverse-n1’,‘qdf24xx’, ‘saphira’, ‘phecda’, ‘xgene1’, ‘vulcan’, ‘octeontx’, ‘octeontx81’, ‘octeontx83’, ‘octeontx2’, ‘octeontx2t98’, ‘octeontx2t96’ ‘octeontx2t93’, ‘octeontx2f95’, ‘octeontx2f95n’, ‘octeontx2f95mm’, ‘a64fx’, ‘thunderx’, ‘thunderxt88’, ‘thunderxt88p1’, ‘thunderxt81’, ‘tsv110’, ‘thunderxt83’, ‘thunderx2t99’, ‘thunderx3t110’, ‘zeus’, ‘cortex-a57.cortex-a53’, ‘cortex-a72.cortex-a53’, ‘cortex-a73.cortex-a35’, ‘cortex-a73.cortex-a53’, ‘cortex-a75.cortex-a55’, ‘cortex-a76.cortex-a55’ ‘native’.<br>The values ‘cortex-a57.cortex-a53’, ‘cortex-a72.cortex-a53’, ‘cortex-a73.cortex-a35’, ‘cortex-a73.cortex-a53’, ‘cortex-a75.cortex-a55’, ‘cortex-a76.cortex-a55’ specify that GCC should tune for a big.LITTLE system.<br>Additionally on native AArch64 GNU/Linux systems the value ‘native’ tunes performance to the host system. This option has no effect if the compiler is unable to recognize the processor of the host system.<br>Where none of -mtune=, -mcpu= or -march= are specified, the code is tuned to perform well across a range of target processors.<br>This option cannot be suffixed by feature modifiers.<br>这个参数比较简单，实际上就是指定目标CPU微架构，让gcc编译的时候，根据目标cpu的微架构进行特定的优化，比如指定为tsv110时，gcc就会根据鲲鹏920的微架构，进行一些指令的流水线重排，会提高一些性能。但是指定这个，同时也就意味着牺牲了兼容性，编译后的软件只能在目标CPU平台上运行。</p><p>最后看一下-mcpu参数，原文描述：<br>-mcpu=name<br>Specify the name of the target processor, optionally suffixed by one or more feature modifiers. This option has the form -mcpu=cpu{+[no]feature}*, where the permissible values for cpu are the same as those available for -mtune. The permissible values for feature are documented in the sub-section on -march and -mcpu Feature Modifiers. Where conflicting feature modifiers are specified, the right-most feature is used.<br>GCC uses name to determine what kind of instructions it can emit when generating assembly code (as if by -march) and to determine the target processor for which to tune for performance (as if by -mtune). Where this option is used in conjunction with -march or -mtune, those options take precedence over the appropriate part of this option.<br>可以看出来，这个参数实际上就是前面两个参数的综合，而且指定两个参数的优先级会高于这个参数。据说后续这个参数会被干掉。建议可以忽略此参数，只用前面两个参数即可。</p><p>总结：在aarch64服务器平台，进行应用软件开发，编译时，除了通用的编译选项之外，一般只需要添加-march=armv8-a即可，考虑到crc扩展选项应用比较多，可以加上crc扩展选项，比如-march=armv8-a+crc，这样可以做到最好的兼容性。 如果确定平台支持最新的原子指令，可以再加上+lse，其余的扩展选项建议根据实际应用情况添加。 如果既想兼顾兼容性和性能，建议直接指定-march=armv8.2-a，这样不仅包含了crc和lse扩展选项，而且最两年新出的CPU都可以做到兼容。 如果想做到最佳性能，还可以添加-mtune参数，但是这个要确保编译出来的软件只在目标平台运行。<br>另外，由于编译器的习惯， ARM平台上char的默认类型为unsigned char，这与x86正好相反（x86的char默认是有符号的），在x86上运行稳定的代码，移植过来将遇到char类型的变化，会带来不少问题。由于x86上的char类型与我们的编程习惯更一致，所以我们一般将ARM平台上的程序的char类型指定为signed char。所以还需要添加编译选项-fsigned-char。</p><p>最后，上述编译选项和扩展是在不同的gcc版本里逐步得到支持的，所以添加这些选项还要注意gcc的版本，对于-march的 name和扩展选项的支持情况，可以参考ARM官方的这个文档：<a href="https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/architecture-support">https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/architecture-support</a>     对于-mtune选项的支持建议参考不同版本的gcc文档，主要看AArch64-Options这个章节就行。</p><p>参考资料：<br><a href="https://gcc.gnu.org/onlinedocs/gcc/AArch64-Options.html">https://gcc.gnu.org/onlinedocs/gcc/AArch64-Options.html</a><br><a href="https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/architecture-support">https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/architecture-support</a><br><a href="https://clang.llvm.org/docs/ClangCommandLineReference.html">https://clang.llvm.org/docs/ClangCommandLineReference.html</a><br><a href="http://www.semiinsights.com/s/electronic_components/23/39697.shtml">http://www.semiinsights.com/s/electronic_components/23/39697.shtml</a><br><a href="http://news.eeworld.com.cn/qrs/2019/ic-news011452649.html">http://news.eeworld.com.cn/qrs/2019/ic-news011452649.html</a><br><a href="http://news.eeworld.com.cn/xfdz/ic497298.html">http://news.eeworld.com.cn/xfdz/ic497298.html</a><br><a href="https://www.cirmall.com/articles/31558">https://www.cirmall.com/articles/31558</a><br><a href="https://koolshare.cn/thread-147215-1-3.html">https://koolshare.cn/thread-147215-1-3.html</a><br><a href="http://www.360doc.com/content/20/0101/18/99071_883541215.shtml">http://www.360doc.com/content/20/0101/18/99071_883541215.shtml</a><br><a href="https://en.wikichip.org/wiki/">https://en.wikichip.org/wiki/</a><br><a href="https://blog.csdn.net/u014470361/article/details/85988772">https://blog.csdn.net/u014470361/article/details/85988772</a><br><a href="https://www.cnblogs.com/panda-w/p/11003389.html">https://www.cnblogs.com/panda-w/p/11003389.html</a><br><a href="https://developer.arm.com/ip-products/processors/cortex-a/cortex-a78">https://developer.arm.com/ip-products/processors/cortex-a/cortex-a78</a><br><a href="https://en.wikipedia.org/wiki/Comparison_of_ARMv8-A_cores">https://en.wikipedia.org/wiki/Comparison_of_ARMv8-A_cores</a><br><a href="https://aijishu.com/a/1060000000133361">https://aijishu.com/a/1060000000133361</a><br><a href="http://phytium.com.cn/article/5">http://phytium.com.cn/article/5</a><br><a href="https://www.sohu.com/a/361552782_163726?scm=1002.44003c.fe021c.PC_ARTICLE_REC">https://www.sohu.com/a/361552782_163726?scm=1002.44003c.fe021c.PC_ARTICLE_REC</a><br><a href="https://www.infoq.cn/article/34moxVRHI8qprbh9I1mp">https://www.infoq.cn/article/34moxVRHI8qprbh9I1mp</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者： &lt;a href=&quot;https://github.com/zhaorenhai&quot;&gt;zhaorenhai&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;   本文简单总结了一下，在aarch64服务器平台进行应用软件开发或者移植工作，编译代码时，编译器应该添加哪些选项。网上类似文章不少，但是由于arm平台涉及了移动开发，嵌入式开发，服务器开发各个领域，编译方式也有交叉编译，本地编译等，而且编译器也有gcc，armcc，armclang，clang等等多种，再加上arm平台历史版本众多，又分了32位，64位，网上这些文档一般都不明确说明文档涉及的开发平台，对应指令集版本，需要的编译器之类的情况，让人看上去比较头疼。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/categories/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
    
  </entry>
  
  <entry>
    <title>ARM使能Landscape</title>
    <link href="https://kunpengcompute.github.io/2020/08/04/arm-shi-neng-landscape/"/>
    <id>https://kunpengcompute.github.io/2020/08/04/arm-shi-neng-landscape/</id>
    <published>2020-08-04T11:32:25.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>ARM使能Landscape</p><table><thead><tr><th>Field</th><th>Project</th><th>Status</th><th>Links</th></tr></thead><tbody><tr><td>Big Data</td><td>Hadoop</td><td>ARM CI</td><td><a href="https://ci-hadoop.apache.org/job/hadoop-qbt-linux-ARM-trunk/">https://ci-hadoop.apache.org/job/hadoop-qbt-linux-ARM-trunk/</a></td></tr><tr><td>Big Data</td><td>Hadoop</td><td>软件包发布</td><td><a href="https://hadoop.apache.org/docs/r3.3.0/index.html">https://hadoop.apache.org/docs/r3.3.0/index.html</a></td></tr><tr><td>Big Data</td><td>Spark</td><td>ARM CI</td><td><a href="https://amplab.cs.berkeley.edu/jenkins/label/spark-arm/">https://amplab.cs.berkeley.edu/jenkins/label/spark-arm/</a></td></tr><tr><td>Big Data</td><td>Hive</td><td>ARM CI</td><td><a href="https://ci-hadoop.apache.org/job/Hive-trunk-linux-ARM/">https://ci-hadoop.apache.org/job/Hive-trunk-linux-ARM/</a></td></tr><tr><td>Big Data</td><td>HBase</td><td>ARM CI</td><td><a href="https://ci-hadoop.apache.org/job/HBase/job/HBase-Nightly-ARM/">https://ci-hadoop.apache.org/job/HBase/job/HBase-Nightly-ARM/</a></td></tr><tr><td>Big Data</td><td>Flink</td><td>ARM CI</td><td><a href="https://status.openlabtesting.org/builds?project=apache%2Fflink">https://status.openlabtesting.org/builds?project=apache%2Fflink</a></td></tr><tr><td>Big Data</td><td>Kudu</td><td>ARM CI</td><td><a href="http://status.openlabtesting.org/builds?project=apache%2Fkudu">http://status.openlabtesting.org/builds?project=apache%2Fkudu</a></td></tr><tr><td>Database</td><td>MariaDB</td><td>ARM CI</td><td><a href="https://buildbot.mariadb.org/#/builders">https://buildbot.mariadb.org/#/builders</a></td></tr><tr><td>Database</td><td>Greenplum</td><td>ARM CI</td><td><a href="https://github.com/greenplum-db/gpdb">https://github.com/greenplum-db/gpdb</a></td></tr><tr><td>AI</td><td>TensorFlow</td><td>ARM CI</td><td><a href="https://github.com/tensorflow/tensorflow#community-supported-builds">https://github.com/tensorflow/tensorflow#community-supported-builds</a></td></tr><tr><td>AI</td><td>Pytorch</td><td>ARM CI</td><td><a href="https://github.com/pytorch/pytorch">https://github.com/pytorch/pytorch</a></td></tr></tbody></table><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ARM使能Landscape&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Project&lt;/th&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;Links&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;Big Data&lt;/td&gt;
&lt;td&gt;Hadoop&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://ci-hadoop.apache.org/job/hadoop-qbt-linux-ARM-trunk/&quot;&gt;https://ci-hadoop.apache.org/job/hadoop-qbt-linux-ARM-trunk/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Big Data&lt;/td&gt;
&lt;td&gt;Hadoop&lt;/td&gt;
&lt;td&gt;软件包发布&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://hadoop.apache.org/docs/r3.3.0/index.html&quot;&gt;https://hadoop.apache.org/docs/r3.3.0/index.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Big Data&lt;/td&gt;
&lt;td&gt;Spark&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://amplab.cs.berkeley.edu/jenkins/label/spark-arm/&quot;&gt;https://amplab.cs.berkeley.edu/jenkins/label/spark-arm/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Big Data&lt;/td&gt;
&lt;td&gt;Hive&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://ci-hadoop.apache.org/job/Hive-trunk-linux-ARM/&quot;&gt;https://ci-hadoop.apache.org/job/Hive-trunk-linux-ARM/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Big Data&lt;/td&gt;
&lt;td&gt;HBase&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://ci-hadoop.apache.org/job/HBase/job/HBase-Nightly-ARM/&quot;&gt;https://ci-hadoop.apache.org/job/HBase/job/HBase-Nightly-ARM/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Big Data&lt;/td&gt;
&lt;td&gt;Flink&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://status.openlabtesting.org/builds?project=apache%2Fflink&quot;&gt;https://status.openlabtesting.org/builds?project=apache%2Fflink&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Big Data&lt;/td&gt;
&lt;td&gt;Kudu&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://status.openlabtesting.org/builds?project=apache%2Fkudu&quot;&gt;http://status.openlabtesting.org/builds?project=apache%2Fkudu&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Database&lt;/td&gt;
&lt;td&gt;MariaDB&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://buildbot.mariadb.org/#/builders&quot;&gt;https://buildbot.mariadb.org/#/builders&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Database&lt;/td&gt;
&lt;td&gt;Greenplum&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/greenplum-db/gpdb&quot;&gt;https://github.com/greenplum-db/gpdb&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AI&lt;/td&gt;
&lt;td&gt;TensorFlow&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/tensorflow/tensorflow#community-supported-builds&quot;&gt;https://github.com/tensorflow/tensorflow#community-supported-builds&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AI&lt;/td&gt;
&lt;td&gt;Pytorch&lt;/td&gt;
&lt;td&gt;ARM CI&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch&quot;&gt;https://github.com/pytorch/pytorch&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>ARM&#39;s LSE (for atomics) and MySQL</title>
    <link href="https://kunpengcompute.github.io/2020/08/04/arm-s-lse-for-atomics-and-mysql/"/>
    <id>https://kunpengcompute.github.io/2020/08/04/arm-s-lse-for-atomics-and-mysql/</id>
    <published>2020-08-04T06:10:00.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Krunal Bauskar<br>原文链接: <a href="https://mysqlonarm.github.io/ARM-LSE-and-MySQL/">https://mysqlonarm.github.io/ARM-LSE-and-MySQL/</a></p><p>来看Mysql大牛Krunal带你分析LSE在Mysql上的情况。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><h1 id="ARM’s-LSE-for-atomics-and-MySQL"><a href="#ARM’s-LSE-for-atomics-and-MySQL" class="headerlink" title="ARM’s LSE (for atomics) and MySQL"></a>ARM’s LSE (for atomics) and MySQL</h1><p>ARM 在其 ARMv8.1规范中引入了 LSE (Large System Extensions)。这意味着如果你的处理器是兼容 ARMv8.1的，它将支持 LSE。LSE 的目的是优化原子指令，通过使用单个 CAS (比较并交换)或 SWP (用于交换)等替换旧式的独占负载存储… … 众所周知，上述扩展本质上会提高使用原子的应用程序的性能。</p><h2 id="理解LSE"><a href="#理解LSE" class="headerlink" title="理解LSE"></a>理解LSE</h2><p>为了更好地理解 LSE，让我们看一个工作示例，看看代码是如何生成的，以及可能的优化。</p><h3 id="LSE-turned-off"><a href="#LSE-turned-off" class="headerlink" title="LSE turned off"></a>LSE turned off</h3><p><img src="https://mysqlonarm.github.io/images/blog10/img1.png" alt="img"><br><img src="https://mysqlonarm.github.io/images/blog10/img2.png" alt="img"></p><p>正如您所看到的，有一个用于执行 CAS 的循环。加载值，检查期望值，如果不同，然后存储值。主循环是一个5步进程与2个独占指令与各自的内存顺序。SWAP 也有一个检查存储是否成功的循环。</p><hr><p>ARM 有多种不同的加载/存储指令，因此在继续之前，让我们花一分钟理解这些变体。</p><p><img src="https://mysqlonarm.github.io/images/blog10/img3.png" alt="img"></p><p><strong>stlxrb:</strong> 与发布语义/排序互斥的存储。提供对上述cacheline的独占访问权。“b”表示字节。（其他 half-word (2), word(4), double-word (8))。</p><p><strong>stlrb:</strong> 与发布语义/排序互斥的存储。帮助建议只进行语义排序，而不进行独占访问。</p><p><strong>strb:</strong> 非原子变量的普通存储(没有排序语义)</p><p>自然有人会问，为什么 <code>seq-cst</code> 和 <code>release memory order</code> 都会产生相同的asm指令。这是因为 ARM-v8中的<strong>store-release in ARM-v8 is multi-copy atomic</strong>，也就是说，如果一个agent看到了存储-释放，那么所有agent 都看到了存储-释放。没有要求普通存储为多拷贝原子存储。[类似于 x86_64中的 mov+fence 或 xchg ]。</p><hr><h3 id="LSE-开启"><a href="#LSE-开启" class="headerlink" title="LSE 开启"></a>LSE 开启</h3><p>所以现在让我们看看如果我们现在打开 lse 会发生什么。LSE 支持是在 ARM-v8.1规范中添加的，因此如果默认编译已经完成，gcc 将尝试使二进制文件与更广泛的 aarch64处理器兼容，并且可能无法启用特定的功能。为了支持 lse 用户需要指定额外的编译标志:</p><p>有多种方式打开lse:</p><ul><li>使用 gcc-6+ 编译，指定 lse 标志为-march = armv8-a+lse</li><li>通过指定 ARMv8.1(或更高版本)使用 gcc-6+ 编译(这将自动启用所有 ARMv8.1功能)。-march = ARMv8.1-a</li></ul><p><img src="https://mysqlonarm.github.io/images/blog10/img4.png" alt="img"></p><p>不再有 while 循环。单条指令(CASALB)执行比较和交换(负责加载和存储) ，与 SWAPLB 执行交换的方法相同。看起来是进行了优化。更多关于性能的信息请见下文。</p><p>但是有一个问题，如果二进制文件是用 +lse 支持编译的，但是目标计算机不支持 lse，因为它只与 arm-v8兼容。通过引入 <code>-moutline-atomics</code> ，gcc-9.4+ 解决了这个问题(使用<code>-mno-outline-atomics</code>禁用 gcc-10.1启用的默认值)。GCC 自动匹配带有动态检查变量(lse 和 non-lse)的代码。运行时作出决定，并执行相应的变量。</p><p>让我们看看如果使用 gcc-10(使用 <code>-moutline-atomic</code>使其在所有 aarch64机器上兼容)编译会发生什么)</p><table><thead><tr><th align="left">code</th><th align="left">asm (perf output)</th></tr></thead><tbody><tr><td align="left">bool expected = true; flag.compare_exchange_strong(expected, false);</td><td align="left">&lt;<strong>aarch64_cas1_acq_rel&gt;: __aarch64_cas1_acq_rel(): │ adrp x16, 11000 &lt;</strong>data_start&gt; │ ldrb w16, [x16, #25] │ ↓ cbz w16, 14 │ casalb w0, w1, [x2] │ ← ret │14: uxtb w16, w0 │ 18: ldaxrb w0, [x2] │ cmp w0, w16 │ ↓ b.ne 2c │ stlxrb w17, w1, [x2] │ ↑ cbnz w17, 18 │2c: ← ret</td></tr></tbody></table><p>请注意用于选择适当逻辑的分支指令(用绿色突出显示)。</p><h2 id="LSE-的表现"><a href="#LSE-的表现" class="headerlink" title="LSE 的表现"></a>LSE 的表现</h2><p>虽然这些听起来很有趣，但是真的有帮助吗？如果新的指令需要更多的周期怎么办。只有一种方法可以找到答案: 基准测试。</p><p>基准测试: 每个线程(总共 n 个线程)都尝试获得锁，这会导致严重的争用。一旦线程拥有了互斥锁，它就会执行基于 crc32的软件，在释放它之前让 cpu 一直处于繁忙状态。每个线程都执行这个流程 m 次。</p><p><img src="https://mysqlonarm.github.io/images/blog10/lse-microbenchmark.png" alt="img"></p><p>Machine: Bare-Metal with 128 cores ARM Kunpeng 920 2.6 Ghz.</p><ul><li>用例表示一个严重的争用，每个线程主要花费时间获得锁。这样的工作负载非常快(crc32在16KB 块上)</li><li>这清楚地证明，LSE严重争用的条件下起到帮助作用。</li></ul><p>但是微基准测试由于应用本身的特性，包括其他重叠部分，如 IO、其他处理元素等，有时不能显示原始应用所需的增益。.现在让我们评估一下lse使能在 MySQL 性能。</p><h3 id="MySQL-benchmarking-with-LSE"><a href="#MySQL-benchmarking-with-LSE" class="headerlink" title="MySQL benchmarking with LSE"></a>MySQL benchmarking with LSE</h3><p>环境描述:</p><ul><li>Server: MySQL-8.0.21, OS: CentOS-7</li><li>Sysbench based point-select, read-only, read-write, update-index and update-non-index workload.</li><li>Executed for higher scalability (&gt;= 64) to explore contention.</li><li>Configuration: 32 cores (single NUMA) ARM Kunpeng 920 2.6 Ghz (28 cores for server, 4 for sysbench)</li><li>Tried 2 use-cases uniform, zipfian (more contention)</li><li>baseline=lse-disabled, lse=lse-enabled (-march=armv8-a+lse).</li></ul><p><img src="https://mysqlonarm.github.io/images/blog10/sysbench-uniform-lseenabled.png" alt="img"><br><img src="https://mysqlonarm.github.io/images/blog10/sysbench-zipfian-lseenabled.png" alt="img"></p><h3 id="观察结果"><a href="#观察结果" class="headerlink" title="观察结果:"></a>观察结果:</h3><ul><li>在Uniform cases ，使用LSE下，我们几乎看不到任何区别</li><li>在Zipfian cases, LSE对于更新用例，会略微退化，但是始终如一(2-4%).</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>LSE 作为特性看起来很有前途，但在 MySQL 用例中确实没有多少提升。可能一旦 MySQL 被调整到使用更多的原子，LSE 可能会显示 +ve 的差异。在那之前，如果不启用 LSE，我们在Mysql不会失去任何性能。</p><p><em>如果有疑问，请联系我，或在下方留言</em></p></div><div id="English" class="tab-content"><p>ARM introduced LSE (Large System Extensions) as part of its ARMv8.1 specs. This means if your processor is ARMv8.1 compatible it would support LSE. LSE are meant to optimize atomic instructions by replacing the old styled exclusive load-store using a single CAS (compare-and-swap) or SWP (for exchange), etc…. Said extensions are known to inherently increase performance of applications using atomics.</p><h2 id="Understanding-LSE"><a href="#Understanding-LSE" class="headerlink" title="Understanding LSE"></a>Understanding LSE</h2><p>To better understand LSE let’s take a working example to see how the code is generated and possible optimization.</p><h3 id="LSE-turned-off-1"><a href="#LSE-turned-off-1" class="headerlink" title="LSE turned off"></a>LSE turned off</h3><p><img src="https://mysqlonarm.github.io/images/blog10/img1.png" alt="img"><br><img src="https://mysqlonarm.github.io/images/blog10/img2.png" alt="img"></p><p>As you can see there is a loop for doing CAS. Load the value, check with expected value and if different then store the value. Main loop is a 5 step process with 2 exclusive instructions with respective memory ordering. SWAP too has a loop for checking if the store is successful.</p><hr><p>ARM has multiple variant of load/store instructions so before we proceed let’s take a minute to understand these variants.</p><p><img src="https://mysqlonarm.github.io/images/blog10/img3.png" alt="img"></p><p><strong>stlxrb:</strong> store exclusive with release semantics/ordering. Provide exclusive access to the said cache line. “b” represents byte. (other variant half-word (2), word(4), double-word (8)).</p><p><strong>stlrb:</strong> store with release semantics/ordering helping suggest ordering semantics only but not exclusive access.</p><p><strong>strb:</strong> normal store to a non-atomic variable (no ordering semantics)</p><p>Naturally one may ask how come both <code>seq-cst</code> and <code>release memory order</code> generate the same asm instruction. This is because <strong>store-release in ARM-v8 is multi-copy atomic</strong>, that is, if one agent has seen a store-release, then all agents have seen the store-release. There are no requirements for ordinary stores to be multi-copy atomic. [Something similar to mov+fence or xchg in x86_64 domain].</p><hr><h3 id="LSE-turned-on"><a href="#LSE-turned-on" class="headerlink" title="LSE turned on"></a>LSE turned on</h3><p>So let’s now see what would happen if we now turn-lse on. LSE support was added with ARM-v8.1 specs and so if the default compilation is done, gcc will try to make binary compatible with a wider aarch64 processors and may not enable the specific functionality. In order to enable lse user need to specify extra compilation flags:</p><p>There are multiple ways to turn-on lse:</p><ul><li>Compile with gcc-6+ by specifying lse flag as -march=armv8-a+lse</li><li>Compile with gcc-6+ by specifying ARMv8.1 (or higher) (that will auto-enable all ARMv8.1 functionalities). -march=armv8.1-a</li></ul><p><img src="https://mysqlonarm.github.io/images/blog10/img4.png" alt="img"></p><p>No more while loop. Single instruction (CASALB) to do the compare and swap (that takes care of load and store) and same way SWAPLB to do the exchange. Sounds optimized. More about performance below.</p><p>But there is one problem, what if binaries are compiled with +lse support but the target machine doesn’t support lse as it is only arm-v8 compatible. This problem is solved with gcc-9.4+ by introducing <code>-moutline-atomics</code> (default enabled with gcc-10.1 can be disabled with <code>-mno-outline-atomics</code>). GCC auto emits a code with dynamic check with both variants (lse and non-lse). Runtime a decision is taken and accordingly said variant is executed.</p><p>Let’s see what is emitted if compiled with gcc-10 (with <code>-moutline-atomic</code> making it compatible on all aarch64 machines)</p><table><thead><tr><th align="left">code</th><th align="left">asm (perf output)</th></tr></thead><tbody><tr><td align="left">bool expected = true; flag.compare_exchange_strong(expected, false);</td><td align="left">&lt;<strong>aarch64_cas1_acq_rel&gt;: __aarch64_cas1_acq_rel(): │ adrp x16, 11000 &lt;</strong>data_start&gt; │ ldrb w16, [x16, #25] │ ↓ cbz w16, 14 │ casalb w0, w1, [x2] │ ← ret │14: uxtb w16, w0 │ 18: ldaxrb w0, [x2] │ cmp w0, w16 │ ↓ b.ne 2c │ stlxrb w17, w1, [x2] │ ↑ cbnz w17, 18 │2c: ← ret</td></tr></tbody></table><p>Notice the branching instruction (highlighted in green) to select appropriate logic.</p><h2 id="LSE-in-action"><a href="#LSE-in-action" class="headerlink" title="LSE in action"></a>LSE in action</h2><p>While all this sounds interesting but does it really help? What if the new instruction takes more cycles. Only one way to find out: Benchmark.</p><p>Benchmark: Simple spin-mutex with each thread (total N threads) trying to get the lock there-by causing heavy contention. Once the thread has the mutex it performs software based crc32 keeping the cpu bit busy before releasing it. Each thread does this M times.</p><p><img src="https://mysqlonarm.github.io/images/blog10/lse-microbenchmark.png" alt="img"></p><p>Machine: Bare-Metal with 128 cores ARM Kunpeng 920 2.6 Ghz.</p><ul><li>Use-case represent a serious contention with each thread mostly spending time for obtaining lock. Workload as such is pretty quick (crc32 on 16KB block).</li><li>This clearly proves that LSE helps in heavily contented cases.</li></ul><p>But micro-benchmark sometime fails to show the needed gain with original application due to nature of application including other overlap components like IO, other processing element, etc… So let’s now evaluate MySQL performance with lse-enabled.</p><h3 id="MySQL-benchmarking-with-LSE-1"><a href="#MySQL-benchmarking-with-LSE-1" class="headerlink" title="MySQL benchmarking with LSE"></a>MySQL benchmarking with LSE</h3><p>Workload:</p><ul><li>Server: MySQL-8.0.21, OS: CentOS-7</li><li>Sysbench based point-select, read-only, read-write, update-index and update-non-index workload.</li><li>Executed for higher scalability (&gt;= 64) to explore contention.</li><li>Configuration: 32 cores (single NUMA) ARM Kunpeng 920 2.6 Ghz (28 cores for server, 4 for sysbench)</li><li>Tried 2 use-cases uniform, zipfian (more contention)</li><li>baseline=lse-disabled, lse=lse-enabled (-march=armv8-a+lse).</li></ul><p><img src="https://mysqlonarm.github.io/images/blog10/sysbench-uniform-lseenabled.png" alt="img"><br><img src="https://mysqlonarm.github.io/images/blog10/sysbench-zipfian-lseenabled.png" alt="img"></p><h3 id="Observations"><a href="#Observations" class="headerlink" title="Observations:"></a>Observations:</h3><ul><li>With Uniform we hardly see any difference with use of LSE</li><li>With Zipfian LSE tend to regress marginally but consistently (by 2-4%) for update use-cases.</li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>LSE as feature looks promising but fails to perform in MySQL use-case. May be once MySQL is tuned to use more atomics, LSE could show a +ve difference. Till then nothing we would not loose if LSE is not enabled.</p><p><em>If you have more questions/queries do let me know. Will try to answer them.</em></p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Krunal Bauskar&lt;br&gt;原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/ARM-LSE-and-MySQL/&quot;&gt;https://mysqlonarm.github.io/ARM-LSE-and-MySQL/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;来看Mysql大牛Krunal带你分析LSE在Mysql上的情况。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Backtraces in PostgreSQL</title>
    <link href="https://kunpengcompute.github.io/2020/08/04/backtraces-in-postgresql/"/>
    <id>https://kunpengcompute.github.io/2020/08/04/backtraces-in-postgresql/</id>
    <published>2020-08-04T06:09:25.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Amit Dattatray Khandekar<br>原文链接: <a href="https://amitdkhan-pg.blogspot.com/2020/07/backtraces-in-postgresql.html">https://amitdkhan-pg.blogspot.com/2020/07/backtraces-in-postgresql.html</a></p><p>PGSQL 13引入了Backtraces特性，方便客户和管理者定位疑难问题，来看社区大牛Amit带你玩转它！</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>PostgreSQL 13引入了一个简单但非常有用的功能，在发生错误时将堆栈跟踪记录到服务器日志中。让我们看看细节</p><p>有一个 GUC 可以启用 生成 stacktrace: backtrace_functions. 设置为逗号隔开的function名字。</p><p>SET backtrace_functions TO ‘func1,func2’;</p><p>如果从这些函数之一抛出错误，将生成回溯跟踪并记录到服务器日志中。</p><p>注意只有超级用户可以设置该GUC。可以在本地会话中设置，也可以在postgresql.conf文件中全局设置。</p><p>在客户报告错误消息的情况下，很容易看出它是如何起作用的。我们可以在源代码中通过 grep 找到它的来源。但除此之外，就只能靠猜测了。现在不是了，随着这个功能的出现，现在，您可以要求客户将 backtrace_functions设置为所有发出此错误消息的函数，并获取堆栈跟踪。在大多数情况下，错误的根本原因并不是发生错误的函数; 它位于堆栈中间的某个位置; 因此堆栈跟踪是关键的</p><p> 这个功能已经在其他数据库中使用，比如 MySQL，Greenplum，Oracle。</p><p>但在 PostgreSQL 中仍然缺少的东西——这个问题也存在于大多数其他数据库中——是能够在服务器后端由于内存区段错误或其他类似意外信号崩溃时生成堆栈跟踪，或者当服务器由于某种原因而宕机时生成堆栈跟踪。这种能力会带来更大的不同。我们将摆脱生成核心文件的解释步骤。更重要的是，这在崩溃只是随机发生的情况下有所帮助。即使发生了单个意外的崩溃，客户也总是准备好了回溯。我希望这能在 PostgreSQL 的下一个主要版本中实现</p><p>让我们看看 PostgreSQL 堆栈跟踪日志是什么样的。我们将尝试使用一个不存在的类型来创建表。假设我们知道“ type does not exist”错误来自源代码中的 typenameType ()。所以我们这样做:</p><p>postgres=# set backtrace_functions TO ‘typenameType’;<br>postgres=# create table tab (id invalidtype);<br>ERROR: type “invalidtype” does not exist<br>LINE 1: create table tab (id invalidtype);</p><p> 以下是服务器日志中的一个片段: :<br>2020-07-28 20:17:01.482 CST [22454] ERROR: type “invalidtype” does not exist at character 22<br>2020-07-28 20:17:01.482 CST [22454] BACKTRACE:<br>  postgres: amit postgres [local] CREATE TABLE(typenameType+0xa4) [0xaaaaafcd2ac4]<br>  postgres: amit postgres [local] CREATE TABLE(+0x20f550) [0xaaaaafcd4550]<br>  postgres: amit postgres [local] CREATE TABLE(transformCreateStmt+0x53c) [0xaaaaafcd7a10]<br>  postgres: amit postgres [local] CREATE TABLE(+0x44df20) [0xaaaaaff12f20]<br>  postgres: amit postgres [local] CREATE TABLE(standard_ProcessUtility+0x16c) [0xaaaaaff1225c]<br>  postgres: amit postgres [local] CREATE TABLE(+0x44a4e4) [0xaaaaaff0f4e4]<br>  postgres: amit postgres [local] CREATE TABLE(+0x44af88) [0xaaaaaff0ff88]<br>  postgres: amit postgres [local] CREATE TABLE(PortalRun+0x198) [0xaaaaaff10ed8]<br>  postgres: amit postgres [local] CREATE TABLE(+0x44764c) [0xaaaaaff0c64c]<br>  postgres: amit postgres [local] CREATE TABLE(PostgresMain+0x970) [0xaaaaaff0d3d4]<br>  postgres: amit postgres [local] CREATE TABLE(+0x3b3be4) [0xaaaaafe78be4]<br>  postgres: amit postgres [local] CREATE TABLE(PostmasterMain+0xdc0) [0xaaaaafe79b70]<br>  postgres: amit postgres [local] CREATE TABLE(main+0x480) [0xaaaaafb82510]<br>  /lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0xe0) [0xffffaac956e0]<br>  postgres: amit postgres [local] CREATE TABLE(+0xbd5d8) [0xaaaaafb825d8]<br>2020-07-29 18:01:02.726 CST [28776] STATEMENT: create table tab (id invalidtype);  </p><p> 回溯的每一行都有函数名、该函数的偏移量和该帧的返回地址</p><p>对于某些堆栈帧，函数名不存在; 相反，函数地址存在。这些都是静态函数。对于这样的函数，函数名不会公开。但是我们可以通过 addr2line 命令行工具从他们的地址中获取他们的名字:</p><p>$ addr2line 0x20f550 0x44df20 -a -f -e <code>which postgres</code><br>0x000000000020f550<br>transformColumnDefinition<br>:?<br>0x000000000044df20<br>ProcessUtilitySlow.constprop.0<br>:?</p><p>如果是调试版本，甚至会打印文件名和偏移量</p><p>现在让我们看看这个简单的特性是如何实现的</p><p>在包括 PostgreSQL、 Greenplum、 MySQL 在内的大多数 RDBMS 中，这个特性都是通过一个简单的函数 backtrace ()来生成 stacktrace:</p><p>int backtrace(void **buffer, int size);</p><p>这个函数只返回帧的所有返回地址。因此，接下来应该调用 backtrace_symbols () ，该函数将 backtrace ()返回的地址转换成字符串，如果可用的话，使用函数名来描述地址:</p><p>char **backtrace_symbols(void *const *buffer, int size);</p><p>这些函数的所有细节都在其man手册页中得到了很好的描述。这些功能在大多数平台上都是可用的.</p><p>注意以下几点:</p><p>\1. 要使函数名可用于 backtrace_symbols () ，必须使用链接器选项构建可执行文件，这些链接器选项允许将所有这些符号添加到“动态符号表”中。这些选项可以通过以下方式之一给出(这些是 gcc 编译器选项) :<br>gcc -rdynamic<br>gcc -Wl,-E</p><p>\2. 当使用 gcc -O2或更高的优化级别编译时，有时可能会丢失特定的堆栈帧。例如，检查这个示例程序<a href="https://drive.google.com/file/d/1UYvT3POmZFmtSa17PcuNvyo9iva8XOS7/view?usp=sharing">backtrace.c 反向追踪</a> 从backtrace() man手册页.</p><p>不使用 -O2来进行编译 :<br>amit:pg:error$ gcc -rdynamic -o backtrace backtrace.c<br>I get the full stack :<br>amit:pg:error$ ./backtrace 6<br>backtrace() returned 11 addresses<br>./backtrace(myfunc3+0x2c) [0xaaaad6b2edc0]<br>./backtrace(+0xe84) [0xaaaad6b2ee84]<br>./backtrace(myfunc+0x2c) [0xaaaad6b2eebc]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(main+0x60) [0xaaaad6b2ef28]<br>/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0xe0) [0xffff8c5ba6e0]<br>./backtrace(+0xcc4) [0xaaaad6b2ecc4]</p><p>使用 -O2来进行编译  :<br>amit:pg:error$ gcc -O2 -rdynamic -o backtrace backtrace.c<br>amit:pg:error$ ./backtrace 6<br>backtrace() returned 4 addresses<br>./backtrace(myfunc3+0x38) [0xaaaac7183e40]<br>./backtrace(main+0x4c) [0xaaaac7183cfc]<br>/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0xe0) [0xffffb91286e0]<br>./backtrace(+0xd38) [0xaaaac7183d38]</p><p>myfunc2()和 myfunc ()没有框架。一种可能性是编译器用尾端调用 myfunc3()替换了 myfunc ()和 myfunc2()的递归调用，这被称为尾端调用优化.</p><p>重点是: 我们需要意识到在一些情况下这种缺失的框架</p></div><div id="English" class="tab-content"><p>PostgreSQL 13 has introduced a simple but extremely useful capability to log a stack trace into the server logs when an error is reported. Let’s see the details.</p><p>There is a GUC to enable stacktrace generation : backtrace_functions. Set it to a comma-separated function names.</p><p>SET backtrace_functions TO ‘func1,func2’;</p><p>If the error is thrown from one of these functions, a backtrace will be generated and logged into the server log.</p><p>Note that only superusers can set the backtrace_functions GUC. It can be set locally in a session, or can be included in postgresql.conf file to globally set it.</p><p>It’s easy to see how it would help in a situation where a customer reports an error message. We can find from where it came from by grep’ing for it in the source code. But beyond that, it was all guess work. Not anymore. Now, you can ask the customer to set backtrace_functions to all such functions which are emitting this error message, and get the stack trace. In most cases, the root cause of the error is not in the function which emits the error; its located somewhere in the middle of the stack; hence the stack trace is critical.</p><p>This capability is already available in many other databases like MySQL, Greenplum, Oracle.</p><p>What’s still missing in PostgreSQL - and is present in most of these other databases - is being able to generate stack trace when a server backend crashes with a segmentation fault or other such unexpected signals, or when the server PANICs due to some reason. This capability would make a much bigger difference. We will get rid of having to explain steps to generate core file. More importantly, this helps in situations where the crash happens only randomly. Even with a single unexpected crash, the customer would always be ready with a backtrace. I am hopeful this would be implemented in the next major release of PostgreSQL.</p><p>Let’s see how a PostgreSQL stack trace log looks like. We will try to use a non-existent type to create a table. Supposing we know that the “type does not exist” error comes from typenameType() in the source code. So we do this :</p><p>postgres=# set backtrace_functions TO ‘typenameType’;<br>postgres=# create table tab (id invalidtype);<br>ERROR:  type “invalidtype” does not exist<br>LINE 1: create table tab (id invalidtype);</p><p>Here’s a snippet from the server log :<br>2020-07-28 20:17:01.482 CST [22454] ERROR:  type “invalidtype” does not exist at character 22<br>2020-07-28 20:17:01.482 CST [22454] BACKTRACE:<br>    postgres: amit postgres [local] CREATE TABLE(typenameType+0xa4) [0xaaaaafcd2ac4]<br>    postgres: amit postgres [local] CREATE TABLE(+0x20f550) [0xaaaaafcd4550]<br>    postgres: amit postgres [local] CREATE TABLE(transformCreateStmt+0x53c) [0xaaaaafcd7a10]<br>    postgres: amit postgres [local] CREATE TABLE(+0x44df20) [0xaaaaaff12f20]<br>    postgres: amit postgres [local] CREATE TABLE(standard_ProcessUtility+0x16c) [0xaaaaaff1225c]<br>    postgres: amit postgres [local] CREATE TABLE(+0x44a4e4) [0xaaaaaff0f4e4]<br>    postgres: amit postgres [local] CREATE TABLE(+0x44af88) [0xaaaaaff0ff88]<br>    postgres: amit postgres [local] CREATE TABLE(PortalRun+0x198) [0xaaaaaff10ed8]<br>    postgres: amit postgres [local] CREATE TABLE(+0x44764c) [0xaaaaaff0c64c]<br>    postgres: amit postgres [local] CREATE TABLE(PostgresMain+0x970) [0xaaaaaff0d3d4]<br>    postgres: amit postgres [local] CREATE TABLE(+0x3b3be4) [0xaaaaafe78be4]<br>    postgres: amit postgres [local] CREATE TABLE(PostmasterMain+0xdc0) [0xaaaaafe79b70]<br>    postgres: amit postgres [local] CREATE TABLE(main+0x480) [0xaaaaafb82510]<br>    /lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0xe0) [0xffffaac956e0]<br>    postgres: amit postgres [local] CREATE TABLE(+0xbd5d8) [0xaaaaafb825d8]<br>2020-07-29 18:01:02.726 CST [28776] STATEMENT:  create table tab (id invalidtype);    </p><p>Each line of the backtrace has the function name, an offset into that function, and the return address of that frame.</p><p>For some stack frames, the function name is not present; instead, the function address is present. These are static functions. For such functions, the function names are not exposed. But we may be able to get their names from their addresses, with the help of addr2line command-line tool :</p><p>$ addr2line  0x20f550 0x44df20 -a -f -e <code>which postgres</code><br>0x000000000020f550<br>transformColumnDefinition<br>:?<br>0x000000000044df20<br>ProcessUtilitySlow.constprop.0<br>:?</p><p>If it’s a debug build, even the file name and offset is printed.</p><p>Now let’s see how this simple feature is implemented.</p><p>In most of the RDBMS’s including PostgreSQL, Greenplum, MySQL, the feature is implemented using a simple function backtrace() to generate the stacktrace:</p><p>int backtrace(void **buffer, int size);</p><p>This function only returns all the return addresses of the frames.  So it should be followed by a call to backtrace_symbols() that converts the addresses returned by backtrace() into strings that describe the addresses using the function names if available :</p><p>char **backtrace_symbols(void *const *buffer, int size);</p><p>All the details of these functions are nicely described in their man pages. These functions are available in most of the platforms.</p><p>Note a couple of points :</p><ol><li><p>For the function names to be available for backtrace_symbols(), the executable has to be built using linker options that allow adding all these symbols into a “dynamic symbol table”. These options can be given with one of the following ways (these are gcc compiler options) :<br>gcc -rdynamic<br>gcc -Wl,-E</p></li><li><p>Sometimes particular stack frames might be missing, when compiled with gcc -O2 or higher optimization level. E.g. check this sample program backtrace.c from the backtrace() man pages.</p></li></ol><p>I compile it without -O2 :<br>amit:pg:error$ gcc -rdynamic -o backtrace backtrace.c<br>I get the full stack :<br>amit:pg:error$ ./backtrace 6<br>backtrace() returned 11 addresses<br>./backtrace(myfunc3+0x2c) [0xaaaad6b2edc0]<br>./backtrace(+0xe84) [0xaaaad6b2ee84]<br>./backtrace(myfunc+0x2c) [0xaaaad6b2eebc]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(myfunc+0x24) [0xaaaad6b2eeb4]<br>./backtrace(main+0x60) [0xaaaad6b2ef28]<br>/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0xe0) [0xffff8c5ba6e0]<br>./backtrace(+0xcc4) [0xaaaad6b2ecc4]</p><p>Now I compile it with -O2 :<br>amit:pg:error$ gcc -O2 -rdynamic -o backtrace backtrace.c<br>amit:pg:error$ ./backtrace 6<br>backtrace() returned 4 addresses<br>./backtrace(myfunc3+0x38) [0xaaaac7183e40]<br>./backtrace(main+0x4c) [0xaaaac7183cfc]<br>/lib/aarch64-linux-gnu/libc.so.6(__libc_start_main+0xe0) [0xffffb91286e0]<br>./backtrace(+0xd38) [0xaaaac7183d38]</p><p>There is no frame for myfunc2() and myfunc(). One possibility is that the compiler has replaced the recursive calls of myfunc() and also myfunc2() call with the tail end call myfunc3(), which is called tail call optimization.</p><p>The point being: we need to be aware of such missing frames in a few scenarios.</p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Amit Dattatray Khandekar&lt;br&gt;原文链接: &lt;a href=&quot;https://amitdkhan-pg.blogspot.com/2020/07/backtraces-in-postgresql.html&quot;&gt;https://amitdkhan-pg.blogspot.com/2020/07/backtraces-in-postgresql.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PGSQL 13引入了Backtraces特性，方便客户和管理者定位疑难问题，来看社区大牛Amit带你玩转它！&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>HAproxy X86 vs ARM64性能比拼</title>
    <link href="https://kunpengcompute.github.io/2020/07/14/haproxy-x86-vs-arm64-xing-neng-bi-pin/"/>
    <id>https://kunpengcompute.github.io/2020/07/14/haproxy-x86-vs-arm64-xing-neng-bi-pin/</id>
    <published>2020-07-14T03:20:58.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>译者: <a href="https://github.com/wangxiyuan">wangxiyuan</a><br>作者: <a href="https://github.com/martin-g">Martin Grigorov</a><br>原文链接: <a href="https://medium.com/@martin.grigorov/compare-haproxy-performance-on-x86-64-and-arm64-cpu-architectures-bfd55d1d5566">https://medium.com/@martin.grigorov/compare-haproxy-performance-on-x86-64-and-arm64-cpu-architectures-bfd55d1d5566</a></p><p>本文是由Apache Tomcat PMC Martin带来的Haproxy最新版本的性能测试报告。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>HAProxy v2.2在几天前刚刚<a href="https://www.haproxy.com/fr/blog/announcing-haproxy-2-2/">发布</a>，所以我决定在 x86_64和 aarch64 虚拟机上对它运行<a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">负载测试</a>:</p><ul><li>x86_64</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Architecture:                    x86_64</span><br><span class="line">CPU op-mode(s):                  32-bit, 64-bit</span><br><span class="line">Byte Order:                      Little Endian</span><br><span class="line">Address sizes:                   42 bits physical, 48 bits virtual</span><br><span class="line">CPU(s):                          8</span><br><span class="line">On-line CPU(s) list:             0-7</span><br><span class="line">Thread(s) per core:              2</span><br><span class="line">Core(s) per socket:              4</span><br><span class="line">Socket(s):                       1</span><br><span class="line">NUMA node(s):                    1</span><br><span class="line">Vendor ID:                       GenuineIntel</span><br><span class="line">CPU family:                      6</span><br><span class="line">Model:                           85</span><br><span class="line">Model name:                      Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz</span><br><span class="line">Stepping:                        7</span><br><span class="line">CPU MHz:                         3000.000</span><br><span class="line">BogoMIPS:                        6000.00</span><br><span class="line">Hypervisor vendor:               KVM</span><br><span class="line">Virtualization type:             full</span><br><span class="line">L1d cache:                       128 KiB</span><br><span class="line">L1i cache:                       128 KiB</span><br><span class="line">L2 cache:                        4 MiB</span><br><span class="line">L3 cache:                        30.3 MiB</span><br><span class="line">NUMA node0 CPU(s):               0-7</span><br><span class="line">Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nons</span><br><span class="line">                                 top_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowpref</span><br><span class="line">                                 etch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx51</span><br><span class="line">                                 2cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat avx512_vnni md_clear flush_l1d arch_capabilities</span><br></pre></td></tr></table></figure><ul><li>aarch64</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Architecture:                    aarch64</span><br><span class="line">CPU op-mode(s):                  64-bit</span><br><span class="line">Byte Order:                      Little Endian</span><br><span class="line">CPU(s):                          8</span><br><span class="line">On-line CPU(s) list:             0-7</span><br><span class="line">Thread(s) per core:              1</span><br><span class="line">Core(s) per socket:              8</span><br><span class="line">Socket(s):                       1</span><br><span class="line">NUMA node(s):                    1</span><br><span class="line">Vendor ID:                       0x48</span><br><span class="line">Model:                           0</span><br><span class="line">Stepping:                        0x1</span><br><span class="line">CPU max MHz:                     2400.0000</span><br><span class="line">CPU min MHz:                     2400.0000</span><br><span class="line">BogoMIPS:                        200.00</span><br><span class="line">L1d cache:                       512 KiB</span><br><span class="line">L1i cache:                       512 KiB</span><br><span class="line">L2 cache:                        4 MiB</span><br><span class="line">L3 cache:                        32 MiB</span><br><span class="line">NUMA node0 CPU(s):               0-7</span><br><span class="line">Flags:                           fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma dcpop asimddp asimdfhm</span><br></pre></td></tr></table></figure><p>注意: 我尽可能的让虚拟机的硬件配置更接近，即使用相同的 RAM 类型和大小、相同的磁盘、网卡和带宽。此外，cpu 尽可能相似，但难免有一些差异：</p><ul><li>CPU 频率: 3000 MHz (x86 _ 64) vs 2400 MHz (aarch64)</li><li>BogoMIPS: 6000(x86 _ 64) vs 200(aarch64)</li><li>一级缓存: 128 KiB (x86 _ 64) vs 512 KiB (aarch64)</li></ul><p>两个虚拟机都运行在最新版的Ubuntu 20.04上。</p><p>我的HAProxy 是从<a href="https://github.com/haproxy/haproxy">master</a>分支的源代码构建的，代码与HAProxy v2.2的几乎没有区别。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">HA-Proxy version 2.3-dev0 2020&#x2F;07&#x2F;07 - https:&#x2F;&#x2F;haproxy.org&#x2F;</span><br><span class="line">Status: development branch - not safe for use in production.</span><br><span class="line">Known bugs: https:&#x2F;&#x2F;github.com&#x2F;haproxy&#x2F;haproxy&#x2F;issues?q&#x3D;is:issue+is:open</span><br><span class="line">Running on: Linux 5.4.0-40-generic #44-Ubuntu SMP Mon Jun 22 23:59:48 UTC 2020 aarch64</span><br><span class="line">Build options :</span><br><span class="line">  TARGET  &#x3D; linux-glibc</span><br><span class="line">  CPU     &#x3D; generic</span><br><span class="line">  CC      &#x3D; clang-9</span><br><span class="line">  CFLAGS  &#x3D; -O2 -Wall -Wextra -Wdeclaration-after-statement -fwrapv -Wno-address-of-packed-member -Wno-unused-label -Wno-sign-compare -Wno-unused-parameter -Wno-missing-field-initializers -Wno-string-plus-int -Wtype-limits -Wshift-negative-value -Wnull-dereference -Werror</span><br><span class="line">  OPTIONS &#x3D; USE_PCRE&#x3D;1 USE_PCRE_JIT&#x3D;1 USE_OPENSSL&#x3D;1 USE_LUA&#x3D;1 USE_ZLIB&#x3D;1 USE_DEVICEATLAS&#x3D;1 USE_51DEGREES&#x3D;1 USE_WURFL&#x3D;1 USE_SYSTEMD&#x3D;1</span><br><span class="line"></span><br><span class="line">Feature list : +EPOLL -KQUEUE +NETFILTER +PCRE +PCRE_JIT -PCRE2 -PCRE2_JIT +POLL -PRIVATE_CACHE +THREAD -PTHREAD_PSHARED +BACKTRACE -STATIC_PCRE -STATIC_PCRE2 +TPROXY +LINUX_TPROXY +LINUX_SPLICE +LIBCRYPT +CRYPT_H +GETADDRINFO +OPENSSL +LUA +FUTEX +ACCEPT4 +ZLIB -SLZ +CPU_AFFINITY +TFO +NS +DL +RT +DEVICEATLAS +51DEGREES +WURFL +SYSTEMD -OBSOLETE_LINKER +PRCTL +THREAD_DUMP -EVPORTS</span><br><span class="line"></span><br><span class="line">Default settings :</span><br><span class="line">  bufsize &#x3D; 16384, maxrewrite &#x3D; 1024, maxpollevents &#x3D; 200</span><br><span class="line"></span><br><span class="line">Built with multi-threading support (MAX_THREADS&#x3D;64, default&#x3D;8).</span><br><span class="line">Built with OpenSSL version : OpenSSL 1.1.1f  31 Mar 2020</span><br><span class="line">Running on OpenSSL version : OpenSSL 1.1.1f  31 Mar 2020</span><br><span class="line">OpenSSL library supports TLS extensions : yes</span><br><span class="line">OpenSSL library supports SNI : yes</span><br><span class="line">OpenSSL library supports : TLSv1.0 TLSv1.1 TLSv1.2 TLSv1.3</span><br><span class="line">Built with Lua version : Lua 5.3.3</span><br><span class="line">Built with DeviceAtlas support (dummy library only).</span><br><span class="line">Built with 51Degrees Pattern support (dummy library).</span><br><span class="line">Built with WURFL support (dummy library version 1.11.2.100)</span><br><span class="line">Built with network namespace support.</span><br><span class="line">Built with zlib version : 1.2.11</span><br><span class="line">Running on zlib version : 1.2.11</span><br><span class="line">Compression algorithms supported : identity(&quot;identity&quot;), deflate(&quot;deflate&quot;), raw-deflate(&quot;deflate&quot;), gzip(&quot;gzip&quot;)</span><br><span class="line">Built with transparent proxy support using: IP_TRANSPARENT IPV6_TRANSPARENT IP_FREEBIND</span><br><span class="line">Built with PCRE version : 8.39 2016-06-14</span><br><span class="line">Running on PCRE version : 8.39 2016-06-14</span><br><span class="line">PCRE library supports JIT : yes</span><br><span class="line">Encrypted password support via crypt(3): yes</span><br><span class="line">Built with clang compiler version 9.0.1 </span><br><span class="line"></span><br><span class="line">Available polling systems :</span><br><span class="line">      epoll : pref&#x3D;300,  test result OK</span><br><span class="line">       poll : pref&#x3D;200,  test result OK</span><br><span class="line">     select : pref&#x3D;150,  test result OK</span><br><span class="line">Total: 3 (3 usable), will use epoll.</span><br><span class="line"></span><br><span class="line">Available multiplexer protocols :</span><br><span class="line">(protocols marked as &lt;default&gt; cannot be specified using &#39;proto&#39; keyword)</span><br><span class="line">            fcgi : mode&#x3D;HTTP       side&#x3D;BE        mux&#x3D;FCGI</span><br><span class="line">       &lt;default&gt; : mode&#x3D;HTTP       side&#x3D;FE|BE     mux&#x3D;H1</span><br><span class="line">              h2 : mode&#x3D;HTTP       side&#x3D;FE|BE     mux&#x3D;H2</span><br><span class="line">       &lt;default&gt; : mode&#x3D;TCP        side&#x3D;FE|BE     mux&#x3D;PASS</span><br><span class="line"></span><br><span class="line">Available services : none</span><br><span class="line"></span><br><span class="line">Available filters :</span><br><span class="line">[SPOE] spoe</span><br><span class="line">[COMP] compression</span><br><span class="line">[TRACE] trace</span><br><span class="line">[CACHE] cache</span><br><span class="line">[FCGI] fcgi-app</span><br></pre></td></tr></table></figure><p>我已经试图通过遵循我在<a href="https://cbonte.github.io/haproxy-dconv/">官方文档</a>和网络上找到的所有最佳实践来尽可能地优化它。</p><p>HAProxy的配置如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">global</span><br><span class="line">  log stdout format raw local0 err</span><br><span class="line">#  nbproc                            8</span><br><span class="line">  nbthread                          32</span><br><span class="line">  cpu-map                           1&#x2F;all 0-7</span><br><span class="line">  tune.ssl.default-dh-param         2048</span><br><span class="line">  tune.ssl.capture-cipherlist-size  1</span><br><span class="line">  ssl-server-verify                 none</span><br><span class="line">  maxconn                           32748</span><br><span class="line">  daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  timeout client                    60s</span><br><span class="line">  timeout client-fin                 1s</span><br><span class="line">  timeout server                    30s</span><br><span class="line">  timeout server-fin                 1s</span><br><span class="line">  timeout connect                   10s</span><br><span class="line">  timeout http-request              10s</span><br><span class="line">  timeout http-keep-alive           10s</span><br><span class="line">  timeout queue                     10m</span><br><span class="line">  timeout check                     10s</span><br><span class="line">  mode                              http</span><br><span class="line">  log                               global</span><br><span class="line">  option                            dontlog-normal</span><br><span class="line">  option                            httplog</span><br><span class="line">  option                            dontlognull</span><br><span class="line">  option                            http-use-htx</span><br><span class="line">  option                            http-server-close</span><br><span class="line">  option                            http-buffer-request</span><br><span class="line">  option                            redispatch</span><br><span class="line">  retries                           3000</span><br><span class="line"></span><br><span class="line">frontend test_fe</span><br><span class="line">  bind :::8080</span><br><span class="line">  #bind :::8080 ssl crt &#x2F;home&#x2F;ubuntu&#x2F;tests&#x2F;tls&#x2F;server.pem</span><br><span class="line">  default_backend test_be</span><br><span class="line"></span><br><span class="line">backend test_be</span><br><span class="line">  #balance roundrobin</span><br><span class="line">  balance leastconn</span><br><span class="line">  #balance random(2)</span><br><span class="line">  server go1 127.0.0.1:8081 no-check </span><br><span class="line">  server go2 127.0.0.1:8082 no-check</span><br><span class="line">  server go3 127.0.0.1:8083 no-check </span><br><span class="line">  server go4 127.0.0.1:8084 no-check</span><br></pre></td></tr></table></figure><p>通过这种方式，HAProxy 被用作四个 HTTP 服务的前端负载均衡器。</p><p>想使用 SSL方式的话，只需要注释掉第34行并取消注释第35行。</p><p>我使用了<a href="https://cbonte.github.io/haproxy-dconv/2.2/configuration.html#nbthread">多线程</a>设置以获得最佳结果。正如文档所说，这是推荐的设置，而且它也使吞吐量提高了近两倍！此外经过我把吞吐量从8个线程增加到16个线程，再从16个线程增加到32个线程的设置后，发现使用32个线程的效果最好，当使用64个线程时吞吐量开始下降。</p><p>我还使用<code>CPU-map 1/all 0-7</code>将线程<a href="https://cbonte.github.io/haproxy-dconv/2.2/configuration.html#cpu-map">固定</a>在同一个 CPU中。</p><p>另一个重要的设置是用于平衡后端的算法。就像Willy Tarreau的<a href="https://www.haproxy.com/blog/power-of-two-load-balancing/">测试</a>一样。</p><p>正如在 HAProxy Enterprice <a href="https://www.haproxy.com/documentation/hapee/1-7r2/configuration/system-tuning/#disable-irqbalance">文档</a>中所推荐的，我已经禁用了<code>irqbalance</code>。</p><p>最后，我应用了以下内核设置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl -w net.ipv4.ip_local_port_range&#x3D;&quot;1024 65024&quot;</span><br><span class="line">sudo sysctl -w net.ipv4.tcp_max_syn_backlog&#x3D;100000</span><br><span class="line">sudo sysctl -w net.core.netdev_max_backlog&#x3D;100000</span><br><span class="line">sudo sysctl -w net.ipv4.tcp_tw_reuse&#x3D;1</span><br><span class="line">sudo sysctl -w fs.file-max&#x3D;500000</span><br></pre></td></tr></table></figure><p><code>fs.file-max</code> 也与<code>/etc/security/limits. conf</code>中的一些更改有关:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root soft nofile 500000</span><br><span class="line">root hard nofile 500000</span><br><span class="line">* soft nofile 500000</span><br><span class="line">* hard nofile 500000</span><br></pre></td></tr></table></figure><p>对于后端，我使用了用 Golang 编写的非常简单的 HTTP 服务器。他们只是将“ Hello World”写回客户机，而不从磁盘或网络读/写:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; run with: env PORT&#x3D;8081 go run http-server.go</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">    &quot;fmt&quot;</span><br><span class="line">    &quot;log&quot;</span><br><span class="line">    &quot;net&#x2F;http&quot;</span><br><span class="line">    &quot;os&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line"></span><br><span class="line">    port :&#x3D; os.Getenv(&quot;PORT&quot;)</span><br><span class="line">    if port &#x3D;&#x3D; &quot;&quot; &#123;</span><br><span class="line">      log.Fatal(&quot;Please specify the HTTP port as environment variable, e.g. env PORT&#x3D;8081 go run http-server.go&quot;)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    http.HandleFunc(&quot;&#x2F;&quot;, func(w http.ResponseWriter, r *http.Request)&#123;</span><br><span class="line">        fmt.Fprintf(w, &quot;Hello World&quot;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    log.Fatal(http.ListenAndServe(&quot;:&quot; + port, nil))</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对用负载测试客户端，我使用了与<a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">测试Apache Tomcat</a>相同设置的<a href="https://github.com/wg/wrk">WRK</a>。</p><p>结果如下:</p><ul><li>aarch64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line"> 8 threads and 96 connections</span><br><span class="line"> Thread Stats Avg    Stdev   Max +&#x2F;-  Stdev</span><br><span class="line"> Latency     6.67ms  8.82ms 196.74ms  89.85%</span><br><span class="line"> Req&#x2F;Sec     2.60k   337.06   5.79k   75.79%</span><br><span class="line"> 621350 requests in 30.09s, 75.85MB read</span><br><span class="line">Requests&#x2F;sec: 20651.69</span><br><span class="line">Transfer&#x2F;sec: 2.52MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line"> 8 threads and 96 connections</span><br><span class="line"> Thread Stats  Avg    Stdev    Max +&#x2F;-  Stdev</span><br><span class="line"> Latency      3.32ms  4.46ms  75.42ms   94.58%</span><br><span class="line"> Req&#x2F;Sec      4.71k   538.41   8.84k    82.41%</span><br><span class="line"> 1127664 requests in 30.10s, 137.65MB read</span><br><span class="line">Requests&#x2F;sec: 37464.85</span><br><span class="line">Transfer&#x2F;sec: 4.57MB</span><br></pre></td></tr></table></figure><ul><li>aarch64, HTTPS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line"> 8 threads and 96 connections</span><br><span class="line"> Thread Stats    Avg   Stdev    Max +&#x2F;-  Stdev</span><br><span class="line"> Latency       7.92ms  12.50ms  248.52ms 91.18%</span><br><span class="line"> Req&#x2F;Sec       2.42k   338.67   4.34k    80.88%</span><br><span class="line"> 578210 requests in 30.08s, 70.58MB read</span><br><span class="line">Requests&#x2F;sec: 19220.81</span><br><span class="line">Transfer&#x2F;sec: 2.35MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTPS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line"> 8 threads and 96 connections</span><br><span class="line"> Thread Stats   Avg   Stdev   Max +&#x2F;-  Stdev</span><br><span class="line"> Latency       3.56ms 4.83ms  111.51ms 94.25%</span><br><span class="line"> Req&#x2F;Sec       4.46k  609.37   7.23k   85.60%</span><br><span class="line"> 1066831 requests in 30.07s, 130.23MB read</span><br><span class="line">Requests&#x2F;sec: 35474.26</span><br><span class="line">Transfer&#x2F;sec: 4.33MB</span><br></pre></td></tr></table></figure><p>我们可以发现：</p><ul><li>在 x86_64 VM 上，HAProxy 的速度几乎是 aarch64 VM 的两倍。</li><li>并且 TLS offloading减少了5-8% 的吞吐量</li></ul><hr><p><strong>更新1</strong>(2020年7月10日) : 为了确定基于 Golang 的 HTTP 服务器是否是上述测试中的瓶颈，我决定直接针对一个后端(即跳过 HAProxy)运行相同的 WRK 负载测试。</p><ul><li>aarch64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency   615.31us  586.70us  22.44ms   90.61%</span><br><span class="line">    Req&#x2F;Sec    20.05k     1.57k   42.29k    73.62%</span><br><span class="line">  4794299 requests in 30.09s, 585.24MB read</span><br><span class="line">Requests&#x2F;sec: 159319.75</span><br><span class="line">Transfer&#x2F;sec:     19.45MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency   774.24us  484.99us  36.43ms   97.04%</span><br><span class="line">    Req&#x2F;Sec    15.28k   413.04    16.89k    73.57%</span><br><span class="line">  3658911 requests in 30.10s, 446.64MB read</span><br><span class="line">Requests&#x2F;sec: 121561.40</span><br><span class="line">Transfer&#x2F;sec:     14.84MB</span><br></pre></td></tr></table></figure><p>在这里我们看到运行在 aarch64上的 HTTP 服务比运行在 x86– 64上的要快30% ！</p><p>更重要的观察结果是，当根本不使用负载均衡器时，arm64的吞吐量要好几倍！我认为问题在于我的设置ーー HAProxy 和4个后端服务器都运行在同一个虚拟机上，所以它们在争夺资源！下面我计划把Golang服务固定到他们自己的 CPU 核心上，让 HAProxy 只使用其他4个 CPU 核心！敬请期待最新消息！</p><hr><p><strong>更新2</strong>(2020年7月10日) :</p><p>为了将进程固定到特定的 cpu，我将使用<code>numactl</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ numactl — hardware</span><br><span class="line">available: 1 nodes (0)</span><br><span class="line">node 0 cpus: 0 1 2 3 4 5 6 7</span><br><span class="line">node 0 size: 16012 MB</span><br><span class="line">node 0 free: 170 MB</span><br><span class="line">node distances:</span><br><span class="line">node 0</span><br><span class="line">0: 10</span><br></pre></td></tr></table></figure><p>我已经将 Golang HTTP 服务固定在以下几个方面:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">numactl — cpunodebind&#x3D;0 — membind&#x3D;0 — physcpubind&#x3D;4 env PORT&#x3D;8081 go run etc&#x2F;haproxy&#x2F;load&#x2F;http-server.</span><br><span class="line">go</span><br></pre></td></tr></table></figure><p>例如，这个后端实例被固定到 CPU 节点0和物理 CPU 4。其他三个后端服务分别固定在物理 cpu 5、6和7上。</p><p>我还对 HAProxy 的配置做了一些改动:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nbthread 4</span><br><span class="line">cpu-map 1&#x2F;all 0–3</span><br><span class="line"></span><br><span class="line">Nbthread 4cpu-map 1&#x2F;all 0-3</span><br></pre></td></tr></table></figure><p>也就是说，HAProxy 将产生4个线程，它们将被固定到物理 cpu 0-3上。</p><p>通过这些改变，aarch64的结果保持不变:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line">  4 threads and 16 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     1.44ms    2.11ms  36.48ms   88.36%</span><br><span class="line">    Req&#x2F;Sec     4.98k   651.34     6.62k    74.40%</span><br><span class="line">  596102 requests in 30.10s, 72.77MB read</span><br><span class="line">Requests&#x2F;sec:  19804.19</span><br><span class="line">Transfer&#x2F;sec:      2.42MB</span><br></pre></td></tr></table></figure><p>但是 x86_64下降了:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line">  4 threads and 16 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency   767.40us  153.24us  19.07ms   97.72%</span><br><span class="line">    Req&#x2F;Sec     5.21k   173.41     5.51k    63.46%</span><br><span class="line">  623911 requests in 30.10s, 76.16MB read</span><br><span class="line">Requests&#x2F;sec:  20727.89</span><br><span class="line">Transfer&#x2F;sec:      2.53MB</span><br></pre></td></tr></table></figure><p>对于 HTTP (没有 TLS)也是如此:</p><ul><li>aarch64</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.232:8080                                                                                                                                                                   </span><br><span class="line">  4 threads and 16 connections                                                                                                                                                                                 </span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev                                                                                                                                                            </span><br><span class="line">    Latency     1.40ms    2.16ms  36.55ms   88.08%                                                                                                                                                             </span><br><span class="line">    Req&#x2F;Sec     5.55k   462.65     6.97k    69.85%                                                                                                                                                             </span><br><span class="line">  665269 requests in 30.10s, 81.21MB read                                                                                                                                                                      </span><br><span class="line">Requests&#x2F;sec:  22102.12                                                                                                                                                                                        </span><br><span class="line">Transfer&#x2F;sec:      2.70MB</span><br></pre></td></tr></table></figure><ul><li>x86_64</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.206:8080                                                                                                                                                                   </span><br><span class="line">  4 threads and 16 connections                                                                                                                                                                                 </span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev                                                                                                                                                            </span><br><span class="line">    Latency   726.01us  125.04us   6.42ms   93.95%                                                                                                                                                             </span><br><span class="line">    Req&#x2F;Sec     5.51k   165.80     5.80k    57.24%                                                                                                                                                             </span><br><span class="line">  658777 requests in 30.10s, 80.42MB read                                                                                                                                                                      </span><br><span class="line">Requests&#x2F;sec:  21886.50                                                                                                                                                                                        </span><br><span class="line">Transfer&#x2F;sec:      2.67MB</span><br></pre></td></tr></table></figure><p>因此，现在 HAProxy 在 aarch64上的速度比 x86_64稍快一些，但仍然远远低于每秒120000多个请求的“空负载均衡器”方法。</p><hr><p><strong>更新3</strong>(2020年7月10日) : 在看到 Golang HTTP 服务的性能非常好(120-160K reqs/sec)并简化设置之后，我决定从 Update 2中删除 CPU固定，并使用来自其他 VM 的后端，例如，当在aarch64虚拟机上运行HAProxy时，它将在x86_64上运行的后端之间进行负载均衡；当使用WRK在x86_64上运行HAProxy时，它将使用aarch64虚拟机上运行的 Golang HTTP服务。以下是新的结果：</p><ul><li>aarch64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     6.33ms    4.93ms  76.85ms   89.14%</span><br><span class="line">    Req&#x2F;Sec     2.10k   316.84     3.52k    74.50%</span><br><span class="line">  501840 requests in 30.07s, 61.26MB read</span><br><span class="line">Requests&#x2F;sec:  16688.53</span><br><span class="line">Transfer&#x2F;sec:      2.04MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     5.32ms    6.71ms  71.29ms   90.25%</span><br><span class="line">    Req&#x2F;Sec     3.26k   639.12     4.14k    65.52%</span><br><span class="line">  779297 requests in 30.08s, 95.13MB read</span><br><span class="line">Requests&#x2F;sec:  25908.50</span><br><span class="line">Transfer&#x2F;sec:      3.16MB</span><br></pre></td></tr></table></figure><ul><li>aarch64, HTTPS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     6.17ms    5.41ms 292.21ms   91.08%</span><br><span class="line">    Req&#x2F;Sec     2.13k   238.74     3.85k    86.32%</span><br><span class="line">  506111 requests in 30.09s, 61.78MB read</span><br><span class="line">Requests&#x2F;sec:  16821.60</span><br><span class="line">Transfer&#x2F;sec:      2.05MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTPS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     3.40ms    2.54ms  58.66ms   97.27%</span><br><span class="line">    Req&#x2F;Sec     3.82k   385.85     4.55k    92.10%</span><br><span class="line">  914329 requests in 30.10s, 111.61MB read</span><br><span class="line">Requests&#x2F;sec:  30376.95</span><br><span class="line">Transfer&#x2F;sec:      3.71MB</span><br></pre></td></tr></table></figure><hr><p>祝你黑客生活愉快，注意安全！</p></div><div id="English" class="tab-content"><p>HAProxy 2.2 has been <a href="https://www.haproxy.com/fr/blog/announcing-haproxy-2-2/">released</a> few days ago so I’ve decided to run <a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">my load tests</a> against it on my x86_64 and aarch64 VMs:</p><ul><li>x86_64</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Architecture:                    x86_64</span><br><span class="line">CPU op-mode(s):                  32-bit, 64-bit</span><br><span class="line">Byte Order:                      Little Endian</span><br><span class="line">Address sizes:                   42 bits physical, 48 bits virtual</span><br><span class="line">CPU(s):                          8</span><br><span class="line">On-line CPU(s) list:             0-7</span><br><span class="line">Thread(s) per core:              2</span><br><span class="line">Core(s) per socket:              4</span><br><span class="line">Socket(s):                       1</span><br><span class="line">NUMA node(s):                    1</span><br><span class="line">Vendor ID:                       GenuineIntel</span><br><span class="line">CPU family:                      6</span><br><span class="line">Model:                           85</span><br><span class="line">Model name:                      Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz</span><br><span class="line">Stepping:                        7</span><br><span class="line">CPU MHz:                         3000.000</span><br><span class="line">BogoMIPS:                        6000.00</span><br><span class="line">Hypervisor vendor:               KVM</span><br><span class="line">Virtualization type:             full</span><br><span class="line">L1d cache:                       128 KiB</span><br><span class="line">L1i cache:                       128 KiB</span><br><span class="line">L2 cache:                        4 MiB</span><br><span class="line">L3 cache:                        30.3 MiB</span><br><span class="line">NUMA node0 CPU(s):               0-7</span><br><span class="line">Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nons</span><br><span class="line">                                 top_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowpref</span><br><span class="line">                                 etch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx51</span><br><span class="line">                                 2cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat avx512_vnni md_clear flush_l1d arch_capabilities</span><br></pre></td></tr></table></figure><ul><li>aarch64</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Architecture:                    aarch64</span><br><span class="line">CPU op-mode(s):                  64-bit</span><br><span class="line">Byte Order:                      Little Endian</span><br><span class="line">CPU(s):                          8</span><br><span class="line">On-line CPU(s) list:             0-7</span><br><span class="line">Thread(s) per core:              1</span><br><span class="line">Core(s) per socket:              8</span><br><span class="line">Socket(s):                       1</span><br><span class="line">NUMA node(s):                    1</span><br><span class="line">Vendor ID:                       0x48</span><br><span class="line">Model:                           0</span><br><span class="line">Stepping:                        0x1</span><br><span class="line">CPU max MHz:                     2400.0000</span><br><span class="line">CPU min MHz:                     2400.0000</span><br><span class="line">BogoMIPS:                        200.00</span><br><span class="line">L1d cache:                       512 KiB</span><br><span class="line">L1i cache:                       512 KiB</span><br><span class="line">L2 cache:                        4 MiB</span><br><span class="line">L3 cache:                        32 MiB</span><br><span class="line">NUMA node0 CPU(s):               0-7</span><br><span class="line">Flags:                           fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma dcpop asimddp asimdfhm</span><br></pre></td></tr></table></figure><p>Note: the VMs are as close as possible in their hardware capabilities — same type and amount of RAM, same disks, network cards and bandwidth. Also the CPUs are as similar as possible but there are some differences</p><ul><li>the CPU frequency: 3000 MHz (x86_64) vs 2400 MHz (aarch64)</li><li>BogoMIPS: 6000 (x86_64) vs 200 (aarch64)</li><li>Level 1 caches: 128 KiB (x86_64) vs 512 KiB (aarch64)</li></ul><p>Both VMs run Ubuntu 20.04 with latest software updates.</p><p>HAProxy is built from source for the <a href="https://github.com/haproxy/haproxy">master</a> branch, so it might have few changes since the cut of haproxy-2.2 tag!</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">HA-Proxy version 2.3-dev0 2020&#x2F;07&#x2F;07 - https:&#x2F;&#x2F;haproxy.org&#x2F;</span><br><span class="line">Status: development branch - not safe for use in production.</span><br><span class="line">Known bugs: https:&#x2F;&#x2F;github.com&#x2F;haproxy&#x2F;haproxy&#x2F;issues?q&#x3D;is:issue+is:open</span><br><span class="line">Running on: Linux 5.4.0-40-generic #44-Ubuntu SMP Mon Jun 22 23:59:48 UTC 2020 aarch64</span><br><span class="line">Build options :</span><br><span class="line">  TARGET  &#x3D; linux-glibc</span><br><span class="line">  CPU     &#x3D; generic</span><br><span class="line">  CC      &#x3D; clang-9</span><br><span class="line">  CFLAGS  &#x3D; -O2 -Wall -Wextra -Wdeclaration-after-statement -fwrapv -Wno-address-of-packed-member -Wno-unused-label -Wno-sign-compare -Wno-unused-parameter -Wno-missing-field-initializers -Wno-string-plus-int -Wtype-limits -Wshift-negative-value -Wnull-dereference -Werror</span><br><span class="line">  OPTIONS &#x3D; USE_PCRE&#x3D;1 USE_PCRE_JIT&#x3D;1 USE_OPENSSL&#x3D;1 USE_LUA&#x3D;1 USE_ZLIB&#x3D;1 USE_DEVICEATLAS&#x3D;1 USE_51DEGREES&#x3D;1 USE_WURFL&#x3D;1 USE_SYSTEMD&#x3D;1</span><br><span class="line"></span><br><span class="line">Feature list : +EPOLL -KQUEUE +NETFILTER +PCRE +PCRE_JIT -PCRE2 -PCRE2_JIT +POLL -PRIVATE_CACHE +THREAD -PTHREAD_PSHARED +BACKTRACE -STATIC_PCRE -STATIC_PCRE2 +TPROXY +LINUX_TPROXY +LINUX_SPLICE +LIBCRYPT +CRYPT_H +GETADDRINFO +OPENSSL +LUA +FUTEX +ACCEPT4 +ZLIB -SLZ +CPU_AFFINITY +TFO +NS +DL +RT +DEVICEATLAS +51DEGREES +WURFL +SYSTEMD -OBSOLETE_LINKER +PRCTL +THREAD_DUMP -EVPORTS</span><br><span class="line"></span><br><span class="line">Default settings :</span><br><span class="line">  bufsize &#x3D; 16384, maxrewrite &#x3D; 1024, maxpollevents &#x3D; 200</span><br><span class="line"></span><br><span class="line">Built with multi-threading support (MAX_THREADS&#x3D;64, default&#x3D;8).</span><br><span class="line">Built with OpenSSL version : OpenSSL 1.1.1f  31 Mar 2020</span><br><span class="line">Running on OpenSSL version : OpenSSL 1.1.1f  31 Mar 2020</span><br><span class="line">OpenSSL library supports TLS extensions : yes</span><br><span class="line">OpenSSL library supports SNI : yes</span><br><span class="line">OpenSSL library supports : TLSv1.0 TLSv1.1 TLSv1.2 TLSv1.3</span><br><span class="line">Built with Lua version : Lua 5.3.3</span><br><span class="line">Built with DeviceAtlas support (dummy library only).</span><br><span class="line">Built with 51Degrees Pattern support (dummy library).</span><br><span class="line">Built with WURFL support (dummy library version 1.11.2.100)</span><br><span class="line">Built with network namespace support.</span><br><span class="line">Built with zlib version : 1.2.11</span><br><span class="line">Running on zlib version : 1.2.11</span><br><span class="line">Compression algorithms supported : identity(&quot;identity&quot;), deflate(&quot;deflate&quot;), raw-deflate(&quot;deflate&quot;), gzip(&quot;gzip&quot;)</span><br><span class="line">Built with transparent proxy support using: IP_TRANSPARENT IPV6_TRANSPARENT IP_FREEBIND</span><br><span class="line">Built with PCRE version : 8.39 2016-06-14</span><br><span class="line">Running on PCRE version : 8.39 2016-06-14</span><br><span class="line">PCRE library supports JIT : yes</span><br><span class="line">Encrypted password support via crypt(3): yes</span><br><span class="line">Built with clang compiler version 9.0.1 </span><br><span class="line"></span><br><span class="line">Available polling systems :</span><br><span class="line">      epoll : pref&#x3D;300,  test result OK</span><br><span class="line">       poll : pref&#x3D;200,  test result OK</span><br><span class="line">     select : pref&#x3D;150,  test result OK</span><br><span class="line">Total: 3 (3 usable), will use epoll.</span><br><span class="line"></span><br><span class="line">Available multiplexer protocols :</span><br><span class="line">(protocols marked as &lt;default&gt; cannot be specified using &#39;proto&#39; keyword)</span><br><span class="line">            fcgi : mode&#x3D;HTTP       side&#x3D;BE        mux&#x3D;FCGI</span><br><span class="line">       &lt;default&gt; : mode&#x3D;HTTP       side&#x3D;FE|BE     mux&#x3D;H1</span><br><span class="line">              h2 : mode&#x3D;HTTP       side&#x3D;FE|BE     mux&#x3D;H2</span><br><span class="line">       &lt;default&gt; : mode&#x3D;TCP        side&#x3D;FE|BE     mux&#x3D;PASS</span><br><span class="line"></span><br><span class="line">Available services : none</span><br><span class="line"></span><br><span class="line">Available filters :</span><br><span class="line">[SPOE] spoe</span><br><span class="line">[COMP] compression</span><br><span class="line">[TRACE] trace</span><br><span class="line">[CACHE] cache</span><br><span class="line">[FCGI] fcgi-app</span><br></pre></td></tr></table></figure><p>I’ve tried to fine tune it as much as I could by following all best practices I was able to find in the <a href="https://cbonte.github.io/haproxy-dconv/">official documentation</a> and in the web.</p><p>The HAProxy config is:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">global</span><br><span class="line">  log stdout format raw local0 err</span><br><span class="line">#  nbproc                            8</span><br><span class="line">  nbthread                          32</span><br><span class="line">  cpu-map                           1&#x2F;all 0-7</span><br><span class="line">  tune.ssl.default-dh-param         2048</span><br><span class="line">  tune.ssl.capture-cipherlist-size  1</span><br><span class="line">  ssl-server-verify                 none</span><br><span class="line">  maxconn                           32748</span><br><span class="line">  daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  timeout client                    60s</span><br><span class="line">  timeout client-fin                 1s</span><br><span class="line">  timeout server                    30s</span><br><span class="line">  timeout server-fin                 1s</span><br><span class="line">  timeout connect                   10s</span><br><span class="line">  timeout http-request              10s</span><br><span class="line">  timeout http-keep-alive           10s</span><br><span class="line">  timeout queue                     10m</span><br><span class="line">  timeout check                     10s</span><br><span class="line">  mode                              http</span><br><span class="line">  log                               global</span><br><span class="line">  option                            dontlog-normal</span><br><span class="line">  option                            httplog</span><br><span class="line">  option                            dontlognull</span><br><span class="line">  option                            http-use-htx</span><br><span class="line">  option                            http-server-close</span><br><span class="line">  option                            http-buffer-request</span><br><span class="line">  option                            redispatch</span><br><span class="line">  retries                           3000</span><br><span class="line"></span><br><span class="line">frontend test_fe</span><br><span class="line">  bind :::8080</span><br><span class="line">  #bind :::8080 ssl crt &#x2F;home&#x2F;ubuntu&#x2F;tests&#x2F;tls&#x2F;server.pem</span><br><span class="line">  default_backend test_be</span><br><span class="line"></span><br><span class="line">backend test_be</span><br><span class="line">  #balance roundrobin</span><br><span class="line">  balance leastconn</span><br><span class="line">  #balance random(2)</span><br><span class="line">  server go1 127.0.0.1:8081 no-check </span><br><span class="line">  server go2 127.0.0.1:8082 no-check</span><br><span class="line">  server go3 127.0.0.1:8083 no-check </span><br><span class="line">  server go4 127.0.0.1:8084 no-check</span><br></pre></td></tr></table></figure><p>This way HAProxy is used as a load balancer in front of four HTTP servers.</p><p>To also use it as a SSL terminator one just needs to comment out line 34 and uncomment line 35.</p><p>The best results I’ve achieved by using the <a href="https://cbonte.github.io/haproxy-dconv/2.2/configuration.html#nbthread">multithreaded</a> setup. As the documentation says this is the recommended setup anyway but it also gave me almost twice better throughput! In addition the best results were with 32 threads. The throughput was increasing from 8 to 16 and from 16 to 32, but dropped when used 64 threads.</p><p>I’ve also <a href="https://cbonte.github.io/haproxy-dconv/2.2/configuration.html#cpu-map">pinned</a> the threads to stay at the same CPU for its lifetime with <code>cpu-map 1/all 0–7</code>.</p><p>The other important setting is the algorithm to use to balance between the backends. Just like in Willy Tarreau’s <a href="https://www.haproxy.com/blog/power-of-two-load-balancing/">tests</a> for me <code>leastconn</code> gave the best performance.</p><p>As recommended at HAProxy Enterprice <a href="https://www.haproxy.com/documentation/hapee/1-7r2/configuration/system-tuning/#disable-irqbalance">documentation</a> I’ve disabled <code>irqbalance</code>.</p><p>Finally I’ve applied the following kernel settings:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl -w net.ipv4.ip_local_port_range&#x3D;&quot;1024 65024&quot;</span><br><span class="line">sudo sysctl -w net.ipv4.tcp_max_syn_backlog&#x3D;100000</span><br><span class="line">sudo sysctl -w net.core.netdev_max_backlog&#x3D;100000</span><br><span class="line">sudo sysctl -w net.ipv4.tcp_tw_reuse&#x3D;1</span><br><span class="line">sudo sysctl -w fs.file-max&#x3D;500000</span><br></pre></td></tr></table></figure><p><code>fs.file-max</code> is related also with a change in <code>/etc/security/limits.conf</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root soft nofile 500000</span><br><span class="line">root hard nofile 500000</span><br><span class="line">* soft nofile 500000</span><br><span class="line">* hard nofile 500000</span><br></pre></td></tr></table></figure><p>For backend I used very simple HTTP servers written in Golang. They just write “Hello World” back to the client without reading/writing from/to disk or to the network:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; run with: env PORT&#x3D;8081 go run http-server.go</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">    &quot;fmt&quot;</span><br><span class="line">    &quot;log&quot;</span><br><span class="line">    &quot;net&#x2F;http&quot;</span><br><span class="line">    &quot;os&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line"></span><br><span class="line">    port :&#x3D; os.Getenv(&quot;PORT&quot;)</span><br><span class="line">    if port &#x3D;&#x3D; &quot;&quot; &#123;</span><br><span class="line">      log.Fatal(&quot;Please specify the HTTP port as environment variable, e.g. env PORT&#x3D;8081 go run http-server.go&quot;)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    http.HandleFunc(&quot;&#x2F;&quot;, func(w http.ResponseWriter, r *http.Request)&#123;</span><br><span class="line">        fmt.Fprintf(w, &quot;Hello World&quot;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    log.Fatal(http.ListenAndServe(&quot;:&quot; + port, nil))</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>As load testing client I have used <a href="https://github.com/wg/wrk">WRK</a> with the same setup as for <a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">testing Apache Tomcat</a>.</p><p>And now the results:</p><ul><li>aarch64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line"> 8 threads and 96 connections</span><br><span class="line"> Thread Stats Avg    Stdev   Max +&#x2F;-  Stdev</span><br><span class="line"> Latency     6.67ms  8.82ms 196.74ms  89.85%</span><br><span class="line"> Req&#x2F;Sec     2.60k   337.06   5.79k   75.79%</span><br><span class="line"> 621350 requests in 30.09s, 75.85MB read</span><br><span class="line">Requests&#x2F;sec: 20651.69</span><br><span class="line">Transfer&#x2F;sec: 2.52MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line"> 8 threads and 96 connections</span><br><span class="line"> Thread Stats  Avg    Stdev    Max +&#x2F;-  Stdev</span><br><span class="line"> Latency      3.32ms  4.46ms  75.42ms   94.58%</span><br><span class="line"> Req&#x2F;Sec      4.71k   538.41   8.84k    82.41%</span><br><span class="line"> 1127664 requests in 30.10s, 137.65MB read</span><br><span class="line">Requests&#x2F;sec: 37464.85</span><br><span class="line">Transfer&#x2F;sec: 4.57MB</span><br></pre></td></tr></table></figure><ul><li>aarch64, HTTPS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line"> 8 threads and 96 connections</span><br><span class="line"> Thread Stats    Avg   Stdev    Max +&#x2F;-  Stdev</span><br><span class="line"> Latency       7.92ms  12.50ms  248.52ms 91.18%</span><br><span class="line"> Req&#x2F;Sec       2.42k   338.67   4.34k    80.88%</span><br><span class="line"> 578210 requests in 30.08s, 70.58MB read</span><br><span class="line">Requests&#x2F;sec: 19220.81</span><br><span class="line">Transfer&#x2F;sec: 2.35MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTPS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line"> 8 threads and 96 connections</span><br><span class="line"> Thread Stats   Avg   Stdev   Max +&#x2F;-  Stdev</span><br><span class="line"> Latency       3.56ms 4.83ms  111.51ms 94.25%</span><br><span class="line"> Req&#x2F;Sec       4.46k  609.37   7.23k   85.60%</span><br><span class="line"> 1066831 requests in 30.07s, 130.23MB read</span><br><span class="line">Requests&#x2F;sec: 35474.26</span><br><span class="line">Transfer&#x2F;sec: 4.33MB</span><br></pre></td></tr></table></figure><p>What we see here is:</p><ul><li>that HAProxy is almost twice faster on the x86_64 VM than the aarch64 VM!</li><li>and also that TLS offloading decreases the throughput with around 5–8%</li></ul><hr><p><strong>Update 1</strong> (Jul 10 2020): To see whether the Golang based HTTP servers are not the bottleneck in the above testing I’ve decided to run the same WRK load tests directly against <strong>one</strong> of the backends, i.e. skip HAProxy.</p><ul><li>aarch64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency   615.31us  586.70us  22.44ms   90.61%</span><br><span class="line">    Req&#x2F;Sec    20.05k     1.57k   42.29k    73.62%</span><br><span class="line">  4794299 requests in 30.09s, 585.24MB read</span><br><span class="line">Requests&#x2F;sec: 159319.75</span><br><span class="line">Transfer&#x2F;sec:     19.45MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency   774.24us  484.99us  36.43ms   97.04%</span><br><span class="line">    Req&#x2F;Sec    15.28k   413.04    16.89k    73.57%</span><br><span class="line">  3658911 requests in 30.10s, 446.64MB read</span><br><span class="line">Requests&#x2F;sec: 121561.40</span><br><span class="line">Transfer&#x2F;sec:     14.84MB</span><br></pre></td></tr></table></figure><p>Here we see that the HTTP server running on aarch64 is around 30% faster than on x86_64!</p><p>And the more important observation is that the throughput is several times better when not using load balancer at all! I think the problem here is in my setup — both HAProxy and the 4 backend servers run on the same VM, so they fight for resources! I will pin the Golang servers to their own CPU cores and let HAProxy use only the other 4 CPU cores! Stay tuned for an update!</p><hr><p><strong>Update 2</strong> (Jul 10 2020):</p><p>To pin the processes to specific CPUs I will use <code>numactl</code>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ numactl — hardware</span><br><span class="line">available: 1 nodes (0)</span><br><span class="line">node 0 cpus: 0 1 2 3 4 5 6 7</span><br><span class="line">node 0 size: 16012 MB</span><br><span class="line">node 0 free: 170 MB</span><br><span class="line">node distances:</span><br><span class="line">node 0</span><br><span class="line">0: 10</span><br></pre></td></tr></table></figure><p>I’ve pinned the Golang HTTP servers with:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">numactl — cpunodebind&#x3D;0 — membind&#x3D;0 — physcpubind&#x3D;4 env PORT&#x3D;8081 go run etc&#x2F;haproxy&#x2F;load&#x2F;http-server.</span><br><span class="line">go</span><br></pre></td></tr></table></figure><p>i.e. this backend instance is pinned to CPU node 0 and to physical CPU 4. The other three backend servers are pinned respectively to physical CPUs 5, 6 and 7.</p><p>Also I’ve changed slightly the HAProxy configuration:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nbthread 4</span><br><span class="line">cpu-map 1&#x2F;all 0–3</span><br><span class="line"></span><br><span class="line">Nbthread 4cpu-map 1&#x2F;all 0-3</span><br></pre></td></tr></table></figure><p>i.e. HAProxy will spawn 4 threads and they will be pinned to physical CPUs 0–3.</p><p>With these changes the results stayed the same for aarch64:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line">  4 threads and 16 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     1.44ms    2.11ms  36.48ms   88.36%</span><br><span class="line">    Req&#x2F;Sec     4.98k   651.34     6.62k    74.40%</span><br><span class="line">  596102 requests in 30.10s, 72.77MB read</span><br><span class="line">Requests&#x2F;sec:  19804.19</span><br><span class="line">Transfer&#x2F;sec:      2.42MB</span><br></pre></td></tr></table></figure><p>but dropped for x86_64:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line">  4 threads and 16 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency   767.40us  153.24us  19.07ms   97.72%</span><br><span class="line">    Req&#x2F;Sec     5.21k   173.41     5.51k    63.46%</span><br><span class="line">  623911 requests in 30.10s, 76.16MB read</span><br><span class="line">Requests&#x2F;sec:  20727.89</span><br><span class="line">Transfer&#x2F;sec:      2.53MB</span><br></pre></td></tr></table></figure><p>and same for HTTP (no TLS):</p><ul><li>aarch64</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.232:8080                                                                                                                                                                   </span><br><span class="line">  4 threads and 16 connections                                                                                                                                                                                 </span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev                                                                                                                                                            </span><br><span class="line">    Latency     1.40ms    2.16ms  36.55ms   88.08%                                                                                                                                                             </span><br><span class="line">    Req&#x2F;Sec     5.55k   462.65     6.97k    69.85%                                                                                                                                                             </span><br><span class="line">  665269 requests in 30.10s, 81.21MB read                                                                                                                                                                      </span><br><span class="line">Requests&#x2F;sec:  22102.12                                                                                                                                                                                        </span><br><span class="line">Transfer&#x2F;sec:      2.70MB</span><br></pre></td></tr></table></figure><ul><li>x86_64</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.206:8080                                                                                                                                                                   </span><br><span class="line">  4 threads and 16 connections                                                                                                                                                                                 </span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev                                                                                                                                                            </span><br><span class="line">    Latency   726.01us  125.04us   6.42ms   93.95%                                                                                                                                                             </span><br><span class="line">    Req&#x2F;Sec     5.51k   165.80     5.80k    57.24%                                                                                                                                                             </span><br><span class="line">  658777 requests in 30.10s, 80.42MB read                                                                                                                                                                      </span><br><span class="line">Requests&#x2F;sec:  21886.50                                                                                                                                                                                        </span><br><span class="line">Transfer&#x2F;sec:      2.67MB</span><br></pre></td></tr></table></figure><p>So now HAProxy is a bit faster on aarch64 than on x86_64 but still far slower than the “no load balancer” approach with 120 000+ requests per second.</p><hr><p><strong>Update 3</strong> (Jul 10 2020): After seeing that the performance of the Golang HTTP server is so good (120–160K reqs/sec) and to simplify the setup I’ve decided to remove the CPU pinning from Update 2 and to use the backends from the other VM, i.e. when hitting HAProxy on the aarch64 VM it will load balance between the backends running on the x86_64 and when WRK hits HAProxy running on the x86_64 VM it will use the Golang HTTP servers running on the aarch64 VM. And here are the new results:</p><ul><li>aarch64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     6.33ms    4.93ms  76.85ms   89.14%</span><br><span class="line">    Req&#x2F;Sec     2.10k   316.84     3.52k    74.50%</span><br><span class="line">  501840 requests in 30.07s, 61.26MB read</span><br><span class="line">Requests&#x2F;sec:  16688.53</span><br><span class="line">Transfer&#x2F;sec:      2.04MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTP</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ http:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     5.32ms    6.71ms  71.29ms   90.25%</span><br><span class="line">    Req&#x2F;Sec     3.26k   639.12     4.14k    65.52%</span><br><span class="line">  779297 requests in 30.08s, 95.13MB read</span><br><span class="line">Requests&#x2F;sec:  25908.50</span><br><span class="line">Transfer&#x2F;sec:      3.16MB</span><br></pre></td></tr></table></figure><ul><li>aarch64, HTTPS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.232:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     6.17ms    5.41ms 292.21ms   91.08%</span><br><span class="line">    Req&#x2F;Sec     2.13k   238.74     3.85k    86.32%</span><br><span class="line">  506111 requests in 30.09s, 61.78MB read</span><br><span class="line">Requests&#x2F;sec:  16821.60</span><br><span class="line">Transfer&#x2F;sec:      2.05MB</span><br></pre></td></tr></table></figure><ul><li>x86_64, HTTPS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 30s test @ https:&#x2F;&#x2F;192.168.0.206:8080</span><br><span class="line">  8 threads and 96 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev</span><br><span class="line">    Latency     3.40ms    2.54ms  58.66ms   97.27%</span><br><span class="line">    Req&#x2F;Sec     3.82k   385.85     4.55k    92.10%</span><br><span class="line">  914329 requests in 30.10s, 111.61MB read</span><br><span class="line">Requests&#x2F;sec:  30376.95</span><br><span class="line">Transfer&#x2F;sec:      3.71MB</span><br></pre></td></tr></table></figure><hr><p>Happy hacking and stay safe!</p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: &lt;a href=&quot;https://github.com/wangxiyuan&quot;&gt;wangxiyuan&lt;/a&gt;&lt;br&gt;作者: &lt;a href=&quot;https://github.com/martin-g&quot;&gt;Martin Grigorov&lt;/a&gt;&lt;br&gt;原文链接: &lt;a href=&quot;https://medium.com/@martin.grigorov/compare-haproxy-performance-on-x86-64-and-arm64-cpu-architectures-bfd55d1d5566&quot;&gt;https://medium.com/@martin.grigorov/compare-haproxy-performance-on-x86-64-and-arm64-cpu-architectures-bfd55d1d5566&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文是由Apache Tomcat PMC Martin带来的Haproxy最新版本的性能测试报告。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Web" scheme="https://kunpengcompute.github.io/categories/Web/"/>
    
    
      <category term="Web" scheme="https://kunpengcompute.github.io/tags/Web/"/>
    
  </entry>
  
  <entry>
    <title>用MySQL EventMutex来理解内存屏障那些事儿</title>
    <link href="https://kunpengcompute.github.io/2020/07/09/yong-mysql-eventmutex-lai-li-jie-nei-cun-ping-zhang-na-xie-shi-er/"/>
    <id>https://kunpengcompute.github.io/2020/07/09/yong-mysql-eventmutex-lai-li-jie-nei-cun-ping-zhang-na-xie-shi-er/</id>
    <published>2020-07-09T01:49:10.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Krunal Bauskar<br>原文链接:  <a href="https://mysqlonarm.github.io/Understanding-Memory-Barrier/">https://mysqlonarm.github.io/Understanding-Memory-Barrier/</a></p><p>组内Mysql大牛Krunal利用Mysql EventMutex来让你彻底理解内存屏障问题，如何优化等。中文版实在是不好翻译，强烈建议阅读英文版增加理解。瓜已经备好了，还等啥？！</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>MySQL 有多种互斥实现，即封装在 pthread 上的、基于 futex 的、基于 Spin-Lock 的(EventMutex)。它们都有自己的优点和缺点，但由于长期以来 MySQL 一直默认使用 EventMutex，因为它被认为是 MySQL应用场景的最佳选择。</p><p>EventMutex 被转换为使用 C++原子操作(MySQL 增加了对C++ 11的支持)。鉴于 MySQL 现在也支持 ARM，正确地使用内存屏障也是保持 EventMutex 向前优化发展的关键。</p><p>在本文中，我们将使用 EventMutex 的一个示例，了解内存障碍，并查看缺少什么，可以优化什么等等。.</p><h2 id="理解获取和释放内存顺序"><a href="#理解获取和释放内存顺序" class="headerlink" title="理解获取和释放内存顺序"></a>理解获取和释放内存顺序</h2><p>ARM/PowerPC 平台使用弱内存模型，这意味着计算操作可以更自由地重新排序，因此同步地确保逻辑正确的屏障非常重要。最简单的方案是依赖使用循序一致性的默认方案(就像x86那样) ，但是这可能会大大影响其他架构的性能。</p><p>通常开发人员必须面对两个障碍(即 顺序) : 获取和释放。</p><ul><li>获取内存顺序意味着在这个内存顺序/屏障之后的任何操作都不能调度/重新排序到该内存顺序/屏障之前(但是在获取该获取内存顺序之前的操作是可以调度/重新排序到它之后)</li><li>释放内存顺序意味着在这个内存顺序/屏障之前的任何操作都不能调度/重新排序到释放该内存顺序/屏障之后(但是在释放该获取内存顺序之后的操作是可以调度/重新排序到它之前)</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img1.png" alt="img"></p><h2 id="理解-EventMutex-结构"><a href="#理解-EventMutex-结构" class="headerlink" title="理解 EventMutex 结构"></a>理解 EventMutex 结构</h2><p>EventMutex 提供了一个普通的互斥类接口，用于帮助同步对临界区的访问。</p><ul><li>Enter (lock mutex)<ul><li>try_lock (如果获得锁，立即返回).<ul><li>利用compare-and-exchange (CAX) 接口设置 m_lock_word 原子变量.</li></ul></li><li>如果锁获取失败<ul><li>进入一个自旋循环，多次尝试后暂停，检查锁是否再次可用。</li><li>如果在“ N”次尝试之后(由 innodb_sync_spin_loops 控制)锁仍然不可用，那么释放(释放 cpu 控制)并通过在 InnoDB 自制的同步数组(sync-array)中注册该线程，让其进入等待状态。另外，在保留插槽后设置一个等待标志(这样可以确保我们在 sync-array 中得到一个插槽)。等待标志是另一个用于协调信号机制的原子变量。</li></ul></li></ul></li><li>Exit (unlock mutex)<ul><li>切换原子变量(m_lock_word)以表示离开临界区。</li><li>检查是否设置了等待标志。如果有设置，那么通过同步数组(sync-array)框架向等待线程发送信号来唤醒它们。</li></ul></li></ul><p>看起来非常简单直接。不是吗？<br>引入内存障碍会使这个过程变得复杂，因为忽略它们将意味着重新排序，这可能会导致代码中出现竞争。</p><p><img src="https://mysqlonarm.github.io/images/blog9/img2.png" alt="img"></p><hr><ul><li>从上面的序列可以很清楚地看出，当锁定 m_lock_word (false-&gt; true)时，它起始于临界区(如果 CAX 成功的话) ，因此在m_lock_word 被获取(设置为 true)之前，流程不应该执行来自临界区的任何语句。</li><li>回到我们的获取-释放屏障的部分，它建议 m_lock_word 应该能<strong>获取一个屏障</strong>一旦它成功了。 <strong>(而不是像它现在这样默认的保持顺序一致(seq_cst))</strong>.</li><li>但是，等等，有两个潜在的结果。失败怎么办？即使在失败的情况下，后续的执行，如自旋，睡眠等待和设置等待标识应该在CAX 评估后。这再次表明<strong>获得屏障</strong>失败的例子 <strong>(而不是像它现在这样默认的保持顺序一致(seq_cst))</strong>. </li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img3.png" alt="img"></p><hr><ul><li>现在让我们看看 m_lock_word <strong>释放屏障</strong>的例子。通常一个释放屏障过程发生在临界区结束要改变m_lock_word的时候。</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img4.png" alt="img"></p><hr><ul><li>还有另外一个原子变量(等待标志)也需要一个合适的屏障</li><li>设置服务员标志的动作应该发生在只有当流程中已确保可以得到一个同步阵列(sync-array)插槽的时候。这自然而然就需要一个 <strong>释放屏障</strong>，在set_waiter以上的代码都不会被重新排序。注意: 这是不同的原子操作，所以这里不适用于协调m_lock_word 的获取和释放。</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img5.png" alt="img"></p><hr><ul><li>同样的信号相关的逻辑也应该在等待标志被清除之后再进行，所以它应该使用一个<strong>获取屏障</strong>，以确保在清除等待之前不会被重新调度。<strong>(而不是像现在这样释放)</strong>。</li><li>这也将帮助我们使用<strong>relaxed屏障</strong>(vs 获取)来改变waiter-load标志检查。(这里有一个潜在的问题，我们将在下面讨论)。</li><li>有了所有这些，我们也应该能够解决那些比较明显的内存屏障了。</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img6.png" alt="img"></p><hr><h3 id="异常情况"><a href="#异常情况" class="headerlink" title="异常情况:"></a>异常情况:</h3><ul><li>在release-barrier的</li></ul>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lock_word</span><br></pre></td></tr></table></figure><p>  在之后的 acquire-barrier</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">waiter</span><br></pre></td></tr></table></figure><p>  这是能够重新排序的。</p><ul><li>这甚至是我对潜在风险的理解，我认为这就是为什么 MySQL 在这两个操作之间引入了一个内存屏障。如上所述<a href="https://preshing.com/20170612/can-reordering-of-release-acquire-operations-introduce-deadlock/">blog</a>, C++ 标准应该限制编译器这样做。</li></ul><ul><li><p><strong>relaxed barrier</strong></p></li></ul><p>  对于等待标识(同时检查其值时)有潜在的重新排序，可以将加载指令移动到 m_lock_word释放之前(注意:<strong>释放屏障</strong> 可以让后续的指示得到预先安排，就是排在它之前). </p><ul><li>If “waiter” is true then 调用信号循环来唤醒线程.</li><li>If “waiter” is false then 信号循环将不会被该线程调用，而可能被其他线程调用。</li><li>如果只有2个线程，并且 thread-1通过在释放屏障之前重新排序得到 waiter = false，然后立即发布waiter被 thread-2设置为 true 并继续等待会怎么样。现在，thread-1 将永远不会向thread-2发出信号。</li></ul><p>因此，使用一个<strong>relaxed 屏障</strong>是不可能的，所以让我们转换它使用一个<strong>获取屏障</strong>，应该避免移动后续语句超出上述情况，并作为澄清以上release-acquire需要遵循 C++ 标准。</p><p>所有这些都是为了节省额外的内存屏障。内存屏障的意图是协调同步非原子的操作，因为示例代码中有固有的原子(等待标志)使用适当的内存屏障可以帮助达到所需的效果。</p><p>所以有了这些注意事项，代码就会变成这样</p><p><img src="https://mysqlonarm.github.io/images/blog9/img7.png" alt="img"></p><hr><h3 id="我们从这次代码改造中得到了什么"><a href="#我们从这次代码改造中得到了什么" class="headerlink" title="我们从这次代码改造中得到了什么?"></a>我们从这次代码改造中得到了什么?</h3><p>我们实现了三个目标</p><ul><li>修正了内存屏障的使用，这也有助于澄清代码/流程/开发人员的意图。(这是使用内存屏障所强调的重要事情之一。正确的使用将有助于使代码流程被理解和遵循)。</li><li>从严格的顺序排序移动到单向屏障而不失去正确性(获取和释放)</li><li>避免在非原子操作同步中使用内存屏障。</li></ul><p>除非具有性能影响，否则没有理由进行改造，这次改造也不例外。改造后 ARM 的性能提高了4-15% ，x86_64的性能提高了4-6% 。</p><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>原子操作是好的，但是内存障碍使它们面临挑战，并且确保正确使用内存屏障是在所有平台上获得最佳性能的关键。Barrier 的改造正在迎头赶上，但仍然在起步阶段(尽管在C+11中有所体现) ，因为大多数软件最近开始移植它。正确使用屏障也有助于描述开发者/代码的意图。</p><p><em>如果你有问题，请联系我。</em></p></div><div id="English" class="tab-content"><p>MySQL has multiple mutex implementations viz. wrapper over pthread, futex based, Spin-Lock based (EventMutex). All of them have their own pros and cons but since long MySQL defaulted to EventMutex as it has been found to be optimal for MySQL use-cases.</p><p>EventMutex was switched to use C++ atomic (with MySQL adding support for C++11). Given that MySQL now also support ARM, ensuring a correct use of memory barrier is key to keep the EventMutex Optimal moving forward too.</p><p>In this article we will use an example of EventMutex and understand the memory barrier and also see what is missing, what could be optimized, etc…</p><h2 id="Understanding-acquire-and-release-memory-order"><a href="#Understanding-acquire-and-release-memory-order" class="headerlink" title="Understanding acquire and release memory order"></a>Understanding acquire and release memory order</h2><p>ARM/PowerPC follows weak memory model that means operations can be re-ordered more freely so ensuring the correct barrier with synchronization logic is important. Easiest alternative is to rely on a default one that uses sequential consistency (as done with x86) but it could affect performance big time on other architectures.</p><p>Often a programmer has to deal with 2 barriers (aka order): acquire and release.</p><ul><li>acquire memory order means any operation after this memory-order/barrier can’t be scheduled/re-ordered before it (but operations before it can be scheduled/re-ordered after it)</li><li>release memory order means any operations before this memory-order/barrier can’t be scheduled/re-ordered after it (but operations after it can be scheduled/re-ordered before it).</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img1.png" alt="img"></p><h2 id="Understanding-EventMutex-structure"><a href="#Understanding-EventMutex-structure" class="headerlink" title="Understanding EventMutex structure"></a>Understanding EventMutex structure</h2><p>EventMutex provides a normal mutex-like interface meant to help synchronize access to the critical sections.</p><ul><li>Enter (lock mutex)<ul><li>try_lock (try to get the lock if procured return immediately).<ul><li>Uses an atomic variable (m_lock_word) that is set using compare-and-exchange (CAX) interface.</li></ul></li><li>If fail to procure<ul><li>Enter a spin-loop that does multiple attempts to pause followed by check if the lock is again available.</li><li>If after “N” attempts (controlled by innodb_sync_spin_loops) lock is not available then yield (releasing the cpu control) and enter wait by registering thread in InnoDB home-grown sync array implementation. Also, set a waiter flag after reserving the slot (this ensures we will get a slot in sync-array). Waiter flag is another atomic that is used to coordinate the signal mechanism.</li></ul></li></ul></li><li>Exit (unlock mutex)<ul><li>Toggling the atomic variable (m_lock_word) to signify leaving the critical section.</li><li>Check if the waiter flag is set. If yes then signal the waiting thread through the sync-array framework.</li></ul></li></ul><p>Looks pretty straightforward and simple. Isn’t it?<br>Things get complicated with introduction of memory barriers as ignoring them would mean re-ordering can cause race in your code.</p><p><img src="https://mysqlonarm.github.io/images/blog9/img2.png" alt="img"></p><hr><ul><li>From the above sequence it is pretty clear that while locking m_lock_word (false-&gt;true) it could potentially begin the critical section (if CAX succeeds) and so flow shouldn’t execute any statement from the critical section before the lock word is acquired (set to true).</li><li>Going back to our acquire-release barrier section it suggests m_lock_word should take an <strong>acquire barrier</strong> incase of success <strong>(instead of default (seq_cst) as it currently does)</strong>.</li><li>But wait, there are 2 potential outcomes. What about failure? Even in case of failure, followup actions like spin, sleep and set-waiter should be done only post CAX evaluation. This again suggests use of an <strong>acquire barrier</strong> for failure case too. <strong>(instead of default (seq_cst) as it currently does)</strong>.</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img3.png" alt="img"></p><hr><ul><li>Now let’s look at the release barrier for m_lock_word. Naturally a <strong>release barrier</strong> will be placed once a critical section is done when the m_lock_word is toggled.</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img4.png" alt="img"></p><hr><ul><li>There is another atomic variable (waiter flag) that needs to get a proper barrier too.</li><li>Action to set a waiter flag should be done only when flow has ensured it can get a sync array slot. This naturally invites the need for a <strong>release barrier</strong> so the code is not re-ordered beyond set_waiter. Note: This is different atomic though so the co-ordination of m_lock_word acquire and release will not apply here.</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img5.png" alt="img"></p><hr><ul><li>Same way signal logic should be done only after the waiter flag is cleared so it should use an <strong>acquire barrier</strong> that will ensure it is not re-scheduled before the clear-waiter. <strong>(instead of release as it currently does)</strong>.</li><li>This will also help us change the waiter-load flag check to use <strong>relaxed barrier</strong> (vs acquire). (There is a potential catch here; we will discuss it below).</li><li>With all that in place we should able to get rid of explicit memory_fence too.</li></ul><p><img src="https://mysqlonarm.github.io/images/blog9/img6.png" alt="img"></p><hr><h3 id="Anomalies"><a href="#Anomalies" class="headerlink" title="Anomalies:"></a>Anomalies:</h3><ul><li>release-barrier on</li></ul>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lock_word</span><br></pre></td></tr></table></figure><p>  followed by an acquire-barrier on</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">waiter</span><br></pre></td></tr></table></figure><p>  this could be reordered.</p><ul><li>This was even my understanding of potential risk and I presume that’s why MySQL introduced a fence between these 2 operations. As per the said <a href="https://preshing.com/20170612/can-reordering-of-release-acquire-operations-introduce-deadlock/">blog</a>, C++ standard should limit compilers from doing so.</li></ul><ul><li>By using a</li></ul><p>  <strong>relaxed barrier</strong></p><p>  for the waiter (while checking for its value) there is potential re-ordering that could move load instruction before the m_lock_word release (note: <strong>release barrier</strong> can allow followup instructions to get scheduled before it).</p><ul><li>If “waiter” is true then a signal loop will be called.</li><li>If “waiter” is false then the signal loop will not be called by this thread but some other thread may call it.</li><li>What if there are only 2 threads and thread-1 evaluates waiter=false by re-ordering it before the release barrier and then immediately posts that waiter is set to true by thread-2 and goes to wait. Now thread-1 will never signal thread-2.</li></ul><p>So using a <strong>relaxed barrier</strong> is not possible so let’s switch it to use an <strong>acquire barrier</strong> that should avoid moving the followup statement beyond the said point and as clarified above release-acquire needs to follow C++ standard.</p><p>All this to help save an extra memory fence. memory-fence intention is to help co-ordinate non-atomic synchronization since our flow has inherent atomic (waiter) using proper memory barrier can help achieve the needed effect.</p><p>So with all that taken-care this is how things would look</p><p><img src="https://mysqlonarm.github.io/images/blog9/img7.png" alt="img"></p><hr><h3 id="What-we-gained-from-this-revamp"><a href="#What-we-gained-from-this-revamp" class="headerlink" title="What we gained from this revamp?"></a>What we gained from this revamp?</h3><p>So we achieved 3 things</p><ul><li>Corrected use of memory barrier that helps also clarify the code/flow/developer intention. (This is one of the important thing stressed with use of memory barrier. Correct use will help make the code flow naturally obvious to understand and follow).</li><li>Moved from strict sequential ordering to one-way barrier without loosing on correctness. (acquire and release)</li><li>Avoided use of fence memory barrier meant for synchronization of non-atomic.</li></ul><p>Revamp is not justified unless it has performance impact and this revamp is no exception. Revamp helps improve performance on ARM in range of 4-15% and on x86_64 in range of 4-6%.</p><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Atomics are good but memory-barrier make them challanging and ensuring proper use of these barriers is key to the optimal performance on all platforms. Adaptation of barrier is catching up but still naive (though present in C+11) as most of the softwares recently started adapting to it. Proper use of barrier help clear the intention too.</p><p><em>If you have more questions/queries do let me know. Will try to answer them.</em></p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Krunal Bauskar&lt;br&gt;原文链接:  &lt;a href=&quot;https://mysqlonarm.github.io/Understanding-Memory-Barrier/&quot;&gt;https://mysqlonarm.github.io/Understanding-Memory-Barrier/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;组内Mysql大牛Krunal利用Mysql EventMutex来让你彻底理解内存屏障问题，如何优化等。中文版实在是不好翻译，强烈建议阅读英文版增加理解。瓜已经备好了，还等啥？！&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>指定NUMA节点运行MySql</title>
    <link href="https://kunpengcompute.github.io/2020/07/03/zhi-ding-numa-jie-dian-yun-xing-mysql/"/>
    <id>https://kunpengcompute.github.io/2020/07/03/zhi-ding-numa-jie-dian-yun-xing-mysql/</id>
    <published>2020-07-03T03:57:23.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Krunal Bauskar<br>原文链接: <a href="https://mysqlonarm.github.io/Running-MySQL-on-Selected-NUMA-nodes/">https://mysqlonarm.github.io/Running-MySQL-on-Selected-NUMA-nodes/</a></p><p>指定NUMA节点来运行Mysql，全路程实践，来试试吧</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>“在选定的 NUMA 节点上运行 MySQL”看起来非常简单，但不幸的是，它并不简单。最近，我遇到了一个情况，需要在2个(甚至4个) NUMA 节点上运行 MySQL。</p><p>当然，我尝试的第一件事就是使用 <code>numactl --physcpubind</code> 限制 CPU/Core 集合，只从选定的NUMA节点选择CPUs和Core。MySQL 配置为 <code>innodb_numa_interleave=1</code> ，因此我希望它仅从这个选定的NUMA 节点分配内存(因为我限制了 CPU/core 的使用)。</p><h3 id="Suprise-1"><a href="#Suprise-1" class="headerlink" title="Suprise-1:"></a>Suprise-1:</h3><p>MySQL 使用 <code>numa_all_nodes_ptr-&gt;maskp</code> 这意味着即使 CPU 任务集被限制为2个 NUMA 节点。</p><p>Daniel Black告诉我并让我意识到的两个问题:</p><ul><li><a href="https://github.com/mysql/mysql-server/pull/104">https://github.com/mysql/mysql-server/pull/104</a> (5.7)</li><li><a href="https://github.com/mysql/mysql-server/pull/138">https://github.com/mysql/mysql-server/pull/138</a> (8.0)</li></ul><p>上述问题建议切换到一个更符合逻辑的 <code>numa_get_mems_allowed()</code>. 根据文档，它应该返回一个节点的掩码，告知这个节点被允许为特定的进程分配内存。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ref-from-doc: numa_get_mems_allowed() returns the mask of nodes from which the process is allowed to allocate memory in it&#39;s current cpuset context.</span><br></pre></td></tr></table></figure><p>所以我决定应用这个补丁并继续测试。</p><h3 id="Suprise-2"><a href="#Suprise-2" class="headerlink" title="Suprise-2:"></a>Suprise-2:</h3><p>仅仅使用补丁和限制 cpu/core集合并没有帮助，所以我想尝试使用 membind 选项。</p><h3 id="Suprise-3"><a href="#Suprise-3" class="headerlink" title="Suprise-3:"></a>Suprise-3:</h3><p>所以现在这个命令看起来像:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numactl --physcpubind&#x3D; --membind&#x3D;0,1</span><br></pre></td></tr></table></figure><p>这一次，我当然只希望从选定的 NUMA 节点分配内存，但它仍然没有。它从所有4个节点分配内存。</p><p>经过一番文档搜索，建议对 <code>numa_all_nodes_ptr</code> 查看  <code>mems_allowed</code> 字段，如下所述a</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numa_all_nodes_ptr: The set of nodes to record is derived from &#x2F;proc&#x2F;self&#x2F;status, field &quot;Mems_allowed&quot;. The user should not alter this bitmask.</span><br></pre></td></tr></table></figure><p>正如 Alexey Kopytov 在 PR # 138中指出的, <code>numa_all_nodes_ptr</code> 和<code>numa_get_mems_allowed</code> r允许读取相同的NUMA掩码。</p><p>这意味着 <code>numa_get_mems_allowed</code>已经失效，或者文档需要更新。</p><p><em>为了完全确认，我还尝试了 numctl-interleave，但这也没有帮助</em></p><h3 id="事实验证"><a href="#事实验证" class="headerlink" title="事实验证:"></a>事实验证:</h3><p>因此，我决定使用一个简单的程序(在 MySQL 之外)来验证上述事实。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;numa.h&gt;</span><br><span class="line">#include &lt;numaif.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">cout &lt;&lt; *numa_all_nodes_ptr-&gt;maskp &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; *numa_get_mems_allowed()-&gt;maskp &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">numactl --membind&#x3D;0-1 .&#x2F;a.out</span><br><span class="line">15</span><br><span class="line">15</span><br></pre></td></tr></table></figure><p>很明显，当 <code>numa_get_mems_allowed</code> 返回的只是允许分配内存的NUMA节点时，两者似乎返回相同的掩码值。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案:"></a>解决方案:</h3><p>我迫切需要一个解决方案，所以尝试使用一个简单的工作方式手动填补掩码(将继续跟进与操作系统供应商 numactl 行为)。这种方法最终奏效了，现在只能从选定的 NUMA 节点分配内存。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">+const unsigned long numa_mask &#x3D; 0x3;</span><br><span class="line"> </span><br><span class="line"> struct set_numa_interleave_t &#123;</span><br><span class="line">   set_numa_interleave_t() &#123;</span><br><span class="line">     if (srv_numa_interleave) &#123;</span><br><span class="line">       ib::info(ER_IB_MSG_47) &lt;&lt; &quot;Setting NUMA memory policy to&quot;</span><br><span class="line">                                 &quot; MPOL_INTERLEAVE&quot;;</span><br><span class="line">-      if (set_mempolicy(MPOL_INTERLEAVE, numa_all_nodes_ptr-&gt;maskp,</span><br><span class="line">+      if (set_mempolicy(MPOL_INTERLEAVE, &amp;numa_mask,</span><br><span class="line">                         numa_all_nodes_ptr-&gt;size) !&#x3D; 0) &#123;</span><br><span class="line">         ib::warn(ER_IB_MSG_48) &lt;&lt; &quot;Failed to set NUMA memory&quot;</span><br><span class="line">                                   &quot; policy to MPOL_INTERLEAVE: &quot;</span><br><span class="line">@@ -1000,7 +1001,7 @@ static buf_chunk_t *buf_chunk_init(</span><br><span class="line"> #ifdef HAVE_LIBNUMA</span><br><span class="line">   if (srv_numa_interleave) &#123;</span><br><span class="line">     int st &#x3D; mbind(chunk-&gt;mem, chunk-&gt;mem_size(), MPOL_INTERLEAVE,</span><br><span class="line">-                   numa_all_nodes_ptr-&gt;maskp, numa_all_nodes_ptr-&gt;size,</span><br><span class="line">+                   &amp;numa_mask, numa_all_nodes_ptr-&gt;size,</span><br><span class="line">                    MPOL_MF_MOVE);</span><br><span class="line">     if (st !&#x3D; 0) &#123;</span><br><span class="line">       ib::warn(ER_IB_MSG_54) &lt;&lt; &quot;Failed to set NUMA memory policy of&quot;</span><br></pre></td></tr></table></figure><p>(当然，这需要从源代码重新构建，而不是二进制/包(如果想用，往下看)).</p><h3 id="那你为什么不用…"><a href="#那你为什么不用…" class="headerlink" title="那你为什么不用…  ?"></a>那你为什么不用…  ?</h3><p>当然，大多数人可能会建议通过将 <code>innodb_numa_interleave</code> 关闭而使用 membind 来避免这种情况. 当然，这种方法是可行的，但是这种方法略有不同，因为所有分配的内存都受上述限制的约束，而<code>innodb_numa_interleave</code> 仅在缓冲池分配期间适用。它可能应用于特定的目的，但可能不能像这样比较。</p><p>这已经在我的待办事项列表中，以检查 complete interleave vs innodb_numa_interleave带来的影响。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>NUMA 节点上的平衡分配有多个方面，包括核心选择、内存分配、线程分配(同样在选定的 NUMA 节点上)等等。更过的惊喜和令人兴奋的东西等待我们去探索。</p><p><em>如果你有问题/疑问，请联系我。</em></p></div><div id="English" class="tab-content"><p>“Running MySQL on selected NUMA node(s)” looks pretty straightforward but unfortunately it isn’t. Recently, I was faced with a situation that demanded running MySQL on 2 (out of 4) NUMA nodes.</p><p>Naturally, the first thing I tried was to restrict CPU/Core set using <code>numactl --physcpubind</code> selecting only the said CPUs/cores from the said NUMA nodes. MySQL was configured to use <code>innodb_numa_interleave=1</code> so I was expecting it to allocate memory from the said NUMA nodes only (as I restricted usage of CPU/core).</p><h3 id="Suprise-1-1"><a href="#Suprise-1-1" class="headerlink" title="Suprise-1:"></a>Suprise-1:</h3><p>MySQL uses <code>numa_all_nodes_ptr-&gt;maskp</code> that means all the nodes are opted even though the CPU task-set is limited to 2 NUMA nodes.</p><p>Some lookout pointed me to these 2 issues from Daniel Black</p><ul><li><a href="https://github.com/mysql/mysql-server/pull/104">https://github.com/mysql/mysql-server/pull/104</a> (5.7)</li><li><a href="https://github.com/mysql/mysql-server/pull/138">https://github.com/mysql/mysql-server/pull/138</a> (8.0)</li></ul><p>Issue proposes to switch to a more logical <code>numa_get_mems_allowed()</code>. As per the documentation it should return a mask of the node that are are allowed to allocate memory for the said process.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ref-from-doc: numa_get_mems_allowed() returns the mask of nodes from which the process is allowed to allocate memory in it&#39;s current cpuset context.</span><br></pre></td></tr></table></figure><p>So I decided to apply the patch and proceed.</p><h3 id="Suprise-2-1"><a href="#Suprise-2-1" class="headerlink" title="Suprise-2:"></a>Suprise-2:</h3><p>Just applying patch and relying on cpu/core set didn’t helped. So I thought of trying with membind option.</p><h3 id="Suprise-3-1"><a href="#Suprise-3-1" class="headerlink" title="Suprise-3:"></a>Suprise-3:</h3><p>So now the command looks like:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numactl --physcpubind&#x3D; --membind&#x3D;0,1</span><br></pre></td></tr></table></figure><p>This time I surely expected that memory would be allocated from the said NUMA nodes only but it still didn’t. Memory was allocated from all 4 nodes.</p><p>Some more documentation search, suggested that for <code>numa_all_nodes_ptr</code> looks at <code>mems_allowed</code> field as mentioned below</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numa_all_nodes_ptr: The set of nodes to record is derived from &#x2F;proc&#x2F;self&#x2F;status, field &quot;Mems_allowed&quot;. The user should not alter this bitmask.</span><br></pre></td></tr></table></figure><p>and as Alexey Kopytov pointed in PR#138, <code>numa_all_nodes_ptr</code> and <code>numa_get_mems_allowed</code> reads the same mask.</p><p>This tends to suggest that <code>numa_get_mems_allowed</code> is broken or documentation needs to be updated.</p><p><em>Just for completeness, I also tried numctl –interleave but that too didn’t helped</em></p><h3 id="Fact-Validation"><a href="#Fact-Validation" class="headerlink" title="Fact Validation:"></a>Fact Validation:</h3><p>So I decided to try this using a simple program (outside MySQL) to validate the said fact.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;numa.h&gt;</span><br><span class="line">#include &lt;numaif.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">cout &lt;&lt; *numa_all_nodes_ptr-&gt;maskp &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; *numa_get_mems_allowed()-&gt;maskp &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">numactl --membind&#x3D;0-1 .&#x2F;a.out</span><br><span class="line">15</span><br><span class="line">15</span><br></pre></td></tr></table></figure><p>It is pretty clear that both seem to return the same mask value when <code>numa_get_mems_allowed</code> should return only memory allowed nodes.</p><h3 id="Workaround"><a href="#Workaround" class="headerlink" title="Workaround:"></a>Workaround:</h3><p>I desperately needed a solution so tried using a simple workaround of manually feeding the mask (will continue to follow up about numactl behavior with OS vendor). This approach finally worked and now I can allocate memory from selected NUMA nodes only.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">+const unsigned long numa_mask &#x3D; 0x3;</span><br><span class="line"> </span><br><span class="line"> struct set_numa_interleave_t &#123;</span><br><span class="line">   set_numa_interleave_t() &#123;</span><br><span class="line">     if (srv_numa_interleave) &#123;</span><br><span class="line">       ib::info(ER_IB_MSG_47) &lt;&lt; &quot;Setting NUMA memory policy to&quot;</span><br><span class="line">                                 &quot; MPOL_INTERLEAVE&quot;;</span><br><span class="line">-      if (set_mempolicy(MPOL_INTERLEAVE, numa_all_nodes_ptr-&gt;maskp,</span><br><span class="line">+      if (set_mempolicy(MPOL_INTERLEAVE, &amp;numa_mask,</span><br><span class="line">                         numa_all_nodes_ptr-&gt;size) !&#x3D; 0) &#123;</span><br><span class="line">         ib::warn(ER_IB_MSG_48) &lt;&lt; &quot;Failed to set NUMA memory&quot;</span><br><span class="line">                                   &quot; policy to MPOL_INTERLEAVE: &quot;</span><br><span class="line">@@ -1000,7 +1001,7 @@ static buf_chunk_t *buf_chunk_init(</span><br><span class="line"> #ifdef HAVE_LIBNUMA</span><br><span class="line">   if (srv_numa_interleave) &#123;</span><br><span class="line">     int st &#x3D; mbind(chunk-&gt;mem, chunk-&gt;mem_size(), MPOL_INTERLEAVE,</span><br><span class="line">-                   numa_all_nodes_ptr-&gt;maskp, numa_all_nodes_ptr-&gt;size,</span><br><span class="line">+                   &amp;numa_mask, numa_all_nodes_ptr-&gt;size,</span><br><span class="line">                    MPOL_MF_MOVE);</span><br><span class="line">     if (st !&#x3D; 0) &#123;</span><br><span class="line">       ib::warn(ER_IB_MSG_54) &lt;&lt; &quot;Failed to set NUMA memory policy of&quot;</span><br></pre></td></tr></table></figure><p>(Of-course this needs re-build from source code and not an option for binary/package user (well there is .. check following section)).</p><h3 id="But-then-why-didn’t-you-used-…"><a href="#But-then-why-didn’t-you-used-…" class="headerlink" title="But then why didn’t you used … ?"></a>But then why didn’t you used … ?</h3><p>Naturally, most of you may suggest that this could be avoided by toggling <code>innodb_numa_interleave</code> back to OFF and using membind. Of-course this approach works but this approach is slightly different because then all the memory allocated is bounded by the said restriction vs <code>innodb_numa_interleave</code> is applicable only during buffer pool allocation. It may serve specific purpose but may not be so called comparable.</p><p>This has been on my todo list to check effect of complete interleave vs innodb_numa_interleave.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Balance distribution on NUMA node has multiple aspects including core-selection, memory allocation, thread allocation (equally on selected numa node), etc…. Lot of exciting and surprising things to explore.</p><p><em>If you have more questions/queries do let me know. Will try to answer them.</em></p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Krunal Bauskar&lt;br&gt;原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/Running-MySQL-on-Selected-NUMA-nodes/&quot;&gt;https://mysqlonarm.github.io/Running-MySQL-on-Selected-NUMA-nodes/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;指定NUMA节点来运行Mysql，全路程实践，来试试吧&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>利用SIMD Vectorization优化PostgreSQL</title>
    <link href="https://kunpengcompute.github.io/2020/06/24/li-yong-simd-vectorization-you-hua-postgresql/"/>
    <id>https://kunpengcompute.github.io/2020/06/24/li-yong-simd-vectorization-you-hua-postgresql/</id>
    <published>2020-06-24T09:03:35.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Amit Dattatray Khandekar<br>原文链接: <a href="https://amitdkhan-pg.blogspot.com/2020/06/leveraging-simd-vectorization.html">https://amitdkhan-pg.blogspot.com/2020/06/leveraging-simd-vectorization.html</a></p><p>团队大牛Amit利用SIMD向量化对ARM和X86硬件平台在PostgreSQL上的优化，欢迎品鉴，相当硬核，坐稳了吗？</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><h3 id="Leveraging-SIMD-Vectorization"><a href="#Leveraging-SIMD-Vectorization" class="headerlink" title="Leveraging SIMD Vectorization"></a>Leveraging SIMD Vectorization</h3><p>随着列式存储数据库的出现，人们迫切需要使用 SIMD 向量处理数据表。 这种情况显然很符合表格数据的排列方式。 让我们首先简单介绍一下什么是 SIMD。 它代表单指令多数据流(Single Instruction Multiple Data)。 在当前，CPU 指令支持这种机制，在这种机制中，同一条指令可以在多个数据元素上同时执行。 例如，你想把所有的列值元素加倍。 或者删除图像像素RGB 值的红色部分。 对于大数据场景来说，这些操作是 CPU 的瓶颈。 因此，SIMD 根据每个数据元素的大小，对2、4、8、16或32个(或更多)数据元素同时进行操作，从而大大缩短了 CPU 时间。 假设我们想对“ int32 arr []”的每个元素执行“ arr [ i ] * = 2”。 通常，我们会遍历每个元素来执行这个操作。 在生成的汇编代码中，MUL 指令将在每个元素上执行。 使用 SIMD，我们将划分4个(或更多)相邻的数组元素加载到128位(或更大) CPU“向量”寄存器中，然后让这个寄存器调用 MUL 指令的“向量化”版本，并对随后的每个4数组元素重复这一步骤。</p><p>我们怎么做才能生成这样的向量化汇编指令？ 一种方法是编写这样的汇编代码。 但是在大多数情况下，我们不会这么做，多亏了以下两个方法的出现:</p><p><strong>1. 内部函数实现向量化</strong></p><p>对于程序员来说，调用内部函数就像调用其他函数一样。 在底层，编译器会用适当的程序集指令替换它。 因此，不必使用 c / c + + 代码中的汇编指令来处理寄存器，而是调用相应的内部函数。 每个 CPU 体系结构都有自己的一组内部函数API 和相应的头文件。 作为一个例子，让我们使用 ARM 架构的 SIMD内部函数对 PostgreSQL 代码片段进行向量化，看看通过向量化代码会产生多大的不同。 在此之前，您可能希望快速浏览<a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/neon-programmers-guide-for-armv8-a/introducing-neon-for-armv8-a/single-page#fundamentals">NEON架构预览</a>来了解寄存器(registers)、通道(lanes)和向量(vectors)的命名规范。 NEON是 ARM SIMD 架构的品牌名称(The implementation of the Advanced SIMD extension used in <em>ARM</em> processors is called <em>NEON</em>,)。 NEON 单元是 ARMv8芯片的必备部分。</p><p>下面是 PostgreSQL 代码片段，<a href="https://doxygen.postgresql.org/backend_2utils_2adt_2numeric_8c.html#a802955bc87af4f3479b776760c12422b">mul_var() 函数</a> 用于将两个PostgreSQL NUMERIC 数据类型的值相乘. 就像下面的例子那样:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for (i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3), i &#x3D; i1 + i2 + 2;</span><br><span class="line">   i2 &gt;&#x3D; 0; i2--)</span><br><span class="line">  dig[i--] +&#x3D; var1digit * var2digits[i2];</span><br></pre></td></tr></table></figure><p>其中变量声明为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int32 *dig;</span><br><span class="line">int16 var1digit, *var2digits;</span><br></pre></td></tr></table></figure><p>这个例子，你将可以看到循环迭代 i2 + 1次。 在每次迭代中，i 和 i2都会递减。 这意味着，两个数组中的每个数组都有一个固定的连续区段，我们希望在这个区段中对每个数组元素重复执行相同的算术运算。 这里所做的算法是: 将两个 int16变量相乘，然后将乘积加起来得到一个 int32变量。 有一条汇编指令正是这样做的: VMLA。 相应的 内部函数是: vmlal _ s16()</p><p>让我们首先将上面的反向 for-loop 简化为一个等效的正向循环 :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3);</span><br><span class="line">count &#x3D; i2 + 1;</span><br><span class="line">digptr &#x3D; &amp;dig[i1 + 2];</span><br><span class="line">for (i &#x3D; 0; i &lt; count; i++)</span><br><span class="line">  digptr[i] +&#x3D; var1digit * var2digits[i];</span><br></pre></td></tr></table></figure><p>当我们想要对上面的 multiply + accumulate 语句进行向量化时，我们应用下面这个内部函数： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int16x8_t  vmlaq_s16(int16x8_t a, int16x8_t b, int16x8_t c);</span><br></pre></td></tr></table></figure><p>这句代码执行 a + (b * c)并返回结果。 a b c 是矢量。 类型 int16x8_t 表示该向量位于一个128位的 NEON 寄存器中，该寄存器有8个通道，每个通道有16位有符号整数。 所以 vmlaq_s16并行地对3个向量的所有8个通道执行相同的multiply + accumulate操作，并在一个int16x8_t 向量中再次返回8个结果值。 每个multiply + accumulate操作都包含在所有3个向量中的一个特定通道中。<br>如上面 c 代码片段所示，为了避免溢出，将所得的乘法累计值计入一个32位整数。 因此，我们不能使用vmlaq_s16() ，而必须使用一个对16位值进行操作并返回32位值的内部函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int32x4_t vmlal_s16(int32x4_t a, int16x4_t b, int16x4_t c);</span><br></pre></td></tr></table></figure><p>由于128位矢量只能容纳4个32位数据元素，因此4个元素可以并行化，而不是8个。</p><p>可以看出，所有这些操作都使用128位寄存器，它们不需要完全占用，就像使用 int16x4向量那样。 我们需要首先将 C 数组元素值加载到这些寄存器中，最后将结果值从寄存器取回至结果数组元素中。 我们也有实现这种想法的内部函数。 尽管有混合使用标量和向量的内部函数，然而上面内部函数只使用到了向量。 因此，同样的 var1digit 值可以装载到16x4矢量的所有4个通道中。</p><p>结合这些内部函数，最终的代码会是:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;arm_neon.h&gt;</span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line">int i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3);</span><br><span class="line">int remainder;</span><br><span class="line">int count &#x3D; i2 + 1;</span><br><span class="line">int32 *digptr &#x3D; &amp;dig[i1 + 2];</span><br><span class="line"></span><br><span class="line">&#x2F;* Load the same var1digit value into all lanes of 16x4 vector. *&#x2F;</span><br><span class="line">int16x4_t  var1digit_16x4 &#x3D; vdup_n_s16(var1digit);   &#x2F;&#x2F; VDUP.16 d0,r0</span><br><span class="line"></span><br><span class="line">&#x2F;* Parallelize each group of 4 digits *&#x2F;</span><br><span class="line">remainder &#x3D; count%4;</span><br><span class="line">count -&#x3D; remainder;</span><br><span class="line">for (i &#x3D; 0; i &lt; count; i +&#x3D; 4)</span><br><span class="line">&#123;</span><br><span class="line">  &#x2F;*</span><br><span class="line">   \* 1. Load required data into vectors</span><br><span class="line">   \* 2. Do multiply-accumulate-long operation using 16x4 vectors,</span><br><span class="line">   \*  whose output is a 32x4 vector which we need, because digptr[]</span><br><span class="line">   \*  is 32bit.</span><br><span class="line">   \* 3. Store back the result vector into digptr[]</span><br><span class="line">   *&#x2F;</span><br><span class="line"></span><br><span class="line">  &#x2F;* Load 4 var2digits into 16x4 vector and digptr into 32x4 *&#x2F;</span><br><span class="line">  int16x4_t  var2digits_16x4 &#x3D; vld1_s16(&amp;var2digits[i]);</span><br><span class="line">  int32x4_t  dig_32x4 &#x3D; vld1q_s32(&amp;digptr[i]);</span><br><span class="line"></span><br><span class="line">  &#x2F;* Vector multiply-accumulate-long: vmlal_&lt;type&gt;. Vr[i] :&#x3D; Va[i] + Vb[i] * Vc[i] *&#x2F;</span><br><span class="line">  dig_32x4 &#x3D; vmlal_s16(dig_32x4, var1digit_16x4, var2digits_16x4);</span><br><span class="line"></span><br><span class="line">  &#x2F;* Store back the result into &amp;digptr[i] *&#x2F;</span><br><span class="line">  vst1q_s32(&amp;digptr[i], dig_32x4);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;* Do the last remaining digits *&#x2F;</span><br><span class="line">for (; remainder !&#x3D; 0; remainder--, i++)</span><br><span class="line">  digptr[i] +&#x3D; var1digit * var2digits[i];</span><br></pre></td></tr></table></figure><p>我创建了一个包含高精度的数据的模型，<a href="https://drive.google.com/file/d/1H7U5QMksnFuz39djRAbAcunOO8dGWwh2/view?usp=sharing">如图所示</a>, 并以多组t1.val 和 t2.val来执行如下查询。在没有向量化时，执行时间为0.874毫秒:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ psql -c &quot;explain analyze SELECT t1.id, t2.id, t1.val * t2.val FROM num_data t1, num_data t2&quot;</span><br><span class="line">                           QUERY PLAN                           </span><br><span class="line">\-----------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Nested Loop (cost&#x3D;0.00..1039.85 rows&#x3D;67600 width&#x3D;40) (actual time&#x3D;0.016..0.840 rows&#x3D;100 loops&#x3D;1)</span><br><span class="line">  -&gt; Seq Scan on num_data t1 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.003..0.004 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line">  -&gt; Materialize (cost&#x3D;0.00..13.90 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;10)</span><br><span class="line">     -&gt; Seq Scan on num_data t2 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line"> Planning Time: 0.156 ms</span><br><span class="line"> Execution Time: **0.874** ms</span><br><span class="line">(6 rows)</span><br><span class="line"></span><br><span class="line">With the above vectorized code, the same query execution time is now .360 ms, i.e. more than 2x speedup :</span><br><span class="line"></span><br><span class="line">$ psql -c &quot;explain analyze SELECT t1.id, t2.id, t1.val * t2.val FROM num_data t1, num_data t2&quot;</span><br><span class="line">                           QUERY PLAN                           </span><br><span class="line">\-----------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Nested Loop (cost&#x3D;0.00..1039.85 rows&#x3D;67600 width&#x3D;40) (actual time&#x3D;0.016..0.322 rows&#x3D;100 loops&#x3D;1)</span><br><span class="line">  -&gt; Seq Scan on num_data t1 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.007..0.008 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line">  -&gt; Materialize (cost&#x3D;0.00..13.90 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;10)</span><br><span class="line">     -&gt; Seq Scan on num_data t2 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line"> Planning Time: 0.169 ms</span><br><span class="line"> Execution Time: **0.360** ms</span><br><span class="line">(6 rows)</span><br></pre></td></tr></table></figure><p>使用上面的向量化代码，相同的查询执行时间现在是0.360 ms，即超过2倍的加速: :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ psql -c &quot;explain analyze SELECT t1.id, t2.id, t1.val * t2.val FROM num_data t1, num_data t2&quot;</span><br><span class="line">                           QUERY PLAN                           </span><br><span class="line">\-----------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Nested Loop (cost&#x3D;0.00..1039.85 rows&#x3D;67600 width&#x3D;40) (actual time&#x3D;0.016..0.322 rows&#x3D;100 loops&#x3D;1)</span><br><span class="line">  -&gt; Seq Scan on num_data t1 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.007..0.008 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line">  -&gt; Materialize (cost&#x3D;0.00..13.90 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;10)</span><br><span class="line">     -&gt; Seq Scan on num_data t2 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line"> Planning Time: 0.169 ms</span><br><span class="line"> Execution Time: **0.360** ms</span><br><span class="line">(6 rows)</span><br></pre></td></tr></table></figure><p>由于数字的个别位数必须与另一个数字的位数相乘，对于精度较高的数字来说，效果更好。 我创建的模式精度在200-600之间。 但是当我在 ARM64 VM 上的测试时，从20精度开始，它的好处就显现出来了。</p><p><strong>2. 自动向量化</strong></p><p>并不总是需要编写使用 内部函数的代码。通常，如果我们组织并简化代码，今天的编译器，使用适当的编译器选项<a href="https://gcc.gnu.org/projects/tree-ssa/vectorization.html#using">尝试识别代码是否可以被向量化</a>, 并生成适当的汇编指令，以便利用 CPU 体系结构的 SIMD。实际上，在上面的代码中，我将反向 for-loop 简化为使用单个变量递增的正向 for-loop，gcc 编译器能够自动对简化的 for-loop 进行向量化。 以下是一些细节:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">diff --git a&#x2F;src&#x2F;backend&#x2F;utils&#x2F;adt&#x2F;numeric.c b&#x2F;src&#x2F;backend&#x2F;utils&#x2F;adt&#x2F;numeric.c</span><br><span class="line">index f3a725271e..4243242ad9 100644</span><br><span class="line">--- a&#x2F;src&#x2F;backend&#x2F;utils&#x2F;adt&#x2F;numeric.c</span><br><span class="line">+++ b&#x2F;src&#x2F;backend&#x2F;utils&#x2F;adt&#x2F;numeric.c</span><br><span class="line">@@ -7226,6 +7226,7 @@ mul_var(const NumericVar *var1, const NumericVar *var2, NumericVar *result,</span><br><span class="line">   int        res_weight;</span><br><span class="line">   int        maxdigits;</span><br><span class="line">   int      *dig;</span><br><span class="line">\+   int      *digptr;</span><br><span class="line">   int        carry;</span><br><span class="line">   int        maxdig;</span><br><span class="line">   int        newdig;</span><br><span class="line">@@ -7362,10 +7363,14 @@ mul_var(const NumericVar *var1, const NumericVar *var2, NumericVar *result,</span><br><span class="line">       *</span><br><span class="line">       \* As above, digits of var2 can be ignored if they don&#39;t contribute,</span><br><span class="line">       \* so we only include digits for which i1+i2+2 &lt;&#x3D; res_ndigits - 1.</span><br><span class="line">\+      *</span><br><span class="line">\+      * For large precisions, this can become a bottleneck; so keep this for</span><br><span class="line">\+      * loop simple so that it can be auto-vectorized.</span><br><span class="line">       *&#x2F;</span><br><span class="line">\-      for (i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3), i &#x3D; i1 + i2 + 2;</span><br><span class="line">\-         i2 &gt;&#x3D; 0; i2--)</span><br><span class="line">\-         dig[i--] +&#x3D; var1digit * var2digits[i2];</span><br><span class="line">\+      i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3);</span><br><span class="line">\+      digptr &#x3D; &amp;dig[i1 + 2];</span><br><span class="line">\+      for (i &#x3D; 0; i &lt;&#x3D; i2; i++)</span><br><span class="line">\+         digptr[i] +&#x3D; var1digit * var2digits[i];</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>通过这个修改，在 mul_var()汇编代码中，我可以看到操作 NEON 向量的乘积指令(这些是 arm64指令) :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">smlal  v1.4s, v2.4h, v3.4h</span><br><span class="line">smlal2 v0.4s, v2.8h, v3.8h</span><br></pre></td></tr></table></figure><p>gcc 编译器选项启用自动向量化是“-ftree-loop-vectorize”. 当使用 gcc -O3时，它始终是开启的。</p><p>虽然有一些例子表明 gcc 能够自动向量化甚至是反向循环，但是在上面的例子中，由于两个递减变量，它不能对原始代码这样做。 这就是为什么我必须将其简化为一个单变量递增的正向循环，这是最简单的方式来规避。</p><p>要检查 gcc 是否能够向量化一段代码，请使用 gcc  -fopt-info-all 选项。输出信息如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">numeric.c:7217:3: optimized: loop vectorized using 16 byte vectors</span><br><span class="line">Or in case it can&#39;t vectorize, you would see something like this :</span><br><span class="line">numeric.c:7380:3: missed: couldn&#39;t vectorize loop</span><br><span class="line">numeric.c:7381:15: missed: not vectorized: relevant stmt not supported: _39 &#x3D; *_38;</span><br></pre></td></tr></table></figure><p>用这种自动向量化的方法，我观察到的加速比大约是2.7倍。 这种加速比内部函数方式更快快，可能是因为编译器可能比我使用了更好的汇编向量化指令组合。</p><p><strong>总结</strong></p><p>向量化操作可以在重复操作中获得显著的性能提升。 虽然它很适合柱状数据结构，但是当前 PostgreSQL 代码中的一些代码可能会受益于利用 SIMD 进行这种调整。 尽可能使用编译器的自动向量化。 因为这样的做会使代码更干净，更容易移植。 与方法1相比，我们必须使用特定于 CPU 体系结构的内部函数。 但是选择这个例子是为了解释如何使用内部函数来向量化。 在编译器不能对代码进行向量化的情况下，我们应该使用编译器内部函数。 例如:<a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/neon-programmers-guide-for-armv8-a/optimizing-c-code-with-neon-intrinsics/single-page#rgb">这个</a>。</p></div><div id="English" class="tab-content"><p>With the advent of column store databases, there was an urge to make use of SIMD vector processing. It naturally fits into the way table data is arranged. Let’s first briefly check what is SIMD. It stands for Single Instruction Multiple Data. Today, CPU instructions support this kind of mechanism where the same instruction can be executed simultaneously on multiple data elements. E.g. Say, you want to double all the column values. Or remove the red component of the RGB values of pixels of an image. For large data, these operations are CPU bottlenecks. So SIMD cuts the CPU time significantly by operating simultaneously on 2, 4, 8, 16 or 32 (or more) data elements depending on the size of each data element. So suppose we want to do “arr[i] *= 2” for each element of “int32 arr[]”. Normally we would iterate through each of the elements for doing this operation. In the generated assembly code, MUL instruction will be run on each of the elements. With SIMD, we would arrange for loading 4 (or more) adjacent array elements into a 128-bit (or larger) CPU “vector” register, and then arrange for a “vectorized” version of the MUL instruction to be called using this register, and repeat this for each subsequent 4 element array section.</p><p>How do we arrange for generating such vectorized assembly instructions ? Well, one way is to write such an assembly code. But in most of the cases, we won’t need this method, thanks to the below two methods :</p><p><strong>1. Vectorization Intrinsics</strong></p><p>For a programmer, an intrinsic is just like any other function call. Underneath, the compiler replaces it with an appropriate assembly instruction. So instead of having to deal with registers using assembly instruction inside C/C++ code, call the corresponding intrinsic function. Each CPU architecture has it’s own set of intrinsics API, and corresponding header file. As an example, let’s vectorize a snippet of PostgreSQL code using ARM architecture’s SIMD intrinsics, to see how big a difference it makes by vectorizing things. Before that, you might want to quickly go through the <a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/neon-programmers-guide-for-armv8-a/introducing-neon-for-armv8-a/single-page#fundamentals">NEON architecture</a> to understand the naming conventions for registers, lanes and vectors. NEON is ARM’s brand name for SIMD architecture. NEON unit is a mandatory part of ARMv8 chip.</p><p>Here is a PostgreSQL code snippet from the <a href="https://doxygen.postgresql.org/backend_2utils_2adt_2numeric_8c.html#a802955bc87af4f3479b776760c12422b">mul_var() function</a> that is used to multiply two PostgreSQL NUMERIC data types. As of this writing, it looks like this :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for (i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3), i &#x3D; i1 + i2 + 2;</span><br><span class="line">   i2 &gt;&#x3D; 0; i2--)</span><br><span class="line">  dig[i--] +&#x3D; var1digit * var2digits[i2];</span><br></pre></td></tr></table></figure><p>where, the variables are declared as :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int32 *dig;</span><br><span class="line">int16 var1digit, *var2digits;</span><br></pre></td></tr></table></figure><p>Here, you can see that the loop iterates i2+1 times. On each iteration, both i and i2 are decremented. That means, there is a fixed contiguous section of each of the two arrays where we want to repeatedly do the same arithmetic operation for every array element in this section. The arithmetic being done here is : multiply two int16 variables, and add up that product into an int32 variable. An assembly instruction is available which exactly does that : VMLA. The corresponding intrinsic is : vmlal_s16()</p><p>Let’s first simplify the above backward for-loop into an equivalent forward loop :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3);</span><br><span class="line">count &#x3D; i2 + 1;</span><br><span class="line">digptr &#x3D; &amp;dig[i1 + 2];</span><br><span class="line">for (i &#x3D; 0; i &lt; count; i++)</span><br><span class="line">  digptr[i] +&#x3D; var1digit * var2digits[i];</span><br></pre></td></tr></table></figure><p>So we want to vectorize the above multiply+accumulate statement. We have this intrinsic :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int16x8_t  vmlaq_s16(int16x8_t a, int16x8_t b, int16x8_t c);</span><br></pre></td></tr></table></figure><p>This does a+(b*c) and returns the result. a, b and c are vectors. The type int16x8_t signifies that the vector is in a 128-bit NEON register having 8 lanes, each lane having 16-bit signed integers. So vmlaq_s16() does the multiply+accumulate operation on all 8 lanes of the 3 vectors in parallel, and returns the 8 result values again in a int16x8_t vector. Each multiple+accumulate is contained in one particular lane of all the 3 vectors.<br>To avoid overflow, as can be seen in the above C snippet, the multiplication is accumulated into a 32-bit integer. So instead of vmlaq_s16(), we have to use an intrinsic that operates on 16-bit values and returns 32bit values :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int32x4_t vmlal_s16(int32x4_t a, int16x4_t b, int16x4_t c);</span><br></pre></td></tr></table></figure><p>Since only 4 32-bit data elements can be accommodated in a 128-bit vector, 4 elements could be parallelized rather than 8.</p><p>As can be seen, all these operations use the 128-bit registers, even though they need not be fully occupied, as in the case with int16x4 vectors. We need to first load the C array element values into these registers, and in the end, store the resultant values back from the registers into the result array elements. We have intrinsics for that also. Although there are intrinsics that operate on a mix of scalar and vectors, the intrinsic used above uses only vectors. So the same var1digit value can be loaded into all 4 lanes of a 16x4 vector.</p><p>With these instrinsics, the final code looks like this :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">\#include &lt;arm_neon.h&gt;</span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line">int i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3);</span><br><span class="line">int remainder;</span><br><span class="line">int count &#x3D; i2 + 1;</span><br><span class="line">int32 *digptr &#x3D; &amp;dig[i1 + 2];</span><br><span class="line"></span><br><span class="line">&#x2F;* Load the same var1digit value into all lanes of 16x4 vector. *&#x2F;</span><br><span class="line">int16x4_t  var1digit_16x4 &#x3D; vdup_n_s16(var1digit);   &#x2F;&#x2F; VDUP.16 d0,r0</span><br><span class="line"></span><br><span class="line">&#x2F;* Parallelize each group of 4 digits *&#x2F;</span><br><span class="line">remainder &#x3D; count%4;</span><br><span class="line">count -&#x3D; remainder;</span><br><span class="line">for (i &#x3D; 0; i &lt; count; i +&#x3D; 4)</span><br><span class="line">&#123;</span><br><span class="line">  &#x2F;*</span><br><span class="line">   \* 1. Load required data into vectors</span><br><span class="line">   \* 2. Do multiply-accumulate-long operation using 16x4 vectors,</span><br><span class="line">   \*  whose output is a 32x4 vector which we need, because digptr[]</span><br><span class="line">   \*  is 32bit.</span><br><span class="line">   \* 3. Store back the result vector into digptr[]</span><br><span class="line">   *&#x2F;</span><br><span class="line"></span><br><span class="line">  &#x2F;* Load 4 var2digits into 16x4 vector and digptr into 32x4 *&#x2F;</span><br><span class="line">  int16x4_t  var2digits_16x4 &#x3D; vld1_s16(&amp;var2digits[i]);</span><br><span class="line">  int32x4_t  dig_32x4 &#x3D; vld1q_s32(&amp;digptr[i]);</span><br><span class="line"></span><br><span class="line">  &#x2F;* Vector multiply-accumulate-long: vmlal_&lt;type&gt;. Vr[i] :&#x3D; Va[i] + Vb[i] * Vc[i] *&#x2F;</span><br><span class="line">  dig_32x4 &#x3D; vmlal_s16(dig_32x4, var1digit_16x4, var2digits_16x4);</span><br><span class="line"></span><br><span class="line">  &#x2F;* Store back the result into &amp;digptr[i] *&#x2F;</span><br><span class="line">  vst1q_s32(&amp;digptr[i], dig_32x4);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;* Do the last remaining digits *&#x2F;</span><br><span class="line">for (; remainder !&#x3D; 0; remainder--, i++)</span><br><span class="line">  digptr[i] +&#x3D; var1digit * var2digits[i];</span><br></pre></td></tr></table></figure><p>I created a schema that contains numerics with large precisions, <a href="https://drive.google.com/file/d/1H7U5QMksnFuz39djRAbAcunOO8dGWwh2/view?usp=sharing">as shown here</a>, and ran the following query that multiplies t1.val and t2.val. With the non-vectorized code, the execution time showed .874 milliseconds :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ psql -c &quot;explain analyze SELECT t1.id, t2.id, t1.val * t2.val FROM num_data t1, num_data t2&quot;</span><br><span class="line">                           QUERY PLAN                           </span><br><span class="line">\-----------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Nested Loop (cost&#x3D;0.00..1039.85 rows&#x3D;67600 width&#x3D;40) (actual time&#x3D;0.016..0.840 rows&#x3D;100 loops&#x3D;1)</span><br><span class="line">  -&gt; Seq Scan on num_data t1 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.003..0.004 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line">  -&gt; Materialize (cost&#x3D;0.00..13.90 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;10)</span><br><span class="line">     -&gt; Seq Scan on num_data t2 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line"> Planning Time: 0.156 ms</span><br><span class="line"> Execution Time: **0.874** ms</span><br><span class="line">(6 rows)</span><br><span class="line"></span><br><span class="line">With the above vectorized code, the same query execution time is now .360 ms, i.e. more than 2x speedup :</span><br><span class="line"></span><br><span class="line">$ psql -c &quot;explain analyze SELECT t1.id, t2.id, t1.val * t2.val FROM num_data t1, num_data t2&quot;</span><br><span class="line">                           QUERY PLAN                           </span><br><span class="line">\-----------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> Nested Loop (cost&#x3D;0.00..1039.85 rows&#x3D;67600 width&#x3D;40) (actual time&#x3D;0.016..0.322 rows&#x3D;100 loops&#x3D;1)</span><br><span class="line">  -&gt; Seq Scan on num_data t1 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.007..0.008 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line">  -&gt; Materialize (cost&#x3D;0.00..13.90 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;10)</span><br><span class="line">     -&gt; Seq Scan on num_data t2 (cost&#x3D;0.00..12.60 rows&#x3D;260 width&#x3D;275) (actual time&#x3D;0.001..0.002 rows&#x3D;10 loops&#x3D;1)</span><br><span class="line"> Planning Time: 0.169 ms</span><br><span class="line"> Execution Time: **0.360** ms</span><br><span class="line">(6 rows)</span><br></pre></td></tr></table></figure><p>Since individual digits of the number have to be multiplied by the digits of the other number, the benefit is more for numerics with large precision. The schema I created has values with precisions in the range of 200-600. But the benefit starts showing up from around 20 precision onwards, with my ARM64 VM.</p><p><strong>2. Auto-vectorization</strong></p><p>It’s not always necessary to write code that uses intrinsics. Often if we arrange/simplify the code, today’s compilers, with appropriate compiler options, <a href="https://gcc.gnu.org/projects/tree-ssa/vectorization.html#using">try to identify if the code can be vectorized</a>, and generate appropriate assembly instructions that leverage the CPU architecture’s SIMD. In fact, above where I simplified the backward for-loop to a forward for-loop that uses a single variable increment, the gcc compiler is able to auto-vectorize the simplified for-loop. Here are the changes again:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">diff --git a&#x2F;src&#x2F;backend&#x2F;utils&#x2F;adt&#x2F;numeric.c b&#x2F;src&#x2F;backend&#x2F;utils&#x2F;adt&#x2F;numeric.c</span><br><span class="line">index f3a725271e..4243242ad9 100644</span><br><span class="line">--- a&#x2F;src&#x2F;backend&#x2F;utils&#x2F;adt&#x2F;numeric.c</span><br><span class="line">+++ b&#x2F;src&#x2F;backend&#x2F;utils&#x2F;adt&#x2F;numeric.c</span><br><span class="line">@@ -7226,6 +7226,7 @@ mul_var(const NumericVar *var1, const NumericVar *var2, NumericVar *result,</span><br><span class="line">   int        res_weight;</span><br><span class="line">   int        maxdigits;</span><br><span class="line">   int      *dig;</span><br><span class="line">\+   int      *digptr;</span><br><span class="line">   int        carry;</span><br><span class="line">   int        maxdig;</span><br><span class="line">   int        newdig;</span><br><span class="line">@@ -7362,10 +7363,14 @@ mul_var(const NumericVar *var1, const NumericVar *var2, NumericVar *result,</span><br><span class="line">       *</span><br><span class="line">       \* As above, digits of var2 can be ignored if they don&#39;t contribute,</span><br><span class="line">       \* so we only include digits for which i1+i2+2 &lt;&#x3D; res_ndigits - 1.</span><br><span class="line">\+      *</span><br><span class="line">\+      * For large precisions, this can become a bottleneck; so keep this for</span><br><span class="line">\+      * loop simple so that it can be auto-vectorized.</span><br><span class="line">       *&#x2F;</span><br><span class="line">\-      for (i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3), i &#x3D; i1 + i2 + 2;</span><br><span class="line">\-         i2 &gt;&#x3D; 0; i2--)</span><br><span class="line">\-         dig[i--] +&#x3D; var1digit * var2digits[i2];</span><br><span class="line">\+      i2 &#x3D; Min(var2ndigits - 1, res_ndigits - i1 - 3);</span><br><span class="line">\+      digptr &#x3D; &amp;dig[i1 + 2];</span><br><span class="line">\+      for (i &#x3D; 0; i &lt;&#x3D; i2; i++)</span><br><span class="line">\+         digptr[i] +&#x3D; var1digit * var2digits[i];</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>With this change, in mul_var() assembly code, I could see the multiply-accumulate instructions that operate on NEON vectors (these are arm64 instructions) :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">smlal  v1.4s, v2.4h, v3.4h</span><br><span class="line">smlal2 v0.4s, v2.8h, v3.8h</span><br></pre></td></tr></table></figure><p>gcc compiler option to enable auto-vectorization is “-ftree-loop-vectorize”. With gcc -O3, it is always enabled.</p><p>Although there are examples where gcc is able to auto-vectorize even backward loops, in the above case, it could not do so for the original code, seemingly because of two decrementing variables. That’s why I had to simplify it to a forward loop with a single variable increment, which is as simple as it gets.</p><p>To check whether gcc has been able to vectorize a particular code, use the gcc -fopt-info-all option. This outputs info such as this :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">numeric.c:7217:3: optimized: loop vectorized using 16 byte vectors</span><br><span class="line">Or in case it can&#39;t vectorize, you would see something like this :</span><br><span class="line">numeric.c:7380:3: missed: couldn&#39;t vectorize loop</span><br><span class="line">numeric.c:7381:15: missed: not vectorized: relevant stmt not supported: _39 &#x3D; *_38;</span><br></pre></td></tr></table></figure><p>With this auto-vectorization method, the speedup I observed was around 2.7x. This speedup is higher than the intrinsics method, probably because the compiler might have used a better combination of assembly vectorized instructions than I did.</p><p><strong>Conclusion</strong></p><p>Vectorizing operations gives significant returns in repetitive operations. Although it suits well for columnar data, there could be some regions in current PostgreSQL code that might benefit from such tweaks to leverage SIMD. As far as possible, we should arrange for the compiler’s auto-vectorization. Such change is cleaner and clearly portable. Compare this with method 1 where we had to use intrinsics specific to the CPU architecture. But that example was chosen for the sake of explaining how to make use of intrinsics. In cases where it is not possible for the compiler to vectorize the code, we should use compiler intrinsics. E.g. <a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/neon-programmers-guide-for-armv8-a/optimizing-c-code-with-neon-intrinsics/single-page#rgb">check this out</a>.</p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Amit Dattatray Khandekar&lt;br&gt;原文链接: &lt;a href=&quot;https://amitdkhan-pg.blogspot.com/2020/06/leveraging-simd-vectorization.html&quot;&gt;https://amitdkhan-pg.blogspot.com/2020/06/leveraging-simd-vectorization.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;团队大牛Amit利用SIMD向量化对ARM和X86硬件平台在PostgreSQL上的优化，欢迎品鉴，相当硬核，坐稳了吗？&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>NUMA 智能全局计数器 -- 来自MySQL的灵感</title>
    <link href="https://kunpengcompute.github.io/2020/05/28/numa-zhi-neng-quan-ju-ji-shu-qi-lai-zi-mysql-de-ling-gan/"/>
    <id>https://kunpengcompute.github.io/2020/05/28/numa-zhi-neng-quan-ju-ji-shu-qi-lai-zi-mysql-de-ling-gan/</id>
    <published>2020-05-28T06:49:20.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Krunal Bauskar<br>原文链接: <a href="https://mysqlonarm.github.io/NUMA-Smart-Global-Counter/">https://mysqlonarm.github.io/NUMA-Smart-Global-Counter/</a></p><p>通过之前在X86, ARM虚机上的调研，与遇到的跨NUMA问题，结合自身在运行benchmark测试中的经验，让应用程序在针对特定的全局数据结构中更好的应用底层硬件，达到极致性能体验。下面来用数据说话。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>在多线程系统中管理全局计数器一直是一项挑战。 它们是限制软件可扩展性的一大难题。NUMA 的引入只是增加了复杂性。 幸运的是，在硬件平台提供的支持下发现了多种选择，以帮助解决 / 缓解其中的一些问题。 在本博客中，我们将讨论如何使全局计数器感知NUMA，做到智能选择NUMA，并且看看每种方法对性能有什么影响。</p><p>注意: 很多这方面的工作都是受 MySQL 代码库的启发，是由于MySQL 代码库不断发展中会尝试解决这个问题。</p><h2 id="全局计数器"><a href="#全局计数器" class="headerlink" title="全局计数器"></a>全局计数器</h2><p>大部分软件(例如: 数据库、web-server等等)都需要维持全局计数器。 由于是全局的，这些计数器只有一个副本，有多个工作线程试图更新它。 当然，这复杂的更新需要进行精细的协调，单从这一点，在性能上它就成为可伸缩性的热点分析对象。</p><p>另一种方式是松散地维护这些计数器（不加任何协调工作），但这意味着它们将表示一个近似的数字（特别是在竞争激烈的系统上）。这样不会对性能有过多影响，有一定的益处。</p><p>让我们看看当前生态系统提供了哪些方法来帮助解决这个问题。</p><h3 id="配置说明"><a href="#配置说明" class="headerlink" title="配置说明"></a>配置说明</h3><p>为了评估不同的方法，我们将考虑一个通用的设置。In order to evaluate different approaches we will consider a common setup.</p><ul><li>试想一些counter-block，它们位于全局级别，因此所有线程都会时不时地更新它们</li><li>试想一些data-block(实际工作负载发生的地方) ，data-block的一部分操作会更新本地全局计数器。 每个数据块都有自己的本地计数器。 只要访问data-block，就会更新这些本地计数器。 这两个data-block是交错的。 请参阅以下的分布图</li></ul><p><img src="https://mysqlonarm.github.io/images/blog7/numa-counter-mem-allocation.png" alt="img"></p><ul><li>让我们通过一些简单的数值例子来理解这个结构。<ul><li>假设我们有100个全局counter-block，每个data-block有10个counter。</li><li>假设我们有1000个全局data-block，它们与counter-block互相交织在一起。</li><li>这意味着，1-counter-block 计数器块之后是10个数据块，这样组合重复100次。</li><li>这样可以确保在 NUMA 节点上分布完整的内存块，并且在访问计数器和data-block时可以看到对NUMA 产生的影响。</li></ul></li><li>Workload (one-round):<ul><li>将访问 n 个data-block(至少足以使 L2缓存失效)。</li><li>作为data-block访问的一部分，还更新与data-block相关联的本地计数器。 data-block是使用 rand ()函数随机选择的，以确保扩散分布。</li><li>接下来是从counter-block访问和更新全局计数器。 随机选择counter-block，并从中随机选择一个计数器(inc 操作)。 重复 M次。</li></ul></li><li>Workload 循环 K 次(rounds).<ul><li>每个线程执行上述Workload循环(K 次) ，从1-256 / 2048开始使用不同的可伸缩性进行基准测试。</li></ul></li></ul><p>注意: 计数器只是简单的uint64_t 值(目前只使用 inc 操作)。</p><p>如果您有兴趣了解更多关于这方面的信息，您可以随时查看<a href="https://github.com/mysqlonarm/benchmark-suites/tree/master/numa/global-counters">这里</a>的详细代码。</p><h3 id="使用的硬件描述"><a href="#使用的硬件描述" class="headerlink" title="使用的硬件描述"></a>使用的硬件描述</h3><ul><li>x86-vm: 24 vCPU (12 cores with HT), 48 GB memory, 2 NUMA nodes, Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz (Ubuntu-18.04)</li><li>arm-vm: 24 vCPU (24 cores), 48 GB memory, 2 NUMA nodes, Kunpeng 920 2.60 GHz (Ubuntu-18.04)</li></ul><p><strong>用于1-2048伸缩性测试的机器</strong></p><ul><li>arm-bms: 128 physical cores, 1 TB of memory, 4 NUMA nodes, Kunpeng 920 2.60 GHz (CentOS-7.8)</li></ul><p>我们的意图不是比较 x86和 ARM，而是比较 NUMA 对全局计数器的影响。</p><h2 id="测试方案"><a href="#测试方案" class="headerlink" title="测试方案"></a>测试方案</h2><p>作为实验的一部分，我们评估了从基础到高级的多种方法。</p><ul><li><strong>pthread-mutex based:</strong> 保护计数器操作的简单 pthread mutexes</li></ul><ul><li><strong>std::mutex:</strong> C++11 支持的 std::mutexes，和pthread mutexes很像，但是更容易使用。</li></ul><ul><li><strong>std::atomic:</strong> C++11 原子变量。</li></ul><ul><li><strong>fuzzy-counter (来自mysql):</strong> 有 n 个 cacheline 对齐的插槽。 随机选择要更新的一个插槽。 若要计算计数器的总值，需要从所有槽中添加值。 没有用于保护槽操作的互斥对象 / 原子对象。 这意味着数值是近似的，但是当需要想之前计算总是时效果最好。 我们将在结果部分看到一个方差系数。[ref: ib_counter_t. N 通常是核心的数量]</li></ul><ul><li><strong>shard-atomic-counter (来自mysql):</strong> 计数器被分割成 N个分片(如上面的 slot)。 每个处理流程都需要知道更新哪个分片。 为了高效的访问，分片都做了cacheline对齐。 [ ref: Counter: : Shard ]</li></ul><ul><li><strong>shard-atomic-counter (thread-id based):</strong> 计数器被分割成 N 个分片(如上面的槽)。 根据执行线程的线程 id 选择要更新的分片。 为了高效的访问，分片都做了cacheline对齐。[这里 N是活动核心的数量]</li></ul><ul><li><p><strong>shard-atomic-counter (cpu-id based):</strong> 计数器被分割成 N个分片。 根据执行核心的 core-id 选择要更新的分片。 为了高效的访问，分片都做了cacheline对齐。[这里N是活动核心的数量。使用sched_getcpu获得cpu-id]</p></li><li><p><strong>shard-atomic-counter (numa-id based):</strong> 计数器被分成 N 个分片。 根据执行核心的 numa-node-id 选择要更新的碎片。 为了高效的访问，分片都做了cacheline对齐。 [这里 N 是活动节点数。 N比较小，在2 / 4 / 8范围内，不是32 / 64 / 128 /等]</p></li></ul><p><em>值得一提的是Mysql 内部还有另一个计数器结构 ut_lock_free_cnt_t()。 它尝试为各自 NUMA 上的每个计数器(值)分配内存，但是在每个numa_alloc_onnode种，即使是8个字节的小块也会按照系统页面大小的分配( Linux 4KB)。 这是大大的空间浪费。 我尝试过这种方法，但由于超负荷的巨大内存，最终没能成功分配内存。</em></p><p>让我们找出哪种方法在 NUMA 环境中最有效。</p><h3 id="Benchmarking"><a href="#Benchmarking" class="headerlink" title="Benchmarking"></a>Benchmarking</h3><p>使用上面解释的结构和workflow进行基准测试。 每次运行分配好内存后，然后为每个可伸缩性测试运行 K 轮Workflow。 下面的Timing 包括处理数据和计数器的时间，但大部分时间来自计数器争用(通过压制data-block处理已确认)。</p><ul><li><strong>x86-vm [x轴: threads(1-256), y轴: time秒. 越低越好]</strong></li></ul><p><em>数据集:</em> 100 个global counter-block, 每个block中10个counter, 1m data-blocks (每个block一个本地counter), 循环10K次</p><p><img src="https://mysqlonarm.github.io/images/blog7/x86.vm.1-256.png" alt="img"></p><h4 id="点评"><a href="#点评" class="headerlink" title="点评"></a>点评</h4><ul><li>正如预期的那样，<strong>带有 cpu-id</strong> 的 shard-atomic-counter 性能最好。</li><li>值得注意的是，简单的原子操作也是最佳的，并且节省了大量的空间。(没有cacheline对齐)。可能是 VM 影响的</li><li>另一个无法解释的行为: fuzzy counter被认为是最快的，但是测试中它不是最快的(我运行了3次同样的benchmark测试证实了这个现象。 在 ARM 上，它表现如预期的那样，因此不像是在基准测试代码出错导致的，需要再分析一下)。</li></ul><p><em>直线非常接近 / 重叠，因此为了确切的表述，共享一下具体数据。</em></p><table><thead><tr><th align="left">threads</th><th align="left">p-mutex</th><th align="left">std-mutex</th><th align="left">atomic</th><th align="left">fuzzy</th><th align="left">shard-rand</th><th align="left">shard-tid</th><th align="left">shard-cpuid</th><th align="left">shard-numaid</th></tr></thead><tbody><tr><td align="left">128</td><td align="left">305.89</td><td align="left">312.78</td><td align="left">275.21</td><td align="left">306.62</td><td align="left">273.52</td><td align="left">278.14</td><td align="left">263.5</td><td align="left">352.45</td></tr><tr><td align="left">256</td><td align="left">608.21</td><td align="left">625.37</td><td align="left">549.15</td><td align="left">611.97</td><td align="left">546.04</td><td align="left">560.18</td><td align="left">521.25</td><td align="left">705.17</td></tr></tbody></table><hr><ul><li><strong>arm-vm [x轴: threads(1-256), y轴: time秒.  越低越好]</strong></li></ul><p><em>数据集:</em> 100 个global counter-block, 每个block中10个counter, 1m data-blocks (每个block一个本地counter), 循环10K次</p><p><img src="https://mysqlonarm.github.io/images/blog7/arm.vm.1-256.png" alt="img"></p><h4 id="点评-1"><a href="#点评-1" class="headerlink" title="点评"></a>点评</h4><ul><li>同样，shard-atomic-counter (这次<strong>使用 thread-id</strong>)的得分高于其他方法。 (原因之一可能是在ARM上消耗巨大的sched_getcpu)。 [对于thread-id，线程开始时，在线程本地存储中缓存该线程的唯一标识符]</li><li>FuzzyCounter正在帮助建立基线(假设没有争用的情况下)。</li><li>老旧的 pthread-mutex 似乎也得到了优化</li><li>令人感兴趣的是，随着可伸缩性的增加，ARM 似乎显示出较低的争用(可能是由于 NUMA 的互连性更好)。</li></ul><p><em>直线线条非常接近，在某些情况下也会重叠，因此为了更好的表述，数据如下。</em></p><table><thead><tr><th align="left">threads</th><th align="left">p-mutex</th><th align="left">std-mutex</th><th align="left">atomic</th><th align="left">fuzzy</th><th align="left">shard-rand</th><th align="left">shard-tid</th><th align="left">shard-cpuid</th><th align="left">shard-numaid</th></tr></thead><tbody><tr><td align="left">128</td><td align="left">265.05</td><td align="left">271.53</td><td align="left">272.06</td><td align="left">241.26</td><td align="left">287.1</td><td align="left">258.9</td><td align="left">337.2</td><td align="left">396.88</td></tr><tr><td align="left">256</td><td align="left">529.74</td><td align="left">546.74</td><td align="left">544.07</td><td align="left">481.71</td><td align="left">574.05</td><td align="left">520</td><td align="left">671.63</td><td align="left">795.92</td></tr></tbody></table><hr><ul><li><strong>arm-bms [x轴: threads(1-2048), y轴: time秒. 越低越好]</strong></li></ul><p><em>数据集:</em> 100 个global counter-block, 每个block中10个counter, 1m data-blocks (每个block一个本地counter), 循环1K次</p><p><img src="https://mysqlonarm.github.io/images/blog7/arm.bms.1-2048.png" alt="img"></p><h4 id="点评-2"><a href="#点评-2" class="headerlink" title="点评"></a>点评</h4><ul><li>Fuzzy-Counter 帮助设置基线，但是这次我们看到 shard-atomic-counter (<strong>带有thread-id</strong>)几乎与 Fuzzy-Counter (无争用情况)相当。 似乎这就是预期的最佳数字。</li></ul><p><em>直线非常接近，在某些情况下也会重叠，因此为了更好的表述，参看下列具体数据。 以防你没有注意到测试循环次数已经减少到了1K。 由于跨NUMA访问和增加的可伸缩性，保持循环10K次可能会造成更多的时间消耗，也会有更多的噪音。 (注意: 我们现在裸机有4个 NUMA节点)。</em></p><table><thead><tr><th align="left">threads</th><th align="left">p-mutex</th><th align="left">std-mutex</th><th align="left">atomic</th><th align="left">fuzzy</th><th align="left">shard-rand</th><th align="left">shard-tid</th><th align="left">shard-cpuid</th><th align="left">shard-numaid</th></tr></thead><tbody><tr><td align="left">128</td><td align="left">62.81</td><td align="left">63.9</td><td align="left">66.24</td><td align="left">57.37</td><td align="left">64.24</td><td align="left">54.09</td><td align="left">57.67</td><td align="left">72.08</td></tr><tr><td align="left">256</td><td align="left">115.39</td><td align="left">119.53</td><td align="left">126.52</td><td align="left">102.68</td><td align="left">119.01</td><td align="left">102.13</td><td align="left">106.3</td><td align="left">140.83</td></tr><tr><td align="left">512</td><td align="left">228.2</td><td align="left">234.5</td><td align="left">252</td><td align="left">199.71</td><td align="left">241.69</td><td align="left">205.66</td><td align="left">211.29</td><td align="left">279.81</td></tr><tr><td align="left">1024</td><td align="left">456.53</td><td align="left">470.55</td><td align="left">503.73</td><td align="left">398.61</td><td align="left">484.82</td><td align="left">412.43</td><td align="left">427.52</td><td align="left">559.21</td></tr><tr><td align="left">2048</td><td align="left">913.58</td><td align="left">953.56</td><td align="left">1007.94</td><td align="left">805.35</td><td align="left">960.53</td><td align="left">817.45</td><td align="left">862.94</td><td align="left">1132.56</td></tr></tbody></table><hr><p>让我们看看fuzzy-counter的近似因子，区别不是很大。</p><table><thead><tr><th align="left">threads</th><th align="left">global-counter (expected)</th><th align="left">global-counter (actual)</th></tr></thead><tbody><tr><td align="left">128</td><td align="left">20480000</td><td align="left">20479994</td></tr><tr><td align="left">256</td><td align="left">40960000</td><td align="left">40959987</td></tr><tr><td align="left">512</td><td align="left">81920000</td><td align="left">81919969</td></tr><tr><td align="left">1024</td><td align="left">163840000</td><td align="left">163839945</td></tr><tr><td align="left">2048</td><td align="left">327680000</td><td align="left">327679875</td></tr></tbody></table><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Benchmark测试证明了，对全局计数器使用 CPU/thread 亲和性的性能最好。 当然，x86和 ARM 有不同的优化点，因此可以相应地调整 MySQL。 Fuzzy counter替换为atomic (或shard-atomic)，可以更好的节省空间和提高精度(在 x86平台上)。</p><p><em>如果你有问题 / 疑问，请联系我。</em></p></div><div id="English" class="tab-content"><p>Managing global counters in a multi-threaded system has always been challenging. They pose serious scalability challenges. Introduction of NUMA just increased the complexity. Fortunately multiple options have been discovered with hardware lending support to help solve/ease some of these issues. In this blog we will go over how we can make Global Counter NUMA SMART and also see what performance impact each of this approach has.</p><p>Note: a lot of this work is inspired from MySQL codebase that is continuously trying to evolve to solve this issue.</p><h2 id="Global-Counters"><a href="#Global-Counters" class="headerlink" title="Global Counters"></a>Global Counters</h2><p>Most of the software (for example: database, web-server, etc…) needs to maintain some global counters. Being global, there is one copy of these counters and multiple worker threads try to update it. Of-course this invites a need of coordination while updating these copies and in turn it becomes scalability hotspots.</p><p>Alternative is to loosely maintain these counters (w/o any coordination) but that means they will represent an approximate number (especially on a heavily contended system). But they have their own benefits.</p><p>Let’s see what all approaches the current ecosystem provides to help solve this problem.</p><h3 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h3><p>In order to evaluate different approaches we will consider a common setup.</p><ul><li>Let’s consider some global counters that are at global level so all threads update them once in a while.</li><li>Let’s consider some data-blocks (where the real workload happens) and as part of this action global counters are updated. Each data-block has its own local counter(s). These local counters are updated whenever data-block is accessed. Both of these blocks are interleaved. Check the arrangement below.</li></ul><p><img src="https://mysqlonarm.github.io/images/blog7/numa-counter-mem-allocation.png" alt="img"></p><ul><li>Let’s try to understand this structure with some simple numeric examples.<ul><li>Say we have 100 global counter-blocks and each data-block has 10 counters.</li><li>Say we have 1000 global data-blocks that are equally interleaved with each counter block.</li><li>That means, 1-counter-block is followed by 10-data-blocks and this combination repeats 100 times.</li><li>This ensures complete memory blocks are distributed across NUMA nodes and we get to see the effect of NUMA while accessing the counters and data-blocks too.</li></ul></li><li>Workload (one-round):<ul><li>Flow will access N data-blocks (at-least enough to invalidate L2 cache).</li><li>As part of the data-block access, local counter(s) associated with the data-block are also updated. Data blocks are randomly selected using rand() function to ensure spread-across distribution.</li><li>This is followed with the access and update of global counters from the counter-block. Random counter-block is selected and a random counter from the selected counter block is updated (inc operation). This operation is repeated M times.</li></ul></li><li>Workload loop is repeated K times (rounds).<ul><li>Each thread executes the said workload loop (K times). Benchmarking is done with different scalability starting from 1-256/2048.</li></ul></li></ul><p>Note: Counter is simply uint64_t value (currently using inc operation only).</p><p>If you are interested in understanding more about this you can always check out the detailed code <a href="https://github.com/mysqlonarm/benchmark-suites/tree/master/numa/global-counters">here</a>.</p><h3 id="Hardware-used"><a href="#Hardware-used" class="headerlink" title="Hardware used"></a>Hardware used</h3><p><strong>For scaling from 1-256</strong></p><ul><li>x86-vm: 24 vCPU (12 cores with HT), 48 GB memory, 2 NUMA nodes, Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz (Ubuntu-18.04)</li><li>arm-vm: 24 vCPU (24 cores), 48 GB memory, 2 NUMA nodes, Kunpeng 920 2.60 GHz (Ubuntu-18.04)</li></ul><p><strong>For scaling from 1-2048</strong></p><ul><li>arm-bms: 128 physical cores, 1 TB of memory, 4 NUMA nodes, Kunpeng 920 2.60 GHz (CentOS-7.8)</li></ul><p>Idea is not to compare x86 vs ARM but the idea is to compare the effect of NUMA on the global counter.</p><h2 id="Approches"><a href="#Approches" class="headerlink" title="Approches"></a>Approches</h2><p>As part of the experiment we evaluated different approaches right from basic to advanced.</p><ul><li><strong>pthread-mutex based:</strong> Simple pthread mutexes that protects operation on counter</li></ul><ul><li><strong>std::mutex:</strong> C++11 enabled std::mutexes just like pthread mutexes but more easier to use with inherent C++11 support.</li></ul><ul><li><strong>std::atomic:</strong> C++11 atomic variable.</li></ul><ul><li><strong>fuzzy-counter (from mysql):</strong> There are N cacheline aligned slots. Flow randomly selects one of the slots to update. To find out the total value of the counter, add value from all the slots. There are no mutexes/atomic that protect the slot operations. This means value is approximate but works best when the flow needs likely count. We will see a variance factor below in result section. [ref: ib_counter_t. N is typically = number of cores].</li></ul><ul><li><strong>shard-atomic-counter (from mysql):</strong> Counter is split into N shards (like slot above). Each flow tells which shard to update. Shards are cache lines aligned for better access. [ref: Counter::Shard]</li></ul><ul><li><strong>shard-atomic-counter (thread-id based):</strong> Counter is split into N shards (like slot above). Shard to update is selected based on thread-id of executing thread. Shards are cache lines aligned for better access. [here N is number-of-active-cores]</li></ul><ul><li><strong>shard-atomic-counter (cpu-id based):</strong> Counter is split into N shards. Shard to update is selected based on core-id of executing core. Shards are cache lines aligned for better access. [here N is number-of-active-cores. cpu-id obtained using sched_getcpu].</li></ul><ul><li><strong>shard-atomic-counter (numa-id based):</strong> Counter is split into N shards. Shard to update is selected based on numa-node-id of the executing core. Shards are cache lines aligned for better access. [here N is number-of-active-numa-nodes. N is small here in the range of 2/4/8 not like 32/64/128/etc…]</li></ul><p><em>There is another counter structure inside MySQL that is worth mentioning ut_lock_free_cnt_t(). It tries to allocate memory for each counter (value) on respective NUMA but as per the numa_alloc_onnode even a smaller chunk of 8 bytes will cause allocation of system-page size (for Linux 4KB). That is too much space wastage. I tried this approach but eventually failed to allocate memory due to enormous memory over-head.</em></p><p>Idea is to find out which approach works best in the NUMA environment.</p><h3 id="Benchmarking-1"><a href="#Benchmarking-1" class="headerlink" title="Benchmarking"></a>Benchmarking</h3><p>Benchmarking is done using the structure and workload explained above. Each run allocates memory and then K rounds of workload loop per scalability. Timing below includes time to process data and counter but majority of it is coming from counter contention (confirmed by supressing data-block processing).</p><ul><li><strong>x86-vm [x-axis: threads(1-256), y-axis: time in seconds. Lesser is best]</strong></li></ul><p><em>Data-set:</em> 100 global counter blocks, 10 counters per block, 1m data-blocks (with a local counter per block), 10K rounds</p><p><img src="https://mysqlonarm.github.io/images/blog7/x86.vm.1-256.png" alt="img"></p><h4 id="Comments"><a href="#Comments" class="headerlink" title="Comments"></a>Comments</h4><ul><li>As expected, shard-atomic-counter <strong>with cpu-id</strong> performs best. (cpu-id = sched_getcpu).</li><li>Suprisingly, simple atomic is optimal too with significant space saved. (No cacheline alignment). May be VM effect.</li><li>Another unexplained behavior: fuzzy counter which is expected to be fastest is not showing up to be fastest (I re-confirmed this behavior with 3 different runs. On ARM, it performing as expected so less likely something going wrong in the benchmarking code. More analysis to be done).</li></ul><p><em>Lines are pretty close/overlapping, so sharing the numeric numbers for higher-sclalability.</em></p><table><thead><tr><th align="left">threads</th><th align="left">p-mutex</th><th align="left">std-mutex</th><th align="left">atomic</th><th align="left">fuzzy</th><th align="left">shard-rand</th><th align="left">shard-tid</th><th align="left">shard-cpuid</th><th align="left">shard-numaid</th></tr></thead><tbody><tr><td align="left">128</td><td align="left">305.89</td><td align="left">312.78</td><td align="left">275.21</td><td align="left">306.62</td><td align="left">273.52</td><td align="left">278.14</td><td align="left">263.5</td><td align="left">352.45</td></tr><tr><td align="left">256</td><td align="left">608.21</td><td align="left">625.37</td><td align="left">549.15</td><td align="left">611.97</td><td align="left">546.04</td><td align="left">560.18</td><td align="left">521.25</td><td align="left">705.17</td></tr></tbody></table><hr><ul><li><strong>arm-vm [x-axis: threads(1-256), y-axis: time in seconds. Lesser is best]</strong></li></ul><p><em>Data-set:</em> 100 global counter blocks, 10 counters per block, 1m data-blocks (with a local counter per block), 10K rounds</p><p><img src="https://mysqlonarm.github.io/images/blog7/arm.vm.1-256.png" alt="img"></p><h4 id="Comments-1"><a href="#Comments-1" class="headerlink" title="Comments"></a>Comments</h4><ul><li>Again, shard-atomic-counter (this time <strong>with thread-id</strong>) scored better than other alternatives. (one of the reason could be sched_getcpu is costly on ARM). [For thread-id, flow cached thread unique identifier during creation, in thread-local storage].</li><li>FuzzyCounter is helping establish baseline (given there is no-contention).</li><li>Good old pthread-mutex seems to be optimized too.</li><li>Intererstingly, ARM seems to be showing lower contention with increase scalability (may be due to better NUMA interconnect).</li></ul><p><em>Lines are pretty close and in some cases overlapping too, so sharing the numeric numbers for higher-sclalability.</em></p><table><thead><tr><th align="left">threads</th><th align="left">p-mutex</th><th align="left">std-mutex</th><th align="left">atomic</th><th align="left">fuzzy</th><th align="left">shard-rand</th><th align="left">shard-tid</th><th align="left">shard-cpuid</th><th align="left">shard-numaid</th></tr></thead><tbody><tr><td align="left">128</td><td align="left">265.05</td><td align="left">271.53</td><td align="left">272.06</td><td align="left">241.26</td><td align="left">287.1</td><td align="left">258.9</td><td align="left">337.2</td><td align="left">396.88</td></tr><tr><td align="left">256</td><td align="left">529.74</td><td align="left">546.74</td><td align="left">544.07</td><td align="left">481.71</td><td align="left">574.05</td><td align="left">520</td><td align="left">671.63</td><td align="left">795.92</td></tr></tbody></table><hr><ul><li><strong>arm-bms [x-axis: threads(1-2048), y-axis: time in seconds. Lesser is best]</strong></li></ul><p><em>Data-set:</em> 100 global counter blocks, 10 counters per block, 1m data-blocks (with a local counter per block), 1K rounds</p><p><img src="https://mysqlonarm.github.io/images/blog7/arm.bms.1-2048.png" alt="img"></p><h4 id="Comments-2"><a href="#Comments-2" class="headerlink" title="Comments"></a>Comments</h4><ul><li>Fuzzy-Counter help set the baseline but this time we see shard-atomic-counter (<strong>with thread-id</strong>) is almost on-par with Fuzzy-Counter (non-contention use-case). That is like optimal number to expect.</li></ul><p><em>Lines are pretty close and in some cases overlapping too, so sharing the numeric numbers for higher-sclalability. Just incase you have not noticed the rounds has been reduced by 1K. Keeping it 10K increases timing like anything due to cross-numa access and increased scalability. (note: we are now on operating machine with 4 numa nodes).</em></p><table><thead><tr><th align="left">threads</th><th align="left">p-mutex</th><th align="left">std-mutex</th><th align="left">atomic</th><th align="left">fuzzy</th><th align="left">shard-rand</th><th align="left">shard-tid</th><th align="left">shard-cpuid</th><th align="left">shard-numaid</th></tr></thead><tbody><tr><td align="left">128</td><td align="left">62.81</td><td align="left">63.9</td><td align="left">66.24</td><td align="left">57.37</td><td align="left">64.24</td><td align="left">54.09</td><td align="left">57.67</td><td align="left">72.08</td></tr><tr><td align="left">256</td><td align="left">115.39</td><td align="left">119.53</td><td align="left">126.52</td><td align="left">102.68</td><td align="left">119.01</td><td align="left">102.13</td><td align="left">106.3</td><td align="left">140.83</td></tr><tr><td align="left">512</td><td align="left">228.2</td><td align="left">234.5</td><td align="left">252</td><td align="left">199.71</td><td align="left">241.69</td><td align="left">205.66</td><td align="left">211.29</td><td align="left">279.81</td></tr><tr><td align="left">1024</td><td align="left">456.53</td><td align="left">470.55</td><td align="left">503.73</td><td align="left">398.61</td><td align="left">484.82</td><td align="left">412.43</td><td align="left">427.52</td><td align="left">559.21</td></tr><tr><td align="left">2048</td><td align="left">913.58</td><td align="left">953.56</td><td align="left">1007.94</td><td align="left">805.35</td><td align="left">960.53</td><td align="left">817.45</td><td align="left">862.94</td><td align="left">1132.56</td></tr></tbody></table><hr><p>Let’s see approximation factor for fuzzy-counter. Not that major difference.</p><table><thead><tr><th align="left">threads</th><th align="left">global-counter (expected)</th><th align="left">global-counter (actual)</th></tr></thead><tbody><tr><td align="left">128</td><td align="left">20480000</td><td align="left">20479994</td></tr><tr><td align="left">256</td><td align="left">40960000</td><td align="left">40959987</td></tr><tr><td align="left">512</td><td align="left">81920000</td><td align="left">81919969</td></tr><tr><td align="left">1024</td><td align="left">163840000</td><td align="left">163839945</td></tr><tr><td align="left">2048</td><td align="left">327680000</td><td align="left">327679875</td></tr></tbody></table><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Benchmark study has proved that using CPU/thread affinity for global counters works best. Of-course x86 and ARM has different optimization point so MySQL could be tuned accordingly. Fuzzy counter could be better replaced with atomic (or shard-atomic) given space saved and improved accurancy (on x86).</p><p><em>If you have more questions/queries do let me know. Will try to answer them.</em></p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Krunal Bauskar&lt;br&gt;原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/NUMA-Smart-Global-Counter/&quot;&gt;https://mysqlonarm.github.io/NUMA-Smart-Global-Counter/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;通过之前在X86, ARM虚机上的调研，与遇到的跨NUMA问题，结合自身在运行benchmark测试中的经验，让应用程序在针对特定的全局数据结构中更好的应用底层硬件，达到极致性能体验。下面来用数据说话。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>PostgreSQL ARM和X86性能比拼</title>
    <link href="https://kunpengcompute.github.io/2020/05/22/postgresql-arm-he-x86-xing-neng-bi-pin/"/>
    <id>https://kunpengcompute.github.io/2020/05/22/postgresql-arm-he-x86-xing-neng-bi-pin/</id>
    <published>2020-05-22T09:03:28.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Amit Dattatray Khandekar<br>原文链接: <a href="https://amitdkhan-pg.blogspot.com/2020/05/postgresql-on-arm.html">https://amitdkhan-pg.blogspot.com/2020/05/postgresql-on-arm.html</a></p><p>由团队内部PostgreSQL大牛Amit在ARM和X86上针对PostgreSQL的性能比拼测试。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><h3 id="PostgreSQL-on-ARM"><a href="#PostgreSQL-on-ARM" class="headerlink" title="PostgreSQL on ARM"></a>PostgreSQL on ARM</h3><h2 id=""><a href="#" class="headerlink" title=""></a></h2><p>在我的<a href="https://amitdkhan-pg.blogspot.com/2020/04/arm-points-to-be-noted.html">上个博客</a>, 我写道，如果那些已经运行在X86的应用程序要在不同的架构上运行，比如 ARM，那么这些应用程序可能需要进行一些优化。 让我们来看看它具体指的是什么。</p><p>最近我一直在使用 ARM64机器测试 PostgreSQL RDBMS。几个月前，我甚至不知道它是否可以在 ARM 上编译，因为忽略了一个事实，即我们有一个用于 ARM64的常规构建机器，<a href="https://buildfarm.postgresql.org/cgi-bin/show_history.pl?nm=eelpout&br=HEAD">已经很长时间了</a>. 现在甚至连 PostgreSQL apt 库也开始制作<a href="https://www.df7cb.de/blog/2020/arm64-on-apt.postgresql.org.html">ARM64 PostgreSQL</a> 的包了。但是在我用不同的场景测试了 PostgreSQL-on-ARM 之后，我才真正对它的可靠性有了信心。</p><p>我从read-only pgbench 测试开始，比较了 x86_64和 ARM_64虚机的测试结果。 目的不是比较任何特定的 CPU 实现。 这个想法是为了找出ARM 上的 PostgreSQL 与X86相比表现不尽如人意的场景。</p><p><strong>Test Configuration</strong></p><p>ARM64 VM:<br>  Ubuntu 18.04.3; 8 CPUs; CPU frequency: 2.6 GHz; available RAM : 11GB<br>x86_64 VM:<br>  Ubuntu 18.04.3; 8 CPUs; CPU frequency: 3.0 GHz; available RAM : 11GB</p><p>以下是所有测试的共用的配置:</p><p>PostgreSQL parameters changed : shared_buffers = 8GB<br>pgbench scale factor : 30<br>pgbench command :<br>for num in 2 4 6 8 10 12 14<br>do<br>  pgbench [-S] -c $num -j $num -M prepared -T 40<br>done<br>它的意思是: pgbench 运行的并行client数量越来越多，从2个到14个不等。</p><p><strong>Select-only workload</strong></p><p>pgbench -S 选项应用在 read-only workload.</p><p><a href="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-only.jpeg"><img src="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-only.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="img"></a></p><p> 在2个线程和4个线程之间，x86的性能比 ARM 高出30% ，而且差距越来越大。 当线程数在4到6之间，曲线变得平坦了一点，再到线程数6到8之间，曲线突然变得陡峭。 然后线程数到达8之后，由于测试机上有8个 cpu，预计它会变平或下降。 但是还有更多的原因。 这里Pgbench client运行在与安装 PostgreSQL server相同的一台机器上。 通过充分利用 cpu，Pgbench client 占用了大约20% 的 cpu。 所以client从6个线程开始对测试产生干扰。 尽管如此，ARM 和 x86的性能差距还是在线程数6到8之间急剧上涨。 我还没有理解为什么会这样，可能与 Linux 调度程序以及 pgbench client和PostgreSQL server之间的交互有关。 注意，x86和 ARM 的曲线形状基本相似。 所以这种行为并不是架构特有的。 不过，这些曲线的一个不同之处在于: ARM 曲线从8个线程开始下降幅度稍大一些。 此外，线程数在6到8之间时，ARM 的处理事务量的增长并不像 x86那样剧烈。 因此，这种情况的最终结果是: 随着 cpu 变得越来越忙，ARM 上的 PostgreSQL 越来越落后于 x86。 让我们看看如果移除 pgbench client带来的干扰会发生什么。</p><p><strong>select exec_query_in_loop(n)</strong></p><p>因此，为了避免由于同一台机器上的pgbench client对PostgreSQL server造成的干扰，我想测试一下它的查询性能。为此，pgbench client运行在另一台机器上，但这可能会产生另一种的噪音: 网络延迟。 所以，我写了一个 PostgreSQL <a href="https://drive.google.com/file/d/1HcnHV5u0unyuH3ve-Jnp8XLqLicOI_PC/view?usp=sharing">C language user-defined function</a> ，用来循环执行与 pgbench 测试运行的完全相同的 SQL 查询。 使用 pgbench 自定义脚本执行此函数。现在，pgbench client大部分都是空闲的。 另外，这不会占用提交 / 回滚程度时间，因为大部分时间将花费在这个C 函数上。</p><p>pgbench 自定义脚本 : select exec_query_in_loop(n);<br>其中 n 是 pgbench 查询在PostgreSQL server上一次循环执行的次数<br>与pgbench -S选项作用下的普通循环查询：<br>SELECT abalance FROM pgbench_accounts WHERE aid = $1<br>详情参看 <a href="https://drive.google.com/file/d/1HcnHV5u0unyuH3ve-Jnp8XLqLicOI_PC/view?usp=sharing">exec_query_in_loop</a>()</p><p><a href="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-exec_query_in_loop.jpeg"><img src="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-exec_query_in_loop.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/620" alt="img"></a></p><p>现在，你看到一个非常不同的曲线。 对于这两条曲线，最多为8个线程，事务率与线程数成线性比例。 正如预期的那样，在线程数达到8以后，处理的事务就不会上升了。 而且，即使对 ARM 来说，它也有相同的行为。 与 x86相比，PostgreSQL 在 ARM上从头到尾慢了35% 左右。 考虑到 ARM 处理器的频率是2.6 GHz，而 x86是3.0 GHz，这么一说看起来这性能还不错。 注意，事务率是个位数，因为函数 exec_query_in_loop(n)中是用 n=100000来执行的。</p><p>这个实验还表明，之前使用内置 pgbench 脚本的性能测试结果与 pgbench client干扰有关。 而且，ARM 对于竞争线程的倾斜曲线不是由服务器中的争用引起的。 请注意，事务率是在客户端计算的。 因此，特别是在高争用场景中, 即使查询中的结果已经准备就绪，然而client请求结果、计算时间戳等仍然可能会有一些延迟。</p><p><strong>select exec_query_in_loop(n) - PL/pgSQL function</strong></p><p>在使用用户定义的 c 函数之前，我使用了 <a href="https://drive.google.com/file/d/1-jiYEOIVHdtp8Yfv6QrYhrO06urHILd5/view?usp=sharing">PL/pgSQL function来做同样的事</a>. 我偶然发现了一种不同的表现行为。</p><p><a href="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-exec_query_in_loop2.jpeg"><img src="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-exec_query_in_loop2.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/620" alt="img"></a></p><p>在这里，无论线程数量如何，ARM 上的 PostgreSQL 都比 x86慢65% 左右。 与之前使用 C 函数的结果相比，由于某种原因，很明显 PL/pgsql 在 ARM 上的执行速度非常慢。 我检查了 perf 输出的报告，在 ARM 和 x86中看到的热点函数大致相同。 但由于某些原因，在 PL/pgsql 函数内执行的任何操作在 ARM 上都比在 x86上慢得多。</p><p>我还没有检查缓存失败，看看缓存失败是否在 ARM 上会更多。 在撰写本文时，我所做的是这样的(在PostgreSQL内部是这样的) : exec_stmt_foreach_a()调用exec_stmt()。 我将 exec_stmt()克隆为 exec_stmt_clone() ，并将 exec_stmt_foreach_a()调用 exec_stmt_clone()。 这加快了整体执行的速度，对于 ARM 来说却加快了20%多 。 到目前为止，这种变化为什么会导致这种行为，对我来说还是一个谜。 可能与程序中某个代码位置有关，这点我还不确定。</p><p><strong>Updates</strong></p><p>默认 pgbench 选项运行与 tpcb类似的内置脚本，该脚本对多个表进行了一些更新操作。</p><p><a href="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/tpcb-like-updates.jpeg"><img src="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/tpcb-like-updates.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/620" alt="img"></a></p><p>与 x86相比，ARM 上的事务处理率大约比X86慢1%-10% 。 这可能是因为大部分时间用于等待锁，而在提交过程中的磁盘写操作。 我使用的磁盘是非 SSD 磁盘。 但总体来看，PostgreSQL在 ARM 上的更新表操作在 ARM 上运行良好。</p><p>接下来，我将测试聚合查询、分页、更多CPU核数 (32 / 64 / 128)、更大的 RAM 和更高的规模因数，以便相对地了解 PostgreSQL 在拥有大量资源的两个平台上的性能扩展情况。</p><p><strong>结论</strong></p><p>我们看到，PostgreSQL RDBMS 在 ARM64上工作得相当稳定。 虽然在比较两个不同平台上的性能很棘手，但是我们仍然可以通过比较两个平台中不同场景中的行为来判断它哪些方面做得不好。</p></div><div id="English" class="tab-content"><h3 id="PostgreSQL-on-ARM-1"><a href="#PostgreSQL-on-ARM-1" class="headerlink" title="PostgreSQL on ARM"></a>PostgreSQL on ARM</h3><h2 id="-1"><a href="#-1" class="headerlink" title=""></a></h2><p>In my <a href="https://amitdkhan-pg.blogspot.com/2020/04/arm-points-to-be-noted.html">last blog</a>, I wrote that applications that have been running on x86 might need to undergo some adaptation if they are to be run on a different architecture such as ARM. Let’s see what it means exactly.</p><p>Recently I have been playing around with PostgreSQL RDBMS using an ARM64 machine. A few months back, I even didn’t know whether it can be compiled on ARM, being oblivious of the fact that we already have a regular build farm member for ARM64 for <a href="https://buildfarm.postgresql.org/cgi-bin/show_history.pl?nm=eelpout&br=HEAD">quite a while</a>. And now even the PostgreSQL apt repository has started making PostgreSQL packages <a href="https://www.df7cb.de/blog/2020/arm64-on-apt.postgresql.org.html">available for ARM64</a> architecture. But the real confidence on the reliability of PostgreSQL-on-ARM came after I tested it with different kinds of scenarios.</p><p>I started with read-only pgbench tests and compared the results on the x86_64 and the ARM64 VMs available to me. The aim was not to compare any specific CPU implementation. The idea was to find out scenarios where PostgreSQL on ARM does not perform in one scenario as good as it performs in other scenarios, when compared to x86.</p><p><strong>Test Configuration</strong></p><p>ARM64 VM:<br>  Ubuntu 18.04.3; 8 CPUs; CPU frequency: 2.6 GHz; available RAM : 11GB<br>x86_64 VM:<br>  Ubuntu 18.04.3; 8 CPUs; CPU frequency: 3.0 GHz; available RAM : 11GB</p><p>Following was common for all tests :</p><p>PostgreSQL parameters changed : shared_buffers = 8GB<br>pgbench scale factor : 30<br>pgbench command :<br>for num in 2 4 6 8 10 12 14<br>do<br>  pgbench [-S] -c $num -j $num -M prepared -T 40<br>done<br>What it means is : pgbench is run with increasing number of parallel clients, starting from 2 to 14.</p><p><strong>Select-only workload</strong></p><p>pgbench -S option is used for read-only workload.</p><p><a href="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-only.jpeg"><img src="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-only.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="img"></a></p><p>Between 2 and 4 threads, the x86 performance is 30% more than ARM, and the difference rises more and more. Between 4 and 6, the curves flatten a bit, and between 6 and 8, the curves suddenly become steep. After 8, it was expected to flatten out or dip, because the machines had 8 CPUs. But there is more to it. The pgbench clients were running on the same machines where servers were installed. And with fully utilized CPUs, the clients took around 20% of the CPUs. So they start to interfere from 6 threads onward. In spite of that, there is a steep rise between 6 and 8, for both ARM and x86. This is not yet understood by me, but possibly it has something to do with the Linux scheduler, and the interaction between the pgbench clients and the servers. Note that, the curve shape is mostly similar on both x86 and ARM. So this behaviour is not specific to architectures. One difference in the curves, though, is : the ARM curve has a bit bigger dip from 8 threads onward. Also, betweeen 6 and 8, the sudden jump in transactions is not that steep for ARM compared to x86. So the end result in this scenario is : As the CPUs become more and more busy, PostgreSQL on ARM lags behind x86 more and more. Let’s see what happens if we remove the interference created by pgbench clients.</p><p><strong>select exec_query_in_loop(n)</strong></p><p>So, to get rid of the noise occurring because of both client and server on the same machines, I arranged for testing exactly what I intended to test: query performance. For this, pgbench clients can run on different machines, but that might create a different noise: network latency. So instead, I wrote a PostgreSQL <a href="https://drive.google.com/file/d/1HcnHV5u0unyuH3ve-Jnp8XLqLicOI_PC/view?usp=sharing">C language user-defined function</a> that keeps on executing in a loop the same exact SQL query that is run by this pgbench test. Execute this function using the pgbench custom script. Now, pgbench clients would be mostly idle. Also, this won’t take into account the commit/rollback time, because most of the time will be spent inside the C function.</p><p>pgbench custom script : select exec_query_in_loop(n);<br>where n is the number of times the pgbench query will be executed on the server in a loop.<br>The loop query is the query that gets normally executed with pgbench -S option:<br>SELECT abalance FROM pgbench_accounts WHERE aid = $1<br>Check details in <a href="https://drive.google.com/file/d/1HcnHV5u0unyuH3ve-Jnp8XLqLicOI_PC/view?usp=sharing">exec_query_in_loop</a>()</p><p><a href="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-exec_query_in_loop.jpeg"><img src="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-exec_query_in_loop.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/620" alt="img"></a></p><p>Now, you see a very different curve. For both curves, upto 8 threads, transactions rate is linearly proportional to number of threads. After 8, as expected, the transactions rate doesn’t rise. And it has not dipped, even for ARM. PostgreSQL is consistently around 35% slower on x86 compared to ARM. This sounds not that bad when we consider that the ARM CPU frequency is 2.6 GHz whereas x86 is 3.0 Gz. Note that the transaction rate is single digit, because the function exec_query_in_loop(n) is executed with n=100000.</p><p>This experiment also shows that the previous results using built-in pgbench script have to do with pgbench client interference. And that, the dip in curve for ARM for contended threads is not caused by the contention in the server. Note that, the transactions rates are calculated at client side. So even when a query is ready for the results, there may be some delay in the client requesting the results , calculating the timestamp, etc, especially in high contention scenarios.</p><p><strong>select exec_query_in_loop(n) - PLpgSQL function</strong></p><p>Before using the user-defined C function, I had earlier used a <a href="https://drive.google.com/file/d/1-jiYEOIVHdtp8Yfv6QrYhrO06urHILd5/view?usp=sharing">PL/pgSQL function to do the same work</a>. There, I stumbled across a different kind of performance behaviour.</p><p><a href="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-exec_query_in_loop2.jpeg"><img src="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/select-exec_query_in_loop2.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/620" alt="img"></a></p><p>Here, PostgreSQL on ARM is around 65% slower than on x86, regardless of number of threads. Comparing with the previous results that used a C function, it is clear that PL/pgSQL execution is remarkably slower on ARM, for some reason. I checked the perf report, but more or less the same hotspot functions are seen in both ARM and x86. But for some reason, anything executed inside PL/pgSQL function becomes much slower on ARM than on x86.</p><p>I am yet to check the cache misses to see if those are more on ARM. As of this writing, what I did was this (some PostgreSQL-internals here) : exec_stmt_foreach_a() calls exec_stmt(). I cloned exec_stmt() to exec_stmt_clone(), and made exec_stmt_foreach_a() call exec_stmt_clone() instead. This sped up the overall execution, but it sped up 20% more for ARM. Why just this change caused this behaviour is kind of a mystery to me as of now. May be it has to do with the location of a function in the program; not sure.</p><p><strong>Updates</strong></p><p>The default pgbench option runs the tpcb-like built-in script, which has some updates on multiple tables.</p><p><a href="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/tpcb-like-updates.jpeg"><img src="https://raw.githubusercontent.com/bzhaoopenstack/dockertoy/master/images/tpcb-like-updates.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/620" alt="img"></a></p><p>Here, the transaction rate is only around 1-10% percent less on ARM compared to x86. This is probably because major portion of the time goes in waiting for locks, and in disk writes during commits. And the disks I used are non-SSD disks. But overall it looks like, updates on PostgreSQL are working good on ARM.</p><p>Next thing, I am going to test with aggregate queries, partitions, high number of CPUs (32/64/128), larger RAM and higher scale factor, to relatively see how PostgreSQL scales on the two platforms with large resources.</p><p><strong>Conclusion</strong></p><p>We saw that PostgreSQL RDBMS works quite robustly on ARM64. While it is tricky to compare the performance on two different platforms, we could still identify which areas it is not doing good by comparing patterns of behaviour in different scenarios in the two platforms.</p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Amit Dattatray Khandekar&lt;br&gt;原文链接: &lt;a href=&quot;https://amitdkhan-pg.blogspot.com/2020/05/postgresql-on-arm.html&quot;&gt;https://amitdkhan-pg.blogspot.com/2020/05/postgresql-on-arm.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;由团队内部PostgreSQL大牛Amit在ARM和X86上针对PostgreSQL的性能比拼测试。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>让压缩库ZSTD在aarch64更顺滑</title>
    <link href="https://kunpengcompute.github.io/2020/05/20/rang-ya-suo-ku-zstd-zai-aarch64-geng-shun-hua/"/>
    <id>https://kunpengcompute.github.io/2020/05/20/rang-ya-suo-ku-zstd-zai-aarch64-geng-shun-hua/</id>
    <published>2020-05-20T08:46:05.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>作者: 姜逸坤 曹亚珍</p><p>Facebook的ZSTD压缩库从1.0版本发布的那天起，就引起了业界的关注，对比业界常用的压缩库lz4、zilib、xz，ZSTD更注重速度和压缩比的均衡，对比zlib来看，更是在保证压缩比的情况下，较zlib压缩性能提升6倍左右，解压性能提升2倍左右。</p><p>我们团队也在2020年年初时，对ZSTD压缩库进行了性能优化，最终优化已推入到Facebook的上游社区中，本文将详细的介绍我们进行的优化。</p><a id="more"></a><h2 id="1-利用neon指令集对数据复制优化。"><a href="#1-利用neon指令集对数据复制优化。" class="headerlink" title="1. 利用neon指令集对数据复制优化。"></a>1. 利用neon指令集对数据复制优化。</h2><p>完整的Patch链接：<a href="https://github.com/facebook/zstd/pull/2041">facebook/zstd#2041</a></p><h3 id="优化思路："><a href="#优化思路：" class="headerlink" title="优化思路："></a>优化思路：</h3><p>aarch64提供了一系列的neon指令，本次优化则利用了VLD和VST指令，借助neon寄存器进行读写加速，<a href="http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0489c/CJAJIIGG.html">ARM的官方文档</a>是这样描述这两个指令的：</p><blockquote><p>VLDn and VSTn (single n-element structure to one lane)</p><ul><li>Vector Load single n-element structure to one lane. It loads one n-element structure from memory into one or more NEON registers. Elements of the register that are not loaded are unaltered.</li><li>Vector Store single n-element structure to one lane. It stores one n-element structure into memory from one or more NEON registers.</li></ul></blockquote><p>来自ARM的官方文档<a href="https://community.arm.com/developer/ip-products/processors/b/processors-ip-blog/posts/coding-for-neon---part-1-load-and-stores">Coding for Neon - Part 1: Load and Stores</a>中，写的非常详细，引用一张图来描述neon寄存器和memory加载和存储的方式，核心思想就是：<strong>利用neon寄存器作为暂存的中转站，加速数据处理</strong>：<br><img src="https://user-images.githubusercontent.com/1736354/82516998-bc78a900-9b4e-11ea-8183-1200678566e8.png" alt="image"></p><p>我们以u8的复制为例，总结下本次我们在ZSTD具体的优化实现：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">ZSTD_copy8</span><span class="params">(<span class="keyword">void</span>* dst, <span class="keyword">const</span> <span class="keyword">void</span>* src)</span> </span>&#123;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> __aarch64__</span></span><br><span class="line">    vst1_u8((<span class="keyword">uint8_t</span>*)dst, vld1_u8((<span class="keyword">const</span> <span class="keyword">uint8_t</span>*)src));</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    <span class="built_in">memcpy</span>(dst, src, <span class="number">8</span>);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心步骤包含两步：</p><ol><li>将src利用vld1加载到neon寄存器。</li><li>使用vst1将neon寄存器的值store到dst的memory中。</li></ol><p>这样便利用neon完成了对u8的memcpy的优化，对于此类优化，有兴趣的可以阅读<a href="http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.faqs/ka13544.html">What is the fastest way to copy memory on a Cortex-A8?</a>，了解在Cortext-A8的架构下，如何快速的进行memory copy。</p><h3 id="性能测试："><a href="#性能测试：" class="headerlink" title="性能测试："></a>性能测试：</h3><p>完成neon优化后，我们对压缩和解压缩都进行了测试，最终，在压缩场景获得了大概1+%的提升：</p><table><thead><tr><th>Average gains(level 1~19)</th><th>gcc9.2.0</th><th>clang9.0.0</th></tr></thead><tbody><tr><td>Compression</td><td>1.67%</td><td>1.23%</td></tr><tr><td>Decompression</td><td>0.02%</td><td>0.36%</td></tr></tbody></table><h2 id="2-使用prefetch机制加速数据读取。"><a href="#2-使用prefetch机制加速数据读取。" class="headerlink" title="2. 使用prefetch机制加速数据读取。"></a>2. 使用prefetch机制加速数据读取。</h2><p>完整的Patch链接：<a href="https://github.com/facebook/zstd/pull/2040">facebook/zstd#2040</a></p><h3 id="优化思路：-1"><a href="#优化思路：-1" class="headerlink" title="优化思路："></a>优化思路：</h3><p>Prefetch的中文是预取，原理是通过将数据预取到cache中，加速数据的访问。一个比较常见的场景就是在循环中，我们可以通过显示的调用，充分的预取未来将会访问的数据或指令便能快速从Cache中加载到处理器内部进行运算或者执行。</p><p>在Jeff Dean的一次经典的talk–<a href="http://static.googleusercontent.com/media/research.google.com/en//people/jeff/stanford-295-talk.pdf">Software Engineering Advice from<br>Building Large-Scale Distributed Systems</a>中，提到了cache和memory的速度差异，大致如下图所示：<br><img src="https://user-images.githubusercontent.com/1736354/82518863-dddb9400-9b52-11ea-937c-55766b53f3a1.png" alt="image"></p><p>可以看到，从cache中拿数据，将比直接从memory拿数据性能提升几十甚至上百倍，因此，我们也在本次的优化中，为aarch64加入的<a href="http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0802b/PRFM_imm.html">预取指令</a>。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PREFETCH_L1(ptr)  __asm__ __volatile__(<span class="meta-string">"prfm pldl1keep, %0"</span> ::<span class="meta-string">"Q"</span>(*(ptr)))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PREFETCH_L2(ptr)  __asm__ __volatile__(<span class="meta-string">"prfm pldl2keep, %0"</span> ::<span class="meta-string">"Q"</span>(*(ptr)))</span></span><br></pre></td></tr></table></figure><p>同时，将预取加速加入到了ZSTD_compressBlock_fast_generic和ZSTD_compressBlock_doubleFast_generic的主循环中，在数据访问前，预先先将数据加载到cache中，从而加速后续访问对数据读取。</p><h3 id="性能测试：-1"><a href="#性能测试：-1" class="headerlink" title="性能测试："></a>性能测试：</h3><p>我们仅对压缩进行了优化，因此，也仅对压缩进行了测试，测试结果可以看出，速度在aarch64架构下获得了1.5-3+%的提升：</p><table><thead><tr><th>Average gains(level 1~19)</th><th>gcc9.2.0</th><th>clang9.0.0</th></tr></thead><tbody><tr><td>level 1~2</td><td>3.10%</td><td>3.69%</td></tr><tr><td>level 3~4</td><td>2.49%</td><td>1.51%</td></tr></tbody></table><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h2><p>在Facebook的ZSTD中，我们使用了neon指令集对memcpy的过程进行了加速，同时，也利用了prefetch机制，加速了循环时数据的访问。</p><p>希望本篇文章，能够对大家带来一些性能优化的启发。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: 姜逸坤 曹亚珍&lt;/p&gt;
&lt;p&gt;Facebook的ZSTD压缩库从1.0版本发布的那天起，就引起了业界的关注，对比业界常用的压缩库lz4、zilib、xz，ZSTD更注重速度和压缩比的均衡，对比zlib来看，更是在保证压缩比的情况下，较zlib压缩性能提升6倍左右，解压性能提升2倍左右。&lt;/p&gt;
&lt;p&gt;我们团队也在2020年年初时，对ZSTD压缩库进行了性能优化，最终优化已推入到Facebook的上游社区中，本文将详细的介绍我们进行的优化。&lt;/p&gt;
    
    </summary>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/categories/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/tags/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Memcached  x86_64 VS arm64 性能对比</title>
    <link href="https://kunpengcompute.github.io/2020/05/20/memcached-x86-64-vs-arm64-xing-neng-dui-bi/"/>
    <id>https://kunpengcompute.github.io/2020/05/20/memcached-x86-64-vs-arm64-xing-neng-dui-bi/</id>
    <published>2020-05-20T03:26:40.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>译者: <a href="https://github.com/wangxiyuan">wangxiyuan</a><br>作者: <a href="https://github.com/martin-g">Martin Grigorov</a><br>原文链接:  <a href="https://medium.com/@martin.grigorov/compare-memcached-performance-on-x86-64-and-arm64-cpu-architectures-7fe781e34ab8">https://medium.com/@martin.grigorov/compare-memcached-performance-on-x86-64-and-arm64-cpu-architectures-7fe781e34ab8</a></p><p>Tomcat PMC Martin Grigorov带来的另一篇ARM64 VS X86性能对比文章。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>上周，我<a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">分享</a>了在 x86_64和 ARM64 CPU 架构上测试 Apache Tomcat 的结果。 在这篇文章中，我将测试 <a href="https://memcached.org/">Memcached</a>。</p><p>什么是 Memcached？</p><p>摘自 <a href="https://en.wikipedia.org/wiki/Memcached">Wikipedia</a>: Memcached 是一个通用的分布式<a href="https://en.wikipedia.org/wiki/Memory_caching">内存缓存</a>系统。 它通常用于加速动态数据库驱动的网站，方法是在 <a href="https://en.wikipedia.org/wiki/Random-access_memory">RAM</a> 中缓存数据和<a href="https://en.wikipedia.org/wiki/Object_(computer_science)">对象</a>，以减少必须读取外部数据源(如数据库或 API)的次数。</p><p>与 Apache Tomcat 不同的是，Apache Tomcat 是用 Java 编写的，因此是多平台通用的。 而Memcached 是用 c 编写的，需要为不同的 CPU 体系构建它。 正如在其硬件 Wiki <a href="https://github.com/memcached/memcached/wiki/Hardware">页面</a> ARM64中所说的那样，它是官方支持的体系结构之一，并且有一个 BuildBot <a href="https://build.memcached.org/#/builders/6">构建器</a>来测试所有的代码更改！ 如果您遇到任何问题，只要在项目的<a href="https://github.com/memcached/memcached/issues">问题跟踪工具</a>中报告它！ 项目的维护者 Dormando 会非常友好和积极响应！</p><p>在我第一次尝试为 Memcached 找到一个好的负载测试工具时，我无意中发现了 <a href="https://github.com/RedisLabs/memtier_benchmark">RedisLabs Memtier Benchmark</a> 工具。 在 Apache Tomcat 的<a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">文章</a>中提到的同一个 vm 上运行它，结果如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ASCII protocol on ARM64</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Type         Ops&#x2F;sec     Hits&#x2F;sec   Misses&#x2F;sec      Latency       KB&#x2F;sec </span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">Sets          985.28          ---          ---     20.02700        67.22 </span><br><span class="line">Gets         9842.00         0.00      9842.00     20.01900       248.83 </span><br><span class="line">Waits           0.00          ---          ---      0.00000          --- </span><br><span class="line">Totals      10827.28         0.00      9842.00     20.02000       316.05</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ASCII protocol on x86_64</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Type         Ops&#x2F;sec     Hits&#x2F;sec   Misses&#x2F;sec      Latency       KB&#x2F;sec </span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">Sets          931.04          ---          ---     20.06800        63.52 </span><br><span class="line">Gets         9300.21         0.00      9300.21     20.32600       235.13 </span><br><span class="line">Waits           0.00          ---          ---      0.00000          --- </span><br><span class="line">Totals      10231.26         0.00      9300.21     20.30200       298.66</span><br></pre></td></tr></table></figure><p>上面我们可以看到，Memcached 服务运行在 ARM64虚拟机会稍微快一点！</p><p><strong>注意</strong>: Memcached 服务器运行的默认设置(最大1024连接，4线程和64M 内存) ，即没有指定自定义值。</p><p>对于二进制协议，数字几乎是一样的:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Binary protocol on ARM64</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Type         Ops&#x2F;sec     Hits&#x2F;sec   Misses&#x2F;sec      Latency       KB&#x2F;sec </span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">Sets          829.68          ---          ---     23.46500        63.90 </span><br><span class="line">Gets         8287.69         0.00      8287.69     23.56100       314.75 </span><br><span class="line">Waits           0.00          ---          ---      0.00000          --- </span><br><span class="line">Totals       9117.37         0.00      8287.69     23.55200       378.65</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Binary protocol on x86_64</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Type         Ops&#x2F;sec     Hits&#x2F;sec   Misses&#x2F;sec      Latency       KB&#x2F;sec </span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">Sets          829.32          ---          ---     23.63600        63.87 </span><br><span class="line">Gets         8284.10         0.00      8284.10     23.58600       314.61 </span><br><span class="line">Waits           0.00          ---          ---      0.00000          --- </span><br><span class="line">Totals       9113.42         0.00      8284.10     23.59100       378.48</span><br></pre></td></tr></table></figure><p>在我与 Memcached 社区<a href="https://groups.google.com/d/msg/memcached/8hT2BT9cgEc/Ldm8Q42xAgAJ">分享</a>了这些结果之后，社区建议我使用 <a href="https://github.com/memcached/mc-crusher">MC Crusher</a> 工具代替。 实际上，结果比之相关的数据要好得多:</p><p><code>ASCII protocol GET operations per second</code></p><p><img src="https://user-images.githubusercontent.com/10891919/82401295-ffbe1380-9a8b-11ea-928f-aa62068b3967.png" alt="image"></p><p><code>ASCII protocol SET operations per second</code></p><p><img src="https://user-images.githubusercontent.com/10891919/82401314-0d739900-9a8c-11ea-9f96-ccb54c081475.png" alt="image"></p><p>在第一个图表中，你可以看到在 x86_64和 ARM64上，每秒大约有150万次 get 操作！</p><p>在第二张图表中，在 ARM64上每秒运行90万次，在 x86_64上每秒运行84万次。</p><p><strong>注意</strong>: 由于 mc-crusher 工具不提供任何统计数据。因而我使用 Memcached 的统计命令，以获得执行的操作的数量。</p><p>下面是用于负载测试的设置:</p><ol><li>服务器的启动方式如下:</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ memcached -t 16 -c 256 -m 2048</span><br></pre></td></tr></table></figure><p>即16线程，最多256个并发连接和2Gb 内存。</p><ol start="2"><li><p>MC Crusher<br> a. GET配置</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">send&#x3D;ascii_get,recv&#x3D;blind_read,conns&#x3D;100,key_prefix&#x3D;foobar,pipelines&#x3D;10</span><br><span class="line">send&#x3D;ascii_set,recv&#x3D;blind_read,conns&#x3D;10,key_prefix&#x3D;foobar,pipelines&#x3D;4,stop_after&#x3D;200000,usleep&#x3D;1000,value_size&#x3D;10</span><br></pre></td></tr></table></figure><p> b. SET配置</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">send&#x3D;ascii_set,recv&#x3D;blind_read,conns&#x3D;100,key_prefix&#x3D;foobar,value_size&#x3D;2,value&#x3D;hi,pipelines&#x3D;10</span><br></pre></td></tr></table></figure></li></ol><p>注意: 在 GET 操作的图表中，你可以看到在2020年5月13日，这个数字从每秒950K 次左右上升到每秒160万次左右。 在那一天，我升级了我用作客户机的 VM，也就是我运行负载测试工具(mc-crusher)的地方，因为我注意到在测试运行期间，当客户机本身超载时会出现峰值。</p><p>我们再一次看到，ARM64服务器可以快到和 x86_64一样！</p><p>如果你对如何改善这个Memcached 测试或如何衡量一些其他方面有任何的想法，随意与我分享您的意见！</p><p>祝你黑客生活愉快，注意安全！</p></div><div id="English" class="tab-content"><p>Last week I’ve <a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">shared</a> with you the results of load testing Apache Tomcat on x86_64 and ARM64 CPU architecture. In this article I will test <a href="https://memcached.org/">Memcached</a>.</p><p>What is Memcached ?</p><p>From <a href="https://en.wikipedia.org/wiki/Memcached">Wikipedia</a>: Memcached is a general-purpose distributed <a href="https://en.wikipedia.org/wiki/Memory_caching">memory-caching</a> system. It is often used to speed up dynamic database-driven websites by caching data and <a href="https://en.wikipedia.org/wiki/Object_(computer_science)">objects</a> in <a href="https://en.wikipedia.org/wiki/Random-access_memory">RAM</a> to reduce the number of times an external data source (such as a database or API) must be read.</p><p>In contrast to Apache Tomcat which is written in Java and thus is multi-platform Memcached is written in C and one needs to build it especially for the your CPU architecture. As stated at its Hardware Wiki <a href="https://github.com/memcached/memcached/wiki/Hardware">page</a> ARM64 is one of the officially supported architectures and there is a BuildBot <a href="https://build.memcached.org/#/builders/6">builder</a> testing all code changes! If you face any issue just report it at the project’s <a href="https://github.com/memcached/memcached/issues">issue tracker</a>! Dormando, the project maintainer, is very friendly and responsive!</p><p>In my first attempt to find a good load testing tool for Memcached I stumbled upon <a href="https://github.com/RedisLabs/memtier_benchmark">RedisLabs Memtier Benchmark</a> tool. Running it on the same VMs as in the <a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">article</a> for Apache Tomcat and the results were:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ASCII protocol on ARM64</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Type         Ops&#x2F;sec     Hits&#x2F;sec   Misses&#x2F;sec      Latency       KB&#x2F;sec </span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">Sets          985.28          ---          ---     20.02700        67.22 </span><br><span class="line">Gets         9842.00         0.00      9842.00     20.01900       248.83 </span><br><span class="line">Waits           0.00          ---          ---      0.00000          --- </span><br><span class="line">Totals      10827.28         0.00      9842.00     20.02000       316.05</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ASCII protocol on x86_64</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Type         Ops&#x2F;sec     Hits&#x2F;sec   Misses&#x2F;sec      Latency       KB&#x2F;sec </span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">Sets          931.04          ---          ---     20.06800        63.52 </span><br><span class="line">Gets         9300.21         0.00      9300.21     20.32600       235.13 </span><br><span class="line">Waits           0.00          ---          ---      0.00000          --- </span><br><span class="line">Totals      10231.26         0.00      9300.21     20.30200       298.66</span><br></pre></td></tr></table></figure><p>Above we see that the Memcached server running on the ARM64 VM was slightly faster!</p><p><strong>Note</strong>: the Memcached server was running with default settings (maximum 1024 connections, 4 threads and 64M memory), i.e. without specifying custom values.</p><p>For binary protocol the numbers are almost the same:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Binary protocol on ARM64</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Type         Ops&#x2F;sec     Hits&#x2F;sec   Misses&#x2F;sec      Latency       KB&#x2F;sec </span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">Sets          829.68          ---          ---     23.46500        63.90 </span><br><span class="line">Gets         8287.69         0.00      8287.69     23.56100       314.75 </span><br><span class="line">Waits           0.00          ---          ---      0.00000          --- </span><br><span class="line">Totals       9117.37         0.00      8287.69     23.55200       378.65</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Binary protocol on x86_64</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Type         Ops&#x2F;sec     Hits&#x2F;sec   Misses&#x2F;sec      Latency       KB&#x2F;sec </span><br><span class="line">-------------------------------------------------------------------------</span><br><span class="line">Sets          829.32          ---          ---     23.63600        63.87 </span><br><span class="line">Gets         8284.10         0.00      8284.10     23.58600       314.61 </span><br><span class="line">Waits           0.00          ---          ---      0.00000          --- </span><br><span class="line">Totals       9113.42         0.00      8284.10     23.59100       378.48</span><br></pre></td></tr></table></figure><p>After <a href="https://groups.google.com/d/msg/memcached/8hT2BT9cgEc/Ldm8Q42xAgAJ">sharing</a> these results with Memcached community it was recommended to me to use <a href="https://github.com/memcached/mc-crusher">MC Crusher</a> tool instead. And indeed the numbers are much better with it:</p><p><code>ASCII protocol GET operations per second</code></p><p><img src="https://user-images.githubusercontent.com/10891919/82401295-ffbe1380-9a8b-11ea-928f-aa62068b3967.png" alt="image"></p><p><code>ASCII protocol SET operations per second</code></p><p><img src="https://user-images.githubusercontent.com/10891919/82401314-0d739900-9a8c-11ea-9f96-ccb54c081475.png" alt="image"></p><p>In the first chart you may see that both on x86_64 and ARM64 it makes around 1.5 million get operations per second!</p><p>On the second chart it makes a little bit more than 900 thousand set operations per second on ARM64 and around 840 thousand ops per second on x86_64.</p><p><strong>Note</strong>: Since mc-crusher tool does not provide any statistics from its execution I used Memcached’s stats command to get the number of executed operations.</p><p>Here are the settings used for the load test:</p><ol><li>The servers are started with:</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ memcached -t 16 -c 256 -m 2048</span><br></pre></td></tr></table></figure><p>i.e. with 16 threads, maximum of 256 simultaneous connections and 2Gb memory.</p><ol start="2"><li><p>MC Crusher<br> a. GET config</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">send&#x3D;ascii_get,recv&#x3D;blind_read,conns&#x3D;100,key_prefix&#x3D;foobar,pipelines&#x3D;10</span><br><span class="line">send&#x3D;ascii_set,recv&#x3D;blind_read,conns&#x3D;10,key_prefix&#x3D;foobar,pipelines&#x3D;4,stop_after&#x3D;200000,usleep&#x3D;1000,value_size&#x3D;10</span><br></pre></td></tr></table></figure><p> b. SET config</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">send&#x3D;ascii_set,recv&#x3D;blind_read,conns&#x3D;100,key_prefix&#x3D;foobar,value_size&#x3D;2,value&#x3D;hi,pipelines&#x3D;10</span><br></pre></td></tr></table></figure></li></ol><p>Note: In the chart for the GET operation you see that the number rises at May 13th 2020 from around 950K operations per second to around 1.6 million ops/s. At that day I’ve upgraded the VM that I use as a client, i.e. where I run the load testing tools (mc-crusher) because I’ve noticed that during the test run there were spikes when the client itself was overloaded.</p><p>Once again we saw that ARM64 on the server could be as fast as x86_64!</p><p>If you have ideas how to improve this test or how to measure some other aspect of Memcached feel free to share it with me in the comments!</p><p>Happy hacking and stay safe!</p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: &lt;a href=&quot;https://github.com/wangxiyuan&quot;&gt;wangxiyuan&lt;/a&gt;&lt;br&gt;作者: &lt;a href=&quot;https://github.com/martin-g&quot;&gt;Martin Grigorov&lt;/a&gt;&lt;br&gt;原文链接:  &lt;a href=&quot;https://medium.com/@martin.grigorov/compare-memcached-performance-on-x86-64-and-arm64-cpu-architectures-7fe781e34ab8&quot;&gt;https://medium.com/@martin.grigorov/compare-memcached-performance-on-x86-64-and-arm64-cpu-architectures-7fe781e34ab8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tomcat PMC Martin Grigorov带来的另一篇ARM64 VS X86性能对比文章。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Web" scheme="https://kunpengcompute.github.io/categories/Web/"/>
    
    
      <category term="Web" scheme="https://kunpengcompute.github.io/tags/Web/"/>
    
  </entry>
  
  <entry>
    <title>Mysql社区ARM优化汇总</title>
    <link href="https://kunpengcompute.github.io/2020/05/13/mysql-she-qu-arm-you-hua-hui-zong/"/>
    <id>https://kunpengcompute.github.io/2020/05/13/mysql-she-qu-arm-you-hua-hui-zong/</id>
    <published>2020-05-13T03:24:14.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Krunal Bauskar<br>原文链接: <a href="https://mysqlonarm.github.io/Community-Contributions-So-Far/">https://mysqlonarm.github.io/Community-Contributions-So-Far/</a></p><p>社区拥有来自不同组织的开发人员为 MySQL 提供了一些很好的补丁。但是这些补丁中的大多数都在等待Oracle的接受。 这篇博客的目的是分析这些补丁以及它们的利弊。希望这将有助于 Mysql / Oracle 接受这些期待已久的补丁。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><h2 id="社区Patches"><a href="#社区Patches" class="headerlink" title="社区Patches"></a>社区Patches</h2><h3 id="1-校验和优化"><a href="#1-校验和优化" class="headerlink" title="1. 校验和优化"></a>1. 校验和优化</h3><ul><li>Mysql 使用的校验和有两种: crc32c 和 crc32。因为它们使用不同的多项式而导致它们的之间的不同。<ul><li>crc32c 被用来在 MySQL Innodb中计算页面校验和</li><li>crc32在 MySQL 中用于表校验和、 binlog-checksum 等…</li></ul></li></ul><h4 id="crc32c"><a href="#crc32c" class="headerlink" title="crc32c"></a>crc32c</h4><ul><li>页面校验和是在读/写每个页面时进行的，所以crc32c可以在perl报告中快速的展示出来。确保在使用优化版本后，它能够提高整个系统的性能。</li><li>crc32c 通常是由硬件完成的功能，例如在 x86(SSE)和 ARM (ACLE)。 Innodb 目前使用基于硬件的 x86实现，但还没有使用 ARM (ACLE)实现。 必要的补丁修复有助于解决上述问题。</li><li>最新的补丁(bug # 85819)还有助于在使用 crypto (PMULL)处理指令时来进行进一步优化.</li></ul><p>开源贡献:<br><a href="https://bugs.mysql.com/bug.php?id=79144">bug#79144</a> No hardware CRC32 implementation for AArch64<br><a href="https://bugs.mysql.com/bug.php?id=85819">bug#85819</a> Optimize AARCH64 CRC32c implementation</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[example test-case runs update-non-index and gets the crc32 as top mysqld function].</span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F;o patch)</span><br><span class="line">+   10.43%          8027  mysqld   [kernel.kallsyms]    [k] _raw_spin_unlock_irqrestore                                                                 </span><br><span class="line">+    3.23%          2486  mysqld   mysqld               [.] ut_crc32_sw                                                                                 </span><br><span class="line">+    2.33%          1797  mysqld   [kernel.kallsyms]    [k] finish_task_switch                                                                          </span><br><span class="line">+    1.73%          1330  mysqld   libc-2.27.so         [.] malloc                                                                                    </span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F; patch)</span><br><span class="line">  Overhead       Samples  Command  Shared Object        Symbol                                                                                          </span><br><span class="line">+   10.60%          8133  mysqld   [kernel.kallsyms]    [k] _raw_spin_unlock_irqrestore                                                                 </span><br><span class="line">+    2.37%          1816  mysqld   [kernel.kallsyms]    [k] finish_task_switch                                                                          </span><br><span class="line">+    1.78%          1366  mysqld   libc-2.27.so         [.] malloc                                                                                      </span><br><span class="line">....</span><br><span class="line">     0.44%           338  mysqld   mysqld               [.] ut_crc32_aarch64</span><br></pre></td></tr></table></figure><p><strong>结论:</strong> 明显可以考到节省了大约3%的吞吐量。另外，在更加广泛的测试中，我们可以看到crc32有助于提高所有类型测试场景下的吞吐量。</p><h4 id="crc32"><a href="#crc32" class="headerlink" title="crc32"></a>crc32</h4><p>为了计算表校验和，MySQL 使用基于 zlib 的 crc32(软实现)。 据我所知，x86不支持 crc32计算的硬件优化版本，但幸运的是 ARM (ACLE)支持。 同时Binlog-checksum 也使用相同的代码 / 处理流程。</p><p>开源贡献:<br><a href="https://bugs.mysql.com/bug.php?id=99118">bug#99118</a> ARM CRC32 intrinsic call to accelerate table-checksum (not crc32c but crc32)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[example test-case runs checksum on all tables and update-non-index].</span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F;o patch)</span><br><span class="line"></span><br><span class="line">checksum:</span><br><span class="line">+   49.46%         13480  mysqld   mysqld               [.] crc32_z</span><br><span class="line"></span><br><span class="line">update-non-index:</span><br><span class="line">     0.40%           311  mysqld   mysqld               [.] crc32_z                                                                                     </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F; patch)</span><br><span class="line"></span><br><span class="line">checksum:</span><br><span class="line">+    8.15%           988  mysqld   mysqld               [.] aarch64_crc32_checksum                                                                      </span><br><span class="line"></span><br><span class="line">update-non-index:</span><br><span class="line">     0.07%            56  mysqld   mysqld               [.] aarch64_crc32_checksum</span><br></pre></td></tr></table></figure><p><strong>结论:</strong> 这个补丁在两个方面都有提升。超级加速表校验和(平均提高50%) ，并且在 binlog 校验和中也是。</p><h3 id="2-my-convert-in-turn-copy-and-convert-在ARM平台表现不好"><a href="#2-my-convert-in-turn-copy-and-convert-在ARM平台表现不好" class="headerlink" title="2. my_convert (in turn copy_and_convert) 在ARM平台表现不好:"></a>2. my_convert (in turn copy_and_convert) 在ARM平台表现不好:</h3><ul><li>my_convert用作发送结果的一部分步骤中，主要是用于在字符集之间进行转换。</li><li>给定转换的数据量，这个函数会出现在 perf top-list 中</li><li>现有的实现对于 x86使用4字节的复制，但对于 ARM 则退回到单字节复制。 通过对 x86-64和 aarch64使用8个字节的复制，然后再到现有逻辑的尾部处理过程，可以总体改进这一点。 这个简单的补丁可以帮助节省大量的时钟周期并提高系统性能。</li></ul><p>开源贡献:<br><a href="https://bugs.mysql.com/bug.php?id=98737">bug#98737</a> my_convert routine is suboptimal in case of non-x86 platforms</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[example test-case runs oltp-read-write on all tables].</span><br><span class="line">perf analysis (w&#x2F;o patch)</span><br><span class="line">+    0.79%          1114  mysqld   mysqld               [.] my_convert</span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F; patch)</span><br><span class="line">     0.22%           299  mysqld   mysqld               [.] my_convert</span><br></pre></td></tr></table></figure><p><strong>结论:</strong> 这个补丁提高了吞吐量。</p><h3 id="3-为原子变量优化内存屏障"><a href="#3-为原子变量优化内存屏障" class="headerlink" title="3. 为原子变量优化内存屏障:"></a>3. 为原子变量优化内存屏障:</h3><ul><li>Mysql / innodb 有很多变量，它使用 gcc 内置的原子函数(sync add and fetch 或 atomic add fetch).</li><li>虽然在x86的强内存模型，它们表现很好，但是大多数这些计数器函数中都是使用顺序内存排序(缺省)来实现的。</li><li>因为Arm 的弱内存模型，因此不推荐使用这种顺序内存排序(缺省的)。</li><li>社区多个补丁来帮助修改这些代码片段。它们有助于实现两个目标:<ul><li>切换到使用 c + + 11原子函数(MySQL现已支持)。</li><li>切换到使用松散的内存顺序(vs 顺序)。</li></ul></li></ul><p>开源贡献:<br><a href="https://bugs.mysql.com/bug.php?id=97228">bug#97228</a> rwlock: refine lock-&gt;lock_word with C11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97230">bug#97230</a> rwlock: refine lock-&gt;waiters with C++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97703">bug#97703</a> innobase/dict: refine dict_temp_file_num with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97704">bug#97704</a> innobase/srv: refine srv0conc with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97765">bug#97765</a> innobase/os: refine os_total_large_mem_allocated with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97766">bug#97766</a> innobase/os_once: optimize os_once with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97767">bug#97767</a> innobase/dict: refine zip_pad_info-&gt;pad with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=99432">bug#99432</a> Improving memory barrier during rseg allocation</p><p><strong>结论:</strong> 由于它的影响分布非常广，所以很难用perf来进行判断。另外，部分修复仅仅是为了改进语义，而不是为了性能因素。</p><h3 id="4-为全局计数器带来核心亲和性调度"><a href="#4-为全局计数器带来核心亲和性调度" class="headerlink" title="4. 为全局计数器带来核心亲和性调度:"></a>4. 为全局计数器带来核心亲和性调度:</h3><p>Arm 以拥有大量的核心(和 numa 套接字)而闻名，为了从多核中获得最大的吞吐量，确保全局计数器的可编程性是非常重要的。拥有一个分布式计数器并将计数器的递增部分尽量与线程核心靠近，应该可以避免跨numa延迟。</p><p>MySQL通过调用call sched_getcpu来获取计数器插槽，但是这个逻辑由于另一个bug的修复而改变了(这对于上述问题来说当然是有意义的) ， 这个bug修复影响了正常的全局计数器。下面的补丁提议纠正这一点，并使用sched_getcpu(核心亲和性)来实现全局计数器。</p><p><em>不幸的是，在 ARM 上，这个补丁由于使用了 sched_getcpu 而产生了开销，但是在x86上在使用VDSO时进行了优化。</em></p><p>开源贡献:<br><a href="https://bugs.mysql.com/bug.php?id=79455">bug#79455</a> Restore get_sched_indexer_t in 5.7</p><h3 id="5-当前UT-RELAX-CPU-在ARM平台上的可伸缩性问题"><a href="#5-当前UT-RELAX-CPU-在ARM平台上的可伸缩性问题" class="headerlink" title="5. 当前UT_RELAX_CPU () 在ARM平台上的可伸缩性问题:"></a>5. 当前UT_RELAX_CPU () 在ARM平台上的可伸缩性问题:</h3><p>Innodb 使用自制的 spin-wait 来实现 rw-locks 和 mutexes。 无论何时需要休眠(或让我纠正称它为PAUSE) ，在 x86 MySQL 上支持PAUSE指令。 ARM不支持 PAUSE 指令，因此流程中使用编译器屏障，但是该指令未能引入所需的延迟。 修补程序建议使用Compare-And-Exchange，这应该有助于引入类似的延迟(如PAUSE)。</p><p>开源贡献:<br><a href="https://bugs.mysql.com/bug.php?id=87933">bug#87933</a> Scalibility issue on Arm platform with the current UT_RELAX_CPU () code.</p><p>基于内部评估，我们得不到补丁所带来的吞吐量提升，因此目前没有将其纳入我们贡献到社区的内容。</p><h3 id="6-在ARM上应用更宽的cacheline来填充"><a href="#6-在ARM上应用更宽的cacheline来填充" class="headerlink" title="6. 在ARM上应用更宽的cacheline来填充:"></a>6. 在ARM上应用更宽的cacheline来填充:</h3><p>大多数 ARM 处理器计划拥有更宽的cacheline size。 补丁提出基于 ARM 处理器使用更大的cachelilne，并填充以避免false sharing问题。</p><p>开源贡献:<br><a href="https://bugs.mysql.com/bug.php?id=98499">bug#98499</a> Improvement about CPU cache line size</p><h3 id="7-其他开源贡献"><a href="#7-其他开源贡献" class="headerlink" title="7. 其他开源贡献"></a>7. 其他开源贡献</h3><p>除了上面列出的6个大类，在其他领域也有很多的贡献。 但是大多数都没有相关的代码提交，或者这个想法已经作为另一个重大改进被放在 MySQL 中(不是针对 ARM 的工作) ，又或者是这个idea不太可能对性能产生影响。</p><h2 id="社区补丁对性能的影响"><a href="#社区补丁对性能的影响" class="headerlink" title="社区补丁对性能的影响"></a>社区补丁对性能的影响</h2><p>基于上面收集的内容，我们分析了引入社区补丁后对性能的影响，下面的表显示了如果我们合入这些补丁，吞吐量将如何提高。 结果限制在较大的可伸缩性(256线程)，因为它显示了主要的影响，我们已经全面运行了测试用例，确定补丁有助于提高总吞吐量(即使对于单线程而言)。</p><table><thead><tr><th></th><th>point select</th><th>read only</th><th>read write</th><th>update index</th><th>update non index</th></tr></thead><tbody><tr><td>without-patch</td><td>218447</td><td>145755</td><td>5646</td><td>22200</td><td>22601</td></tr><tr><td>with-patch</td><td>224355</td><td>149718</td><td>5829</td><td>23070</td><td>23292</td></tr><tr><td>%</td><td>2.7</td><td>2.72</td><td>3.24</td><td>3.92</td><td>3.06</td></tr></tbody></table><p><em>使用 mysql-8.0.20进行评估。配置参看 <a href="https://github.com/mysqlonarm/benchmark-suites/blob/master/sysbench/conf/96tx1.5m_cpubound.cnf">here</a>. 处理器: ARM Kunpeng 920 24vCPU/48GB</em></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>社区提供的补丁确实有助于优化 ARM 上的 MySQL，但影响程度有限，并且需要覆盖很多领域才能看到 MySQL 在 ARM 上加速的程度。 如果你有好的想法，请联系我或者直接在社区进行交流。 ARM MySQL 社区可以尽情针对这一问题爆发头脑风暴，并尝试实现修复这个问题的idea。</p><p><em>如果你有问题 / 疑问，请让我知道，我会尽力回答。</em></p></div><div id="English" class="tab-content"><h2 id="Community-Patches"><a href="#Community-Patches" class="headerlink" title="Community Patches"></a>Community Patches</h2><h3 id="1-Optimizing-checksum"><a href="#1-Optimizing-checksum" class="headerlink" title="1. Optimizing checksum"></a>1. Optimizing checksum</h3><ul><li>MySQL uses 2 types of checksum: crc32c and crc32. They both are different since both uses different polynomials.<ul><li>crc32c is used in MySQL by InnoDB to calculate page-checksum.</li><li>crc32 is used in MySQL for table checksum, binlog-checksum, etc…</li></ul></li></ul><h4 id="crc32c-1"><a href="#crc32c-1" class="headerlink" title="crc32c"></a>crc32c</h4><ul><li>Page checksum is calculated during each page read/write so crc32c can quickly show up as one of the top functions in perf report. Ensuring use of optimized versions of it could help improve the overall throughput of the system.</li><li>crc32c has been implemented as a hardware function on both x86 (SSE) and ARM (ACLE). InnoDB currently uses hardware based implementation for x86 but not yet for ARM (ACLE). Patch helps address the said issue.</li><li>Latest patch (bug#85819) also helps further optimize it using crypto (PMULL) processing instruction.</li></ul><p>Open Contributions:<br><a href="https://bugs.mysql.com/bug.php?id=79144" title="suggest use of crc32c">bug#79144</a> No hardware CRC32 implementation for AArch64<br><a href="https://bugs.mysql.com/bug.php?id=85819">bug#85819</a> Optimize AARCH64 CRC32c implementation</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[example test-case runs update-non-index and gets the crc32 as top mysqld function].</span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F;o patch)</span><br><span class="line">+   10.43%          8027  mysqld   [kernel.kallsyms]    [k] _raw_spin_unlock_irqrestore                                                                 </span><br><span class="line">+    3.23%          2486  mysqld   mysqld               [.] ut_crc32_sw                                                                                 </span><br><span class="line">+    2.33%          1797  mysqld   [kernel.kallsyms]    [k] finish_task_switch                                                                          </span><br><span class="line">+    1.73%          1330  mysqld   libc-2.27.so         [.] malloc                                                                                    </span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F; patch)</span><br><span class="line">  Overhead       Samples  Command  Shared Object        Symbol                                                                                          </span><br><span class="line">+   10.60%          8133  mysqld   [kernel.kallsyms]    [k] _raw_spin_unlock_irqrestore                                                                 </span><br><span class="line">+    2.37%          1816  mysqld   [kernel.kallsyms]    [k] finish_task_switch                                                                          </span><br><span class="line">+    1.78%          1366  mysqld   libc-2.27.so         [.] malloc                                                                                      </span><br><span class="line">....</span><br><span class="line">     0.44%           338  mysqld   mysqld               [.] ut_crc32_aarch64</span><br></pre></td></tr></table></figure><p><strong>Conclusion:</strong> Clearly a saving of around 3% in overall throughput can be seen. Also, as part of wider testing we see crc32c helps in overall throughput gain for all kind of test-cases.</p><h4 id="crc32-1"><a href="#crc32-1" class="headerlink" title="crc32"></a>crc32</h4><p>For calculating table checksum MySQL uses zlib-based crc32. As per my knowledge, x86 doesn’t support hardware optimized versions for crc32 calculation but fortunately ARM (ACLE) supports it. The same code/flow path is also used for binlog-checksum.</p><p>Open Contributions:<br><a href="https://bugs.mysql.com/bug.php?id=99118">bug#99118</a> ARM CRC32 intrinsic call to accelerate table-checksum (not crc32c but crc32)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[example test-case runs checksum on all tables and update-non-index].</span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F;o patch)</span><br><span class="line"></span><br><span class="line">checksum:</span><br><span class="line">+   49.46%         13480  mysqld   mysqld               [.] crc32_z</span><br><span class="line"></span><br><span class="line">update-non-index:</span><br><span class="line">     0.40%           311  mysqld   mysqld               [.] crc32_z                                                                                     </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F; patch)</span><br><span class="line"></span><br><span class="line">checksum:</span><br><span class="line">+    8.15%           988  mysqld   mysqld               [.] aarch64_crc32_checksum                                                                      </span><br><span class="line"></span><br><span class="line">update-non-index:</span><br><span class="line">     0.07%            56  mysqld   mysqld               [.] aarch64_crc32_checksum</span><br></pre></td></tr></table></figure><p><strong>Conclusion:</strong> This patch helps on both front. Super-accelerate table checksum (average improvement of 50%) and also marginally helps in binlog-checksum.</p><h3 id="2-my-convert-in-turn-copy-and-convert-routine-is-suboptimal-for-ARM"><a href="#2-my-convert-in-turn-copy-and-convert-routine-is-suboptimal-for-ARM" class="headerlink" title="2. my_convert (in turn copy_and_convert) routine is suboptimal for ARM:"></a>2. my_convert (in turn copy_and_convert) routine is suboptimal for ARM:</h3><ul><li>my_convert is used as part of the send result for converting among charsets.</li><li>Given the amount of the data that is converted this function gets spotted in perf top-list.</li><li>Existing implementation uses 4 bytes copying for x86 but falls back to byte copy for ARM. This could be overall improved by using 8 bytes copying for x86-64 and aarch64 and then falling back for trailing things to existing logic. Patch for this simple operation help save significant cycles and help improve performance.</li></ul><p>Open Contributions:<br><a href="https://bugs.mysql.com/bug.php?id=98737">bug#98737</a> my_convert routine is suboptimal in case of non-x86 platforms</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[example test-case runs oltp-read-write on all tables].</span><br><span class="line">perf analysis (w&#x2F;o patch)</span><br><span class="line">+    0.79%          1114  mysqld   mysqld               [.] my_convert</span><br><span class="line"></span><br><span class="line">perf analysis (w&#x2F; patch)</span><br><span class="line">     0.22%           299  mysqld   mysqld               [.] my_convert</span><br></pre></td></tr></table></figure><p><strong>Conclusion:</strong> Patch can help improve overall throuhgput.</p><h3 id="3-Improving-memory-barrier-for-atomic-variables"><a href="#3-Improving-memory-barrier-for-atomic-variables" class="headerlink" title="3. Improving memory barrier for atomic variables:"></a>3. Improving memory barrier for atomic variables:</h3><ul><li>MySQL/InnoDB has a lot of variables for which it uses gcc inbuilt atomic functions (__sync_add_and_fetch or __atomic_add_fetch).</li><li>While this is all good x86 being a strong memory model most of these counter functions were implemented to use sequential memory ordering (default).</li><li>ARM has a relaxed memory model so using sequential memory ordering (default one) is not recommended.</li><li>Multiple patches were submitted to help revamp the said snippets. Patches help achieve 2 things:<ul><li>Switch to use C++11 atomics. (Now that MySQL supports it).</li><li>Switch to use relaxed memory order (vs sequential).</li></ul></li></ul><p>Open Contributions:<br><a href="https://bugs.mysql.com/bug.php?id=97228">bug#97228</a> rwlock: refine lock-&gt;lock_word with C11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97230">bug#97230</a> rwlock: refine lock-&gt;waiters with C++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97703">bug#97703</a> innobase/dict: refine dict_temp_file_num with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97704">bug#97704</a> innobase/srv: refine srv0conc with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97765">bug#97765</a> innobase/os: refine os_total_large_mem_allocated with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97766">bug#97766</a> innobase/os_once: optimize os_once with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=97767">bug#97767</a> innobase/dict: refine zip_pad_info-&gt;pad with c++11 atomics<br><a href="https://bugs.mysql.com/bug.php?id=99432">bug#99432</a> Improving memory barrier during rseg allocation</p><p><strong>Conclusion:</strong> Impact is wide spread so difficult to judge using perf. Also, some of the fixes help improve semantics and may not be for performance reason as such.</p><h3 id="4-Restore-core-affinity-scheduler-for-global-counter"><a href="#4-Restore-core-affinity-scheduler-for-global-counter" class="headerlink" title="4. Restore core affinity scheduler for global counter:"></a>4. Restore core affinity scheduler for global counter:</h3><p>ARM is known for its large number of cores (and numa sockets) and to harvest the max throughput from multi-cores it is important to ensure that global counters are programmed accordingly. Having a distributed counter and incrementing part of the counter closer to the thread core should avoid cross-numa latency.</p><p>MySQL use to call sched_getcpu for getting the counter slots but this logic was changed as part of the different bug fix (that surely made sense for the said issue) but it also affected the normal global counters. Patch proposes to correct this and use sched_getcpu (core affinity) based counter for global counters.</p><p><em>On ARM this patch unfortunately is running into overhead resulting from use of sched_getcpu which is optimized on x86 using VDSO.</em></p><p>Open Contributions:<br><a href="https://bugs.mysql.com/bug.php?id=79455">bug#79455</a> Restore get_sched_indexer_t in 5.7</p><h3 id="5-Scalability-issue-on-ARM-platform-with-the-current-UT-RELAX-CPU-code"><a href="#5-Scalability-issue-on-ARM-platform-with-the-current-UT-RELAX-CPU-code" class="headerlink" title="5. Scalability issue on ARM platform with the current UT_RELAX_CPU () code:"></a>5. Scalability issue on ARM platform with the current UT_RELAX_CPU () code:</h3><p>InnoDB uses home-grown spin-wait implementation for rw-locks and mutexes. Whenever there is a need to sleep (or let me correctly say PAUSE) then on x86 MySQL uses supported PAUSE instruction. ARM doesn’t have support for PAUSE instruction so the flow uses a compiler barrier but this statement fails to introduce the needed delay. Patch suggest use of Compare-And-Exchange that should help introduce comparable delay (like PAUSE).</p><p>Open Contributions:<br><a href="https://bugs.mysql.com/bug.php?id=87933">bug#87933</a> Scalibility issue on Arm platform with the current UT_RELAX_CPU () code.</p><p>Based on internal evaluation we couldn’t get the patch to help improve on throughput so have not-considered it as part of our community-patch branch for now.</p><h3 id="6-Using-wider-cacheline-padding-for-ARM"><a href="#6-Using-wider-cacheline-padding-for-ARM" class="headerlink" title="6. Using wider cacheline padding for ARM:"></a>6. Using wider cacheline padding for ARM:</h3><p>Most of the ARM processors are scheduled to have a wider cache line. Patch proposes use of a wider cache line padding for ARM based processors to avoid false sharing.</p><p>Open Contributions:<br><a href="https://bugs.mysql.com/bug.php?id=98499">bug#98499</a> Improvement about CPU cache line size</p><h3 id="7-Other-open-contributions"><a href="#7-Other-open-contributions" class="headerlink" title="7. Other open contributions"></a>7. Other open contributions</h3><p>Besides the 6 main categories listed above there are more contributions in other areas too. But most of them didn’t have a patch associated or the said idea has been folded in MySQL as part of another major revamp (not specific to ARM work) or the idea is less likely to have a performance impact. So for now we were not able to consider these set of patches.</p><h2 id="Performance-impact-of-Community-Patches"><a href="#Performance-impact-of-Community-Patches" class="headerlink" title="Performance impact of Community Patches"></a>Performance impact of Community Patches</h2><p>Based on the inputs collected above we have analyzed performance impact of community patches and below table help shows how the throughput would improve if the said patches are accepted. Limiting results for higher scalability (256 threads) where it shows major effect but we have run test-case across the board and patches helps improve overall throughput (even for single threaded).</p><table><thead><tr><th></th><th>point select</th><th>read only</th><th>read write</th><th>update index</th><th>update non index</th></tr></thead><tbody><tr><td>without-patch</td><td>218447</td><td>145755</td><td>5646</td><td>22200</td><td>22601</td></tr><tr><td>with-patch</td><td>224355</td><td>149718</td><td>5829</td><td>23070</td><td>23292</td></tr><tr><td>%</td><td>2.7</td><td>2.72</td><td>3.24</td><td>3.92</td><td>3.06</td></tr></tbody></table><p><em>Evaluated using mysql-8.0.20. For configuration check <a href="https://github.com/mysqlonarm/benchmark-suites/blob/master/sysbench/conf/96tx1.5m_cpubound.cnf">here</a>. Processor: ARM Kunpeng 920 24vCPU/48GB</em></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Patches contributed by community surely helps in optimizing MySQL on ARM but the impact is still limited and lot of ground to cover to make MySQL accelerate on ARM. If you have good ideas on how things could be pushed further then let’s connect. ARM MySQL community can help brainstorm the idea and aid/help in materializing it to a contribution.</p><p><em>If you have more questions/queries do let me know. Will try to answer them.</em></p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Krunal Bauskar&lt;br&gt;原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/Community-Contributions-So-Far/&quot;&gt;https://mysqlonarm.github.io/Community-Contributions-So-Far/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;社区拥有来自不同组织的开发人员为 MySQL 提供了一些很好的补丁。但是这些补丁中的大多数都在等待Oracle的接受。 这篇博客的目的是分析这些补丁以及它们的利弊。希望这将有助于 Mysql / Oracle 接受这些期待已久的补丁。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Apache Tomcat X86 vs ARM64 性能比拼</title>
    <link href="https://kunpengcompute.github.io/2020/05/13/apache-tomcat-x86-vs-arm64-xing-neng-bi-pin/"/>
    <id>https://kunpengcompute.github.io/2020/05/13/apache-tomcat-x86-vs-arm64-xing-neng-bi-pin/</id>
    <published>2020-05-13T01:28:58.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>译者: <a href="https://github.com/wangxiyuan">wangxiyuan</a><br>作者:  <a href="https://github.com/martin-g">Martin Grigorov</a><br>原文链接:  <a href="https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6">https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6</a></p><p>Tomcat PMC Martin Grigorov带来的Tomcat X86 VS ARM64性能测试。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>大多数软件开发人员通常不会考虑他们的软件将在何种 CPU 架构上运行。 尽管没有官方的统计数据，但根据我的经验，大多数桌面和后端应用软件都运行在 x86_64架构(英特尔和 AMD 处理器)上，大多数移动和物联网设备都运行在 ARM 架构上。 开发人员使用一些高级编程语言为各自的 CPU 架构编写软件，并不考虑在运行时执行何种汇编指令。 而这正是高级编程语言的目的—- 让编译器处理低级硬件指令，并简化我们的任务，使其只专注于高级业务相关问题。</p><p>生活简单而美好，但有时候，笔记本电脑和台式机硬件及软件制造业的<a href="https://www.apple.com/">巨头</a>会说，我们的软件必须在不同的架构上运行——先是从 <a href="https://en.wikipedia.org/wiki/Apple%27s_transition_to_Intel_processors#Timeline">PowerPC 到英特尔</a>，现在从英特尔到 ARM64(消息来源: <a href="https://www.bloomberg.com/news/articles/2020-04-23/apple-aims-to-sell-macs-with-its-own-chips-starting-in-2021">Bloomberg</a> &amp; <a href="https://appleinsider.com/articles/20/02/25/why-apple-will-move-macs-to-arm-and-what-consumers-get">AppleInsider</a>)。 由于电力消耗较低，甚至一些较大的云供应商也开始提供 ARM64虚拟机(如<a href="https://aws.amazon.com/ec2/graviton/">亚马逊 AWS</a>、<a href="https://www.huaweicloud.com/en-us/product/ecs.html">华为云</a>、 <a href="https://www.linaro.cloud/">Linaro</a>)。 但还有以下不确定性:</p><ul><li>我的软件能在新的 CPU 架构上运行吗?</li><li>我需要做出什么样的改变才能让它发挥作用</li><li>它会像以前一样表现出色吗</li></ul><p>为了能够回答这些问题，你必须撸起袖子进行测试！</p><p>您可以在任何云供应商上部署软件。 有些还提供免费试用期！ 或者如果你的预算很少，你可以试试 <a href="https://www.raspberrypi.org/">RaspberryPi</a>。</p><p>根据您编写软件所使用的编程语言，您可能需要进行一些更改，或者根本不需要更改！ 如果你使用一个直译语言文件(例如 Python，Perl，Ruby，JVM，…) ，那么解释器已经支持 ARM64的可能性相当高，你可以不做任何改变就继续使用它！ 但是，如果你的软件需要被编译，那么你需要调整你的工具链，并确保有 ARM64二进制文件为你所有的依赖！ 根据您的软件开发堆栈，您的修改量可能会有所不同！</p><p>一旦我们的软件在新架构上运行良好，我们将能够检查它是否像以前那样执行良好。 最近一些用户在 Apache Tomcat 邮件列表中询问是否支持 ARM64架构。 因为 Apache Tomcat 大部分代码是用 Java 编写的，所以它可以基本的运行在ARM64上。 如果您需要使用 <code>libtcnative</code> 和 / 或 <code>mod_jk</code>，那么您需要自己在 ARM64上构建它们。 Apache Tomcat 团队使用 TravisCI 在 ARM64上测试 <a href="https://travis-ci.org/github/apache/tomcat">Java</a> 和 <a href="https://travis-ci.org/github/martin-g/tomcat-connectors">C</a> 代码，目前还没有已知的问题！</p><p>为了比较某些软件的两个版本的性能，通常您将在同一个硬件上运行测试，但在这种情况下，由于我们使用不同的 CPU 架构，这是不可能的。 在我的测试中，我使用了两个具有类似规范的 vm：</p><ul><li><p>X86_64处理器是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Architecture:        x86_64</span><br><span class="line">CPU op-mode(s):      32-bit, 64-bit</span><br><span class="line">Byte Order:          Little Endian</span><br><span class="line">CPU(s):              8</span><br><span class="line">On-line CPU(s) list: 0-7</span><br><span class="line">Thread(s) per core:  2</span><br><span class="line">Core(s) per socket:  4</span><br><span class="line">Socket(s):           1</span><br><span class="line">NUMA node(s):        1</span><br><span class="line">Vendor ID:           GenuineIntel</span><br><span class="line">CPU family:          6</span><br><span class="line">Model:               85</span><br><span class="line">Model name:          Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz</span><br><span class="line">Stepping:            7</span><br><span class="line">CPU MHz:             3000.000</span><br><span class="line">BogoMIPS:            6000.00</span><br><span class="line">Hypervisor vendor:   KVM</span><br><span class="line">Virtualization type: full</span><br><span class="line">L1d cache:           32K</span><br><span class="line">L1i cache:           32K</span><br><span class="line">L2 cache:            1024K</span><br><span class="line">L3 cache:            30976K</span><br><span class="line">NUMA node0 CPU(s):   0-7</span><br><span class="line">Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat avx512_vnni md_clear flush_l1d arch_capabilities</span><br></pre></td></tr></table></figure></li><li><p>Arm64处理器是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Architecture:        aarch64</span><br><span class="line">Byte Order:          Little Endian</span><br><span class="line">CPU(s):              8</span><br><span class="line">On-line CPU(s) list: 0-7</span><br><span class="line">Thread(s) per core:  1</span><br><span class="line">Core(s) per socket:  8</span><br><span class="line">Socket(s):           1</span><br><span class="line">NUMA node(s):        1</span><br><span class="line">Vendor ID:           0x48</span><br><span class="line">Model:               0</span><br><span class="line">Stepping:            0x1</span><br><span class="line">BogoMIPS:            200.00</span><br><span class="line">L1d cache:           64K</span><br><span class="line">L1i cache:           64K</span><br><span class="line">L2 cache:            512K</span><br><span class="line">L3 cache:            32768K</span><br><span class="line">NUMA node0 CPU(s):   0-7</span><br><span class="line">Flags:               fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma dcpop asimddp asimdfhm</span><br></pre></td></tr></table></figure></li></ul><p>两个虚拟机具有相同数量的 RAM、磁盘和网络连接。</p><p>测试应用程序基于 Spring Boot (2.2.7) ，运行嵌入式 Apache Tomcat 9.0.x + OpenSSL 1.1.1h-dev 和 Apache Apr 1.7.x。 每晚构建，并且有一个单独的 REST 客户端，该客户端公开一个用于创建实体的 PUT Endpoint、一个用于读取它的 GET Endpoint、一个用于更新它的 POST Endpoint和一个用于删除它的 DELETE Endpoint。 它使用 <a href="https://memcached.org/">Memcached</a> 作为数据库。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line">package info.mgsolutions.testbed.rest;</span><br><span class="line"></span><br><span class="line">import info.mgsolutions.testbed.domain.Error;</span><br><span class="line">import info.mgsolutions.testbed.domain.Person;</span><br><span class="line">import info.mgsolutions.testbed.domain.Response;</span><br><span class="line">import lombok.extern.slf4j.Slf4j;</span><br><span class="line">import net.rubyeye.xmemcached.MemcachedClient;</span><br><span class="line">import net.rubyeye.xmemcached.exception.MemcachedException;</span><br><span class="line">import net.rubyeye.xmemcached.transcoders.SerializingTranscoder;</span><br><span class="line">import org.springframework.http.HttpStatus;</span><br><span class="line">import org.springframework.http.MediaType;</span><br><span class="line">import org.springframework.http.ResponseEntity;</span><br><span class="line">import org.springframework.http.server.ServletServerHttpRequest;</span><br><span class="line">import org.springframework.web.bind.annotation.DeleteMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.GetMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.PostMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.PutMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.RequestBody;</span><br><span class="line">import org.springframework.web.bind.annotation.RequestMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.RequestParam;</span><br><span class="line">import org.springframework.web.bind.annotation.RestController;</span><br><span class="line">import org.springframework.web.util.UriComponents;</span><br><span class="line">import org.springframework.web.util.UriComponentsBuilder;</span><br><span class="line"></span><br><span class="line">import javax.servlet.http.HttpServletRequest;</span><br><span class="line">import java.net.URI;</span><br><span class="line">import java.nio.charset.StandardCharsets;</span><br><span class="line">import java.util.Base64;</span><br><span class="line">import java.util.concurrent.TimeoutException;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * A REST endpoint that uses Memcached to get its data.</span><br><span class="line"> *&#x2F;</span><br><span class="line">@RestController</span><br><span class="line">@RequestMapping(&quot;testbed&#x2F;memcached&quot;)</span><br><span class="line">@Slf4j</span><br><span class="line">public class MemcachedTestController &#123;</span><br><span class="line"></span><br><span class="line">public static final int TTL_IN_SECONDS &#x3D; 1000;</span><br><span class="line"></span><br><span class="line">private final SerializingTranscoder coder &#x3D; new SerializingTranscoder();</span><br><span class="line">private final MemcachedClient client;</span><br><span class="line"></span><br><span class="line">public MemcachedTestController(MemcachedClient client) &#123;</span><br><span class="line">this.client &#x3D; client;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@PutMapping(value &#x3D;&quot;&quot;, consumes &#x3D; MediaType.APPLICATION_JSON_VALUE, produces &#x3D; MediaType.APPLICATION_JSON_VALUE)</span><br><span class="line">public ResponseEntity&lt;Response&gt; create(@RequestBody Person person,</span><br><span class="line">                                       HttpServletRequest servletRequest) throws InterruptedException, MemcachedException, TimeoutException &#123;</span><br><span class="line"></span><br><span class="line">final String base64Name &#x3D; base64(person.name);</span><br><span class="line">Person existing &#x3D; client.get(base64Name);</span><br><span class="line">if (existing !&#x3D; null) &#123;</span><br><span class="line">log.info(&quot;Create: Person with name &#123;&#125; already exists!&quot;, person.name);</span><br><span class="line">Error error &#x3D; new Error(&quot;Person with name &quot; + person.name + &quot; already exists!&quot;);</span><br><span class="line">return ResponseEntity.status(HttpStatus.NOT_ACCEPTABLE)</span><br><span class="line">                     .body(error);</span><br><span class="line">&#125;</span><br><span class="line">log.info(&quot;Create: Going to create &#39;&#123;&#125;&#39;&quot;, person);</span><br><span class="line">client.set(base64Name, TTL_IN_SECONDS, person, coder);</span><br><span class="line"></span><br><span class="line">ServletServerHttpRequest request &#x3D; new ServletServerHttpRequest(servletRequest);</span><br><span class="line">final UriComponents uriComponents &#x3D; UriComponentsBuilder.fromHttpRequest(request).build();</span><br><span class="line">final URI uri &#x3D; uriComponents.encode(StandardCharsets.UTF_8).toUri();</span><br><span class="line">return ResponseEntity.created(uri).contentType(MediaType.APPLICATION_JSON).body(person);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@GetMapping(value &#x3D;&quot;&quot;, consumes &#x3D; MediaType.ALL_VALUE, produces &#x3D; MediaType.APPLICATION_JSON_VALUE)</span><br><span class="line">public ResponseEntity&lt;Person&gt; get(@RequestParam String name) throws InterruptedException, MemcachedException, TimeoutException &#123;</span><br><span class="line"></span><br><span class="line">Person person &#x3D; (Person) client.get(base64(name), coder);</span><br><span class="line">if (person &#x3D;&#x3D; null) &#123;</span><br><span class="line">log.info(&quot;Get: Cannot find a person with name &#123;&#125;!&quot;, name);</span><br><span class="line">return ResponseEntity.notFound().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">log.info(&quot;Get: Found person with name: &#123;&#125;!&quot;, name);</span><br><span class="line">return ResponseEntity.ok().body(person);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@PostMapping(value &#x3D;&quot;&quot;, consumes &#x3D; MediaType.APPLICATION_JSON_VALUE, produces &#x3D; MediaType.APPLICATION_JSON_VALUE)</span><br><span class="line">public ResponseEntity&lt;Person&gt; update(@RequestBody Person person) throws InterruptedException, MemcachedException, TimeoutException &#123;</span><br><span class="line">final String name &#x3D; person.name;</span><br><span class="line">final String base64Name &#x3D; base64(name);</span><br><span class="line">final Person existing &#x3D; (Person) client.get(base64Name, coder);</span><br><span class="line">if (existing !&#x3D; null) &#123;</span><br><span class="line">log.info(&quot;Update: Going to update: &#123;&#125;&quot;, person);</span><br><span class="line">client.set(base64Name, TTL_IN_SECONDS, person, coder);</span><br><span class="line">return ResponseEntity.ok().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">log.info(&quot;Update: Cannot find a person with name &#123;&#125;!&quot;, person.name);</span><br><span class="line">return ResponseEntity.notFound().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@DeleteMapping(value &#x3D;&quot;&quot;, produces &#x3D; MediaType.APPLICATION_JSON_VALUE)</span><br><span class="line">public ResponseEntity&lt;Person&gt; delete(@RequestParam String name) throws InterruptedException, MemcachedException, TimeoutException &#123;</span><br><span class="line"></span><br><span class="line">final String base64Name &#x3D; base64(name);</span><br><span class="line">final Person person &#x3D; client.get(base64Name);</span><br><span class="line">if (person !&#x3D; null) &#123;</span><br><span class="line">log.info(&quot;Delete: Going to delete: &#123;&#125;&quot;, person);</span><br><span class="line">client.delete(base64Name);</span><br><span class="line">return ResponseEntity.ok().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">log.info(&quot;Delete: Cannot find a person with name &#123;&#125;!&quot;, name);</span><br><span class="line">return ResponseEntity.notFound().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private String base64(String name) &#123;</span><br><span class="line">return Base64.getEncoder().encodeToString(name.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于负载测试，我使用了 <a href="https://jmeter.apache.org/">Apache JMeter 5.2.1</a>和基于主干代码的 <a href="https://github.com/wg/wrk">wrk</a>。 Jmeter 用于一个真实的情况场景，有1000个并发用户，在 HTTP 请求之间有一个适应期和一段思考时间。 然后用 wrk 测试最大吞吐量。</p><p>使用以下参数执行 JMeter:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">jmeter.sh \</span><br><span class="line">--testfile JMeter_plan.jmx \</span><br><span class="line">--logfile $RESULTS_FILE \</span><br><span class="line">--reportoutputfolder $RESULTS_FOLDER \</span><br><span class="line">--reportatendofloadtests \</span><br><span class="line">--nongui \</span><br><span class="line">--forceDeleteResultFile \</span><br><span class="line">--jmeterproperty httpclient4.validate_after_inactivity&#x3D;4900 \</span><br><span class="line">--jmeterproperty httpclient4.time_to_live&#x3D;120000 \</span><br><span class="line">-Jhost&#x3D;$JMETER_HOST \</span><br><span class="line">-Jport&#x3D;$JMETER_PORT \</span><br><span class="line">-Jprotocol&#x3D;$JMETER_PROTOCOL \</span><br><span class="line">-JresourceFolder&#x3D;$JMETER_RESOURCE_FOLDER \</span><br><span class="line">-Jusers&#x3D;1000 \</span><br><span class="line">-JrampUpSecs&#x3D;5 \</span><br><span class="line">-Jloops&#x3D;10 \</span><br><span class="line">-JrequestPath&#x3D;&#x2F;testbed&#x2F;memcached</span><br></pre></td></tr></table></figure><p>重用 HTTPS 连接需要 <code>httpclient4. * *</code> 属性，否则 Keep-Alive 不会有效。</p><p>Jmeter 和 wrk 的结果与存储在 Elasticsearch 的 Logstash 一起解析，并由 Kibana 进行可视化。</p><p>Jmeter 的响应时间:<br><img src="https://user-images.githubusercontent.com/10891919/81760860-f6b4cb80-94fa-11ea-9d31-27fb43687cc5.png" alt="image"></p><p>正如你所看到的，在5月8日之前 HTTPS 的结果并不是很好。 没有重用 HTTPS 连接，每个请求都进行了 TLS 握手，尽管请求头“ Connection: keep-alive”。 因为 wrk 没有这样的问题，我在 JMeter 邮件列表中询问过，他们给了我上面提到的 httpclient4参数。 (谢谢你，菲利普 · 穆瓦德!) . 不管有没有 HttpClient 的调整，我们看到 x8664和 arm64的响应时间非常相似。 太棒了！</p><p>对于 wrk 的吞吐量测试，我使用以下参数运行它:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wrk -c96 -t8 -d30s -s &#x2F;scripts&#x2F;wrk-report-to-csv.lua $HOST:$PORT</span><br></pre></td></tr></table></figure><p>例如，8个线程将使用96个 HTTP (s)连接访问服务器30秒。</p><p>为了收集 CSV 文件中的摘要，我使用了这个自定义 Lua 脚本:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line">-- Initialize the pseudo random number generator</span><br><span class="line">-- Resource: http:&#x2F;&#x2F;lua-users.org&#x2F;wiki&#x2F;MathLibraryTutorial</span><br><span class="line">math.randomseed(os.time())</span><br><span class="line">math.random(); math.random(); math.random()</span><br><span class="line"></span><br><span class="line">local _request &#x3D; &#123;&#125;</span><br><span class="line">method &#x3D; &#39;&#39;</span><br><span class="line"></span><br><span class="line">-- Load URL config from the file</span><br><span class="line">function load_request_objects_from_file(csvFile)</span><br><span class="line">  local data &#x3D; &#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  for line in io.lines(csvFile) do</span><br><span class="line">    local idx &#x3D; string.find(line, &quot;:&quot;)</span><br><span class="line">    local key &#x3D; string.sub(line, 0, idx-1)</span><br><span class="line">    local value &#x3D; string.sub(line, idx+1, string.len(line))</span><br><span class="line">    data[key] &#x3D; value</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  return data</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function trim(s)</span><br><span class="line">  return s:match &quot;^%s*(.-)%s*$&quot;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function readMethod()</span><br><span class="line">  local method &#x3D; &#39;&#39;</span><br><span class="line">  local f &#x3D; io.open(&#39;&#x2F;data&#x2F;method.txt&#39;,&quot;r&quot;)</span><br><span class="line">  if f ~&#x3D; nil then</span><br><span class="line">    method &#x3D; f:read(&quot;*all&quot;)</span><br><span class="line">    io.close(f)</span><br><span class="line">    return trim(method)</span><br><span class="line">  else</span><br><span class="line">    print(&#39;Cannot read the method name from &#x2F;data&#x2F;method.txt&#39;)</span><br><span class="line">    os.exit(123)</span><br><span class="line">  end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function init(args)</span><br><span class="line">  local method &#x3D; readMethod()</span><br><span class="line">  -- Load request config from file</span><br><span class="line">  _request &#x3D; load_request_objects_from_file(&quot;&#x2F;data&#x2F;&quot; .. method ..&quot;.conf&quot;)</span><br><span class="line"></span><br><span class="line">  --for i, val in pairs(_request) do</span><br><span class="line">  --  print(&#39;Request:\t&#39;, i, val)</span><br><span class="line">  --end</span><br><span class="line"></span><br><span class="line">  if _request[method] ~&#x3D; nil then</span><br><span class="line">    print(&quot;multiplerequests: No requests found.&quot;)</span><br><span class="line">    os.exit()</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  print(&quot;multiplerequests: Found a &quot; .. _request.method .. &quot; request&quot;)</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">request &#x3D; function()</span><br><span class="line">  local request_object &#x3D; _request</span><br><span class="line"></span><br><span class="line">  -- Return the request object with the current URL path</span><br><span class="line">  local headers &#x3D;  &#123;&#125;</span><br><span class="line">  headers[&quot;Content-type&quot;] &#x3D; &quot;application&#x2F;json&quot;</span><br><span class="line"></span><br><span class="line">  local url &#x3D; wrk.format(request_object.method, request_object.path, headers, request_object.body)</span><br><span class="line">  return url</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function done(summary, latency, reqs)</span><br><span class="line"></span><br><span class="line">  local date_table &#x3D; os.date(&quot;*t&quot;)</span><br><span class="line">  local ms &#x3D; string.match(tostring(os.clock()), &quot;%d%.(%d+)&quot;) &#x2F; 1000</span><br><span class="line">  local hour, minute, second &#x3D; date_table.hour, date_table.min, date_table.sec</span><br><span class="line">  local year, month, day &#x3D; date_table.year, date_table.month, date_table.day</span><br><span class="line">  local timeStamp &#x3D; string.format(&quot;%04d-%02d-%02dT%02d:%02d:%02d.%03d&quot;, year, month, day, hour, minute, second, ms)</span><br><span class="line">  print(&quot;Timestamp: &quot; .. timeStamp)</span><br><span class="line"></span><br><span class="line">  local method &#x3D; readMethod()</span><br><span class="line">  file &#x3D; io.open(&#39;&#x2F;results&#x2F;today&#x2F;&#39; .. method .. &#39;.csv&#39;, &#39;w&#39;)</span><br><span class="line">  io.output(file)</span><br><span class="line"></span><br><span class="line">  -- summary</span><br><span class="line">  io.write(&quot;timeStamp,&quot;)</span><br><span class="line">  io.write(&quot;duration_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;num_requests,&quot;)</span><br><span class="line">  io.write(&quot;total_bytes,&quot;)</span><br><span class="line">  io.write(&quot;connect_errors,&quot;)</span><br><span class="line">  io.write(&quot;read_errors,&quot;)</span><br><span class="line">  io.write(&quot;write_errors,&quot;)</span><br><span class="line">  io.write(&quot;error_status_codes,&quot;)</span><br><span class="line">  io.write(&quot;timeouts,&quot;)</span><br><span class="line">  io.write(&quot;requests_per_sec,&quot;)</span><br><span class="line">  io.write(&quot;bytes_per_sec,&quot;)</span><br><span class="line">  -- latency</span><br><span class="line">  io.write(&quot;lat_min_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_max_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_mean_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_stdev_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_percentile_90_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_percentile_95_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_percentile_99_microseconds\n&quot;)</span><br><span class="line"></span><br><span class="line">  -- summary</span><br><span class="line">  io.write(string.format(&quot;%s,&quot;, timeStamp))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.duration))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.requests))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.bytes))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.connect))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.read))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.write))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.status))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.timeout))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, summary.requests&#x2F;(summary.duration &#x2F; 1000 &#x2F; 1000)))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, summary.bytes&#x2F;summary.duration))</span><br><span class="line">  -- latency</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency.min))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency.max))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency.mean))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency.stdev))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency:percentile(90.0)))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency:percentile(95.0)))</span><br><span class="line">  io.write(string.format(&quot;%.2f\n&quot;,  latency:percentile(99.0)))</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>结果显示，x86_64上的 Tomcat 比 arm64快两倍:<br><img src="https://user-images.githubusercontent.com/10891919/81760902-16e48a80-94fb-11ea-84a9-ea88b895d06e.png" alt="image"></p><p>我将试图找出这种差异的原因，并在后续的帖子中与你分享。 如果你有什么想法和建议，我很乐意试试！</p><p>祝你黑客生活愉快，注意安全！</p></div><div id="English" class="tab-content"><p>The majority of the software developers usually do not think about the CPU architecture their software will run on. I do not have official statistics but in my experience most of the software for desktop and backend applications run on x86_64 architecture (Intel and AMD processors) and most of the mobile and IoT devices run on ARM architecture. The developers write their software for the respective CPU architecture using some high level programming language and do not think what kind of Assembly instructions are being executed at runtime. And this is the purpose of the high level programming languages — to let the compiler deal with the low level hardware instructions and simplify our task to focus only on the high level business related problems.</p><p>Life is simple and beautiful but there are times when a <a href="https://www.apple.com/">big player</a> in the laptop and desktop hardware and software manufacturing comes and says that our software will have to run on a different architecture — first from <a href="https://en.wikipedia.org/wiki/Apple%27s_transition_to_Intel_processors#Timeline">PowerPC to Intel</a> and now from Intel to ARM64 (sources: <a href="https://www.bloomberg.com/news/articles/2020-04-23/apple-aims-to-sell-macs-with-its-own-chips-starting-in-2021">Bloomberg</a> &amp; <a href="https://appleinsider.com/articles/20/02/25/why-apple-will-move-macs-to-arm-and-what-consumers-get">AppleInsider</a>). Due to the lower consumption of electricity even several of the bigger cloud providers started providing ARM64 virtual machines (e.g. <a href="https://aws.amazon.com/ec2/graviton/">Amazon AWS</a>, <a href="https://www.huaweicloud.com/en-us/product/ecs.html">HuaweiCloud</a>, <a href="https://www.linaro.cloud/">Linaro</a>). And here comes the uncertainty —</p><ul><li>Will my software run on the new CPU architecture ?!</li><li>What kind of changes I will have to do to make it work ?!</li><li>Will it perform as good as before ?!</li></ul><p>To be able to answer these questions you will have to roll up your sleeves and test!</p><p>You can deploy your software on any of the cloud providers. Some of them give free trial period! Or if you are on a low budget you can experiment on <a href="https://www.raspberrypi.org/">RaspberryPi</a>.</p><p>Depending on what programming language you use to write your software you might need to do some changes or not at all! If you use an interpreted language (e.g. Python, Perl, Ruby, JVM, …) then the chances the interpreter already supports ARM64 are pretty high and you are good to go without any changes! But if your software needs to be compiled then you will need to adapt your toolchain and make sure that there are ARM64 binaries for all your dependencies! Depending on your software development stack your mileage may vary!</p><p>Once our software runs fine on the new architecture we will be able to check whether it performs as good as before. Recently some users have asked in Apache Tomcat mailing lists whether ARM64 architecture is supported. Since Apache Tomcat is written mostly in Java it “Just Works”. If you need to use libtcnative and/or mod_jk then you will need to build them yourself on ARM64. Apache Tomcat team uses TravisCI to test both <a href="https://travis-ci.org/github/apache/tomcat">Java</a> and <a href="https://travis-ci.org/github/martin-g/tomcat-connectors">C</a> code on ARM64 and there are no known issues at the moment!</p><p>To compare the performance of two versions of some software usually you will run it on the same hardware but in this case since we use different CPU architectures this makes it impossible. For my tests I have used two VMs with similar specifications:</p><ul><li><p>The x86_64 processor is:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Architecture:        x86_64</span><br><span class="line">CPU op-mode(s):      32-bit, 64-bit</span><br><span class="line">Byte Order:          Little Endian</span><br><span class="line">CPU(s):              8</span><br><span class="line">On-line CPU(s) list: 0-7</span><br><span class="line">Thread(s) per core:  2</span><br><span class="line">Core(s) per socket:  4</span><br><span class="line">Socket(s):           1</span><br><span class="line">NUMA node(s):        1</span><br><span class="line">Vendor ID:           GenuineIntel</span><br><span class="line">CPU family:          6</span><br><span class="line">Model:               85</span><br><span class="line">Model name:          Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz</span><br><span class="line">Stepping:            7</span><br><span class="line">CPU MHz:             3000.000</span><br><span class="line">BogoMIPS:            6000.00</span><br><span class="line">Hypervisor vendor:   KVM</span><br><span class="line">Virtualization type: full</span><br><span class="line">L1d cache:           32K</span><br><span class="line">L1i cache:           32K</span><br><span class="line">L2 cache:            1024K</span><br><span class="line">L3 cache:            30976K</span><br><span class="line">NUMA node0 CPU(s):   0-7</span><br><span class="line">Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat avx512_vnni md_clear flush_l1d arch_capabilities</span><br></pre></td></tr></table></figure></li><li><p>The ARM64 processor is:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Architecture:        aarch64</span><br><span class="line">Byte Order:          Little Endian</span><br><span class="line">CPU(s):              8</span><br><span class="line">On-line CPU(s) list: 0-7</span><br><span class="line">Thread(s) per core:  1</span><br><span class="line">Core(s) per socket:  8</span><br><span class="line">Socket(s):           1</span><br><span class="line">NUMA node(s):        1</span><br><span class="line">Vendor ID:           0x48</span><br><span class="line">Model:               0</span><br><span class="line">Stepping:            0x1</span><br><span class="line">BogoMIPS:            200.00</span><br><span class="line">L1d cache:           64K</span><br><span class="line">L1i cache:           64K</span><br><span class="line">L2 cache:            512K</span><br><span class="line">L3 cache:            32768K</span><br><span class="line">NUMA node0 CPU(s):   0-7</span><br><span class="line">Flags:               fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma dcpop asimddp asimdfhm</span><br></pre></td></tr></table></figure></li></ul><p>Both VMs have same amount of RAM, disk and network connectivity.</p><p>The test application is based on Spring Boot (2.2.7) running an embedded Apache Tomcat 9.0.x nightly builds and has a single REST controller that exposes a PUT endpoint for creating an entity, a GET endpoint to read it, a POST endpoint to update it and a DELETE endpoint to remove it. It uses <a href="https://memcached.org/">Memcached</a> as a database.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line">package info.mgsolutions.testbed.rest;</span><br><span class="line"></span><br><span class="line">import info.mgsolutions.testbed.domain.Error;</span><br><span class="line">import info.mgsolutions.testbed.domain.Person;</span><br><span class="line">import info.mgsolutions.testbed.domain.Response;</span><br><span class="line">import lombok.extern.slf4j.Slf4j;</span><br><span class="line">import net.rubyeye.xmemcached.MemcachedClient;</span><br><span class="line">import net.rubyeye.xmemcached.exception.MemcachedException;</span><br><span class="line">import net.rubyeye.xmemcached.transcoders.SerializingTranscoder;</span><br><span class="line">import org.springframework.http.HttpStatus;</span><br><span class="line">import org.springframework.http.MediaType;</span><br><span class="line">import org.springframework.http.ResponseEntity;</span><br><span class="line">import org.springframework.http.server.ServletServerHttpRequest;</span><br><span class="line">import org.springframework.web.bind.annotation.DeleteMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.GetMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.PostMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.PutMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.RequestBody;</span><br><span class="line">import org.springframework.web.bind.annotation.RequestMapping;</span><br><span class="line">import org.springframework.web.bind.annotation.RequestParam;</span><br><span class="line">import org.springframework.web.bind.annotation.RestController;</span><br><span class="line">import org.springframework.web.util.UriComponents;</span><br><span class="line">import org.springframework.web.util.UriComponentsBuilder;</span><br><span class="line"></span><br><span class="line">import javax.servlet.http.HttpServletRequest;</span><br><span class="line">import java.net.URI;</span><br><span class="line">import java.nio.charset.StandardCharsets;</span><br><span class="line">import java.util.Base64;</span><br><span class="line">import java.util.concurrent.TimeoutException;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * A REST endpoint that uses Memcached to get its data.</span><br><span class="line"> *&#x2F;</span><br><span class="line">@RestController</span><br><span class="line">@RequestMapping(&quot;testbed&#x2F;memcached&quot;)</span><br><span class="line">@Slf4j</span><br><span class="line">public class MemcachedTestController &#123;</span><br><span class="line"></span><br><span class="line">public static final int TTL_IN_SECONDS &#x3D; 1000;</span><br><span class="line"></span><br><span class="line">private final SerializingTranscoder coder &#x3D; new SerializingTranscoder();</span><br><span class="line">private final MemcachedClient client;</span><br><span class="line"></span><br><span class="line">public MemcachedTestController(MemcachedClient client) &#123;</span><br><span class="line">this.client &#x3D; client;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@PutMapping(value &#x3D;&quot;&quot;, consumes &#x3D; MediaType.APPLICATION_JSON_VALUE, produces &#x3D; MediaType.APPLICATION_JSON_VALUE)</span><br><span class="line">public ResponseEntity&lt;Response&gt; create(@RequestBody Person person,</span><br><span class="line">                                       HttpServletRequest servletRequest) throws InterruptedException, MemcachedException, TimeoutException &#123;</span><br><span class="line"></span><br><span class="line">final String base64Name &#x3D; base64(person.name);</span><br><span class="line">Person existing &#x3D; client.get(base64Name);</span><br><span class="line">if (existing !&#x3D; null) &#123;</span><br><span class="line">log.info(&quot;Create: Person with name &#123;&#125; already exists!&quot;, person.name);</span><br><span class="line">Error error &#x3D; new Error(&quot;Person with name &quot; + person.name + &quot; already exists!&quot;);</span><br><span class="line">return ResponseEntity.status(HttpStatus.NOT_ACCEPTABLE)</span><br><span class="line">                     .body(error);</span><br><span class="line">&#125;</span><br><span class="line">log.info(&quot;Create: Going to create &#39;&#123;&#125;&#39;&quot;, person);</span><br><span class="line">client.set(base64Name, TTL_IN_SECONDS, person, coder);</span><br><span class="line"></span><br><span class="line">ServletServerHttpRequest request &#x3D; new ServletServerHttpRequest(servletRequest);</span><br><span class="line">final UriComponents uriComponents &#x3D; UriComponentsBuilder.fromHttpRequest(request).build();</span><br><span class="line">final URI uri &#x3D; uriComponents.encode(StandardCharsets.UTF_8).toUri();</span><br><span class="line">return ResponseEntity.created(uri).contentType(MediaType.APPLICATION_JSON).body(person);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@GetMapping(value &#x3D;&quot;&quot;, consumes &#x3D; MediaType.ALL_VALUE, produces &#x3D; MediaType.APPLICATION_JSON_VALUE)</span><br><span class="line">public ResponseEntity&lt;Person&gt; get(@RequestParam String name) throws InterruptedException, MemcachedException, TimeoutException &#123;</span><br><span class="line"></span><br><span class="line">Person person &#x3D; (Person) client.get(base64(name), coder);</span><br><span class="line">if (person &#x3D;&#x3D; null) &#123;</span><br><span class="line">log.info(&quot;Get: Cannot find a person with name &#123;&#125;!&quot;, name);</span><br><span class="line">return ResponseEntity.notFound().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">log.info(&quot;Get: Found person with name: &#123;&#125;!&quot;, name);</span><br><span class="line">return ResponseEntity.ok().body(person);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@PostMapping(value &#x3D;&quot;&quot;, consumes &#x3D; MediaType.APPLICATION_JSON_VALUE, produces &#x3D; MediaType.APPLICATION_JSON_VALUE)</span><br><span class="line">public ResponseEntity&lt;Person&gt; update(@RequestBody Person person) throws InterruptedException, MemcachedException, TimeoutException &#123;</span><br><span class="line">final String name &#x3D; person.name;</span><br><span class="line">final String base64Name &#x3D; base64(name);</span><br><span class="line">final Person existing &#x3D; (Person) client.get(base64Name, coder);</span><br><span class="line">if (existing !&#x3D; null) &#123;</span><br><span class="line">log.info(&quot;Update: Going to update: &#123;&#125;&quot;, person);</span><br><span class="line">client.set(base64Name, TTL_IN_SECONDS, person, coder);</span><br><span class="line">return ResponseEntity.ok().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">log.info(&quot;Update: Cannot find a person with name &#123;&#125;!&quot;, person.name);</span><br><span class="line">return ResponseEntity.notFound().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@DeleteMapping(value &#x3D;&quot;&quot;, produces &#x3D; MediaType.APPLICATION_JSON_VALUE)</span><br><span class="line">public ResponseEntity&lt;Person&gt; delete(@RequestParam String name) throws InterruptedException, MemcachedException, TimeoutException &#123;</span><br><span class="line"></span><br><span class="line">final String base64Name &#x3D; base64(name);</span><br><span class="line">final Person person &#x3D; client.get(base64Name);</span><br><span class="line">if (person !&#x3D; null) &#123;</span><br><span class="line">log.info(&quot;Delete: Going to delete: &#123;&#125;&quot;, person);</span><br><span class="line">client.delete(base64Name);</span><br><span class="line">return ResponseEntity.ok().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">log.info(&quot;Delete: Cannot find a person with name &#123;&#125;!&quot;, name);</span><br><span class="line">return ResponseEntity.notFound().build();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private String base64(String name) &#123;</span><br><span class="line">return Base64.getEncoder().encodeToString(name.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>For load testing I have used <a href="https://jmeter.apache.org/">Apache JMeter</a> 5.2.1 and <a href="https://github.com/wg/wrk">wrk</a> from its master branch. JMeter is used for a real case scenario with 1000 simultaneous users, ramp-up period and think time between the HTTP requests. And wrk is used to test the maximal throughput.<br>JMeter is executed with these arguments:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">jmeter.sh \</span><br><span class="line">--testfile JMeter_plan.jmx \</span><br><span class="line">--logfile $RESULTS_FILE \</span><br><span class="line">--reportoutputfolder $RESULTS_FOLDER \</span><br><span class="line">--reportatendofloadtests \</span><br><span class="line">--nongui \</span><br><span class="line">--forceDeleteResultFile \</span><br><span class="line">--jmeterproperty httpclient4.validate_after_inactivity&#x3D;4900 \</span><br><span class="line">--jmeterproperty httpclient4.time_to_live&#x3D;120000 \</span><br><span class="line">-Jhost&#x3D;$JMETER_HOST \</span><br><span class="line">-Jport&#x3D;$JMETER_PORT \</span><br><span class="line">-Jprotocol&#x3D;$JMETER_PROTOCOL \</span><br><span class="line">-JresourceFolder&#x3D;$JMETER_RESOURCE_FOLDER \</span><br><span class="line">-Jusers&#x3D;1000 \</span><br><span class="line">-JrampUpSecs&#x3D;5 \</span><br><span class="line">-Jloops&#x3D;10 \</span><br><span class="line">-JrequestPath&#x3D;&#x2F;testbed&#x2F;memcached</span><br></pre></td></tr></table></figure><p>The httpclient4.** properties are needed to reuse the HTTPS connections, otherwise Keep-Alive was not effective.</p><p>The results from both JMeter and wrk are parsed with Logstash, stored in Elasticsearch and visualized by Kibana.</p><p>JMeter’s response times:<br><img src="https://user-images.githubusercontent.com/10891919/81760860-f6b4cb80-94fa-11ea-9d31-27fb43687cc5.png" alt="image"></p><p>As you can see the results for HTTPS were not very good before May 8th. The HTTPS connections were not reused and TLS handshake has been done for each request, despite request header “Connection: keep-alive”. Since there was no such issue with wrk I’ve asked at JMeter mailing lists and they gave me the httpclient4 arguments above. (Thank you, Philippe Mouawad!). With or without the HttpClient tweak we see that the response times are very similar for x86_64 and arm64.</p><p>For the throughput test with wrk I have run it with these parameters:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wrk -c96 -t8 -d30s -s &#x2F;scripts&#x2F;wrk-report-to-csv.lua $HOST:$PORT</span><br></pre></td></tr></table></figure><p>i.e. 8 threads will hit the server for 30 seconds using 96 HTTP(S) connections.</p><p>To collect the summary in a CSV file I used this custom Lua script:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line">-- Initialize the pseudo random number generator</span><br><span class="line">-- Resource: http:&#x2F;&#x2F;lua-users.org&#x2F;wiki&#x2F;MathLibraryTutorial</span><br><span class="line">math.randomseed(os.time())</span><br><span class="line">math.random(); math.random(); math.random()</span><br><span class="line"></span><br><span class="line">local _request &#x3D; &#123;&#125;</span><br><span class="line">method &#x3D; &#39;&#39;</span><br><span class="line"></span><br><span class="line">-- Load URL config from the file</span><br><span class="line">function load_request_objects_from_file(csvFile)</span><br><span class="line">  local data &#x3D; &#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  for line in io.lines(csvFile) do</span><br><span class="line">    local idx &#x3D; string.find(line, &quot;:&quot;)</span><br><span class="line">    local key &#x3D; string.sub(line, 0, idx-1)</span><br><span class="line">    local value &#x3D; string.sub(line, idx+1, string.len(line))</span><br><span class="line">    data[key] &#x3D; value</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  return data</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function trim(s)</span><br><span class="line">  return s:match &quot;^%s*(.-)%s*$&quot;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function readMethod()</span><br><span class="line">  local method &#x3D; &#39;&#39;</span><br><span class="line">  local f &#x3D; io.open(&#39;&#x2F;data&#x2F;method.txt&#39;,&quot;r&quot;)</span><br><span class="line">  if f ~&#x3D; nil then</span><br><span class="line">    method &#x3D; f:read(&quot;*all&quot;)</span><br><span class="line">    io.close(f)</span><br><span class="line">    return trim(method)</span><br><span class="line">  else</span><br><span class="line">    print(&#39;Cannot read the method name from &#x2F;data&#x2F;method.txt&#39;)</span><br><span class="line">    os.exit(123)</span><br><span class="line">  end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function init(args)</span><br><span class="line">  local method &#x3D; readMethod()</span><br><span class="line">  -- Load request config from file</span><br><span class="line">  _request &#x3D; load_request_objects_from_file(&quot;&#x2F;data&#x2F;&quot; .. method ..&quot;.conf&quot;)</span><br><span class="line"></span><br><span class="line">  --for i, val in pairs(_request) do</span><br><span class="line">  --  print(&#39;Request:\t&#39;, i, val)</span><br><span class="line">  --end</span><br><span class="line"></span><br><span class="line">  if _request[method] ~&#x3D; nil then</span><br><span class="line">    print(&quot;multiplerequests: No requests found.&quot;)</span><br><span class="line">    os.exit()</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  print(&quot;multiplerequests: Found a &quot; .. _request.method .. &quot; request&quot;)</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">request &#x3D; function()</span><br><span class="line">  local request_object &#x3D; _request</span><br><span class="line"></span><br><span class="line">  -- Return the request object with the current URL path</span><br><span class="line">  local headers &#x3D;  &#123;&#125;</span><br><span class="line">  headers[&quot;Content-type&quot;] &#x3D; &quot;application&#x2F;json&quot;</span><br><span class="line"></span><br><span class="line">  local url &#x3D; wrk.format(request_object.method, request_object.path, headers, request_object.body)</span><br><span class="line">  return url</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function done(summary, latency, reqs)</span><br><span class="line"></span><br><span class="line">  local date_table &#x3D; os.date(&quot;*t&quot;)</span><br><span class="line">  local ms &#x3D; string.match(tostring(os.clock()), &quot;%d%.(%d+)&quot;) &#x2F; 1000</span><br><span class="line">  local hour, minute, second &#x3D; date_table.hour, date_table.min, date_table.sec</span><br><span class="line">  local year, month, day &#x3D; date_table.year, date_table.month, date_table.day</span><br><span class="line">  local timeStamp &#x3D; string.format(&quot;%04d-%02d-%02dT%02d:%02d:%02d.%03d&quot;, year, month, day, hour, minute, second, ms)</span><br><span class="line">  print(&quot;Timestamp: &quot; .. timeStamp)</span><br><span class="line"></span><br><span class="line">  local method &#x3D; readMethod()</span><br><span class="line">  file &#x3D; io.open(&#39;&#x2F;results&#x2F;today&#x2F;&#39; .. method .. &#39;.csv&#39;, &#39;w&#39;)</span><br><span class="line">  io.output(file)</span><br><span class="line"></span><br><span class="line">  -- summary</span><br><span class="line">  io.write(&quot;timeStamp,&quot;)</span><br><span class="line">  io.write(&quot;duration_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;num_requests,&quot;)</span><br><span class="line">  io.write(&quot;total_bytes,&quot;)</span><br><span class="line">  io.write(&quot;connect_errors,&quot;)</span><br><span class="line">  io.write(&quot;read_errors,&quot;)</span><br><span class="line">  io.write(&quot;write_errors,&quot;)</span><br><span class="line">  io.write(&quot;error_status_codes,&quot;)</span><br><span class="line">  io.write(&quot;timeouts,&quot;)</span><br><span class="line">  io.write(&quot;requests_per_sec,&quot;)</span><br><span class="line">  io.write(&quot;bytes_per_sec,&quot;)</span><br><span class="line">  -- latency</span><br><span class="line">  io.write(&quot;lat_min_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_max_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_mean_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_stdev_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_percentile_90_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_percentile_95_microseconds,&quot;)</span><br><span class="line">  io.write(&quot;lat_percentile_99_microseconds\n&quot;)</span><br><span class="line"></span><br><span class="line">  -- summary</span><br><span class="line">  io.write(string.format(&quot;%s,&quot;, timeStamp))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.duration))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.requests))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.bytes))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.connect))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.read))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.write))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.status))</span><br><span class="line">  io.write(string.format(&quot;%d,&quot;, summary.errors.timeout))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, summary.requests&#x2F;(summary.duration &#x2F; 1000 &#x2F; 1000)))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, summary.bytes&#x2F;summary.duration))</span><br><span class="line">  -- latency</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency.min))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency.max))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency.mean))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency.stdev))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency:percentile(90.0)))</span><br><span class="line">  io.write(string.format(&quot;%.2f,&quot;, latency:percentile(95.0)))</span><br><span class="line">  io.write(string.format(&quot;%.2f\n&quot;,  latency:percentile(99.0)))</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>the results show that Tomcat on x86_64 is twice faster than on arm64:<br><img src="https://user-images.githubusercontent.com/10891919/81760902-16e48a80-94fb-11ea-84a9-ea88b895d06e.png" alt="image"></p><p>I will try to find out </p><p>what is the reason for this difference and share it with you in a follow up post. If you have any ideas I would be happy to test them!</p><p>Happy hacking and stay safe!</p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: &lt;a href=&quot;https://github.com/wangxiyuan&quot;&gt;wangxiyuan&lt;/a&gt;&lt;br&gt;作者:  &lt;a href=&quot;https://github.com/martin-g&quot;&gt;Martin Grigorov&lt;/a&gt;&lt;br&gt;原文链接:  &lt;a href=&quot;https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6&quot;&gt;https://medium.com/@martin.grigorov/compare-apache-tomcat-performance-on-x86-64-and-arm64-cpu-architectures-aacfbb0b5bb6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tomcat PMC Martin Grigorov带来的Tomcat X86 VS ARM64性能测试。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Web" scheme="https://kunpengcompute.github.io/categories/Web/"/>
    
    
      <category term="Web" scheme="https://kunpengcompute.github.io/tags/Web/"/>
    
  </entry>
  
  <entry>
    <title>跑benchmark？当心你的芯</title>
    <link href="https://kunpengcompute.github.io/2020/04/28/pao-benchmark-dang-xin-ni-de-xin/"/>
    <id>https://kunpengcompute.github.io/2020/04/28/pao-benchmark-dang-xin-ni-de-xin/</id>
    <published>2020-04-28T12:32:23.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>译者: bzhaoopenstack<br>作者: Krunal Bauskar<br>原文链接: <a href="https://mysqlonarm.github.io/Benchmarking-Mind-Your-Core/">https://mysqlonarm.github.io/Benchmarking-Mind-Your-Core/</a></p><p>最近，我们在运行基准测试时发现 MySQL 吞吐量的抖动。 即使对于普通用户来说也是如此，但是还有很多其他事情需要注意(尤其是 IO 性能瓶颈) ，以至于我们今天计划讨论的一些方面可能会被暂时省略。 在本文中，我们将讨论可能影响 MySQL 性能的一个原因。</p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><h2 id="在启用-NUMA-的-vm-machine-上的线程调度"><a href="#在启用-NUMA-的-vm-machine-上的线程调度" class="headerlink" title="在启用 NUMA 的 vm / machine 上的线程调度"></a><strong>在启用 NUMA 的 vm / machine 上的线程调度</strong></h2><p>Numa 通常是从内存分配的角度来看待的，但本文试图探讨在不同的 vCPUs上启动线程会如何大幅度地影响性能。 在我们的实验中，我们已经看到性能上升到66% 。</p><p>Mysql 有一个名为 <code>innodb_numa_interleave</code>的选项，如果启用，它将尝试在 NUMA 节点之间统一分配缓冲池。 这很好，但是工作线程呢。 这些工作线程是否在 NUMA 节点上分配过于一致？ 跨 NUMA 访问成本较高，因此最好让工作线程更接近数据，但鉴于这些工作线程的通用性，它们不应该被均匀分布。</p><p>假设我在一台24个 vCPU 机器上启动12个工作线程，这台机器上有2个 NUMA 节点，那么统一的分布预计会有6个工作线程绑定到 NUMA-node-0的 vCPU，剩下6个工作线程绑定到 NUMA-node-1的 vCPU。</p><p>操作系统(Linux)调度程序不是这样工作的。 它将尝试从一个 NUMA节点耗尽 vCPU，然后才会继续到另一个NUMA节点获取。</p><p>当工作线程(可伸缩性) &lt; 核心数，所有这些都会大大影响性能。 甚至还会看到同一测试用例的不同性能，这些都是因为CPU跨NUMA的高访问成本, OS层面调度的不均衡, 以及核心切换导致的。</p><h2 id="实验开始"><a href="#实验开始" class="headerlink" title="实验开始:"></a>实验开始:</h2><p>现在，让我们尝试看看Mysql吞吐量是如何如何由于工作线程所在的位置而更改的。</p><p>我使用同一台机器来运行client(sysbench)和server，因此client也会占用几个核心。 我们还考虑了客户端线程的位置，因为它是运行基准测试时的一个重要方面(除非你计划使用一些专用机器)。</p><ul><li>24 vCPU/48 GB VM，2 个NUMA nodes.<ul><li>NUMA-1: 0-11 vCPU/24GB</li><li>NUMA-2: 12-23 vCPU/24GB</li></ul></li><li>x86 (Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz) VM中每个物理内核有2个线程，所以24个 vCPU就是12个物理内核。 因此，我们还将探索两个工作线程位于不同的 vCPU 但处在相同物理核心的情况。</li><li>Test-Case: oltp-point-select(2 threads). 故意将其限制在两个线程内，以保持其他内核处于开放状态，从而允许操作系统执行内核切换(这会导致它独特的效果)。此外，所有测试数据都在内存中，使用 point-select 意味着没有执行 IO操作，因此 IO 瓶颈或后台线程大多处于空闲状态。 所有迭代的时间是60秒。</li><li>为了让测试更加灵活，vCPUs/cores 使用 numactl (vs taskset)绑定到 sysbench 和 mysqld。</li><li>对于服务器配置请看 <a href="https://github.com/mysqlonarm/benchmark-suites/blob/master/sysbench/conf/96tx1.5m_cpubound.cnf">here</a>.<br>Data-Size: 34G and BP: 36G. 在内存中生成测试数据并平均分布50% 的数据到 numa-0，剩下50% 的 numa-1。 Sysbench 使用range-type=uniform，这会让其触及大多数测试表的不同部分。</li></ul><table><thead><tr><th>client-threads</th><th>server-threads</th><th>tps</th></tr></thead><tbody><tr><td>Client-Threads bounded to vCPU: (0, 1, 12, 13)</td><td>Server Thread bounded to vCPUs: (2-11, 14-23)</td><td><strong>35188, 37426, 35140, 37640 37625, 35574, 35709, 37680</strong></td></tr></tbody></table><p>很显然 tps 在波动。 我做了进一步的研究，操作系统会一直继续做核心切换，导致 TPS 的波动(7% 的范围对于像这样的小测试场景来说太高了)。 此外，操作系统也同样一直在切换客户端线程核心。</p><p>这样的景象促使我对服务器核心绑定进行了更多的探索。</p><h3 id="Client和Server线程所处的位置"><a href="#Client和Server线程所处的位置" class="headerlink" title="Client和Server线程所处的位置"></a><strong>Client和Server线程所处的位置</strong></h3><table><thead><tr><th></th><th>Server-Threads: (Numa Node: 0, Physical Core: 2-5, vCPU: 4-11)</th><th>Server-Threads: (Numa Node: 0, Physical Core: 2, 3, vCPU: 4, 6)</th><th>Server-Threads: (Numa Node: 0, Physical Core: 2, vCPU: 4, 5)</th></tr></thead><tbody><tr><td>Client Threads</td><td></td><td></td><td></td></tr><tr><td>(Numa Node: 0, Physical Core: 0, vCPU: 0,1)</td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性大)做核心切换 (基于OS调度) - Client threads 在同一物理核心上 <strong>TPS: 39570, 38656, 39633</strong></td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性小)执行核心切换. - Client threads 在同一物理核心上 <strong>TPS: 39395, 39481, 39814</strong></td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性小)执行核心切换 (在同一物理内核上) - Client threads 在同一物理核心上. <strong>TPS: 39889, 40270, 40457</strong></td></tr><tr><td>(Numa Node: 0, Physical Core: 0,1, vCPU: 0,2)</td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性大)做核心切换 (基于OS调度) - Client threads 在不同物理核心上 <strong>TPS: 39890, 38698, 40005</strong></td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性小)执行核心切换. - Client threads on 在不同物理核心上. <strong>TPS: 40068, 40309, 39961</strong></td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性小)执行核心切换 (在同一物理内核上) - Client threads 在不同物理核心上. <strong>TPS: 40680, 40571, 40481</strong></td></tr><tr><td>(Numa Node: 0, Physical Core: 0, vCPU: 0)</td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性大)做核心切换 (基于OS调度) - Client threads 在同一物理核心和同一vCPU上 <strong>TPS: 37642, 39730, 35984</strong></td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性小)执行核心切换. - Client threads 在同一物理核心和同一vCPU上 <strong>TPS: 40426, 40063, 40200</strong></td><td>- Client+Server threads 处在同一NUMA节点 - Server threads (可能性小)执行核心切换 (在同一物理内核上) - Client threads 在同一物理核心和同一vCPU上 <strong>TPS: 40292, 40158, 40125</strong></td></tr><tr><td>(Numa Node: 1, Physical Core: 6, vCPU: 12,13)</td><td>- Client+Server threads 处在不同NUMA节点 - Server threads (可能性大)做核心切换 (基于OS调度) -Client threads 在同一物理核心上 <strong>TPS: 34224, 34463, 34295</strong></td><td>- Client+Server threads 处在不同NUMA节点 - Server threads (可能性小)执行核心切换. - Client threads 在同一物理核心上 <strong>TPS: 34518, 34418, 34436</strong></td><td>- Client+Server threads 处在不同NUMA节点 - Server threads (可能性小)执行核心切换 (在同一物理内核上) - Client threads 在同一物理核心上 <strong>TPS: 34282, 34512, 34583</strong></td></tr><tr><td>(Numa Node: 1, Physical Core: 6,7, vCPU: 12,14)</td><td>- Client+Server threads 处在不同NUMA节点 - Server threads (可能性大)做核心切换 (基于OS调度) -Client threads 处在不同的物理核心上 <strong>TPS: 34462, 34127, 34620</strong></td><td>- Client+Server threads 处在不同NUMA节点 - Server threads (可能性小)执行核心切换. - Client threads 处在不同的物理核心上. <strong>TPS: 34438, 34379, 34419</strong></td><td>- Client+Server threads 处在不同NUMA节点 - Server threads(可能性小)执行核心切换 (在同一物理内核上) - Client threads 处在不同的物理核心上. <strong>TPS: 34804,34453,34729</strong></td></tr><tr><td>(Numa Node: 1, Physical Core: 6, vCPU: 12)</td><td>- Client+Server threads 处在不同NUMA节点 - Server threads (可能性大)做核心切换 (基于OS调度) - Client threads处在相同的物理核心和vCPU上 <strong>TPS: 34989, 35162, 35245</strong></td><td>- Client+Server threads 处在不同NUMA节点 - Server threads (可能性小)执行核心切换. - Client threads 处在相同的物理核心和vCPU上 <strong>TPS: 35503, 35455, 35632</strong></td><td>- Client+Server threads 处在不同NUMA节点 - Server threads (可能性小)执行核心切换 (在同一物理内核上) - Client threads处在相同的物理核心和vCPU上 <strong>TPS: 35572, 35481, 35692</strong></td></tr></tbody></table><h3 id="观察结果"><a href="#观察结果" class="headerlink" title="观察结果:"></a><strong>观察结果</strong>:</h3><ul><li>限制服务器线程的核心有助于稳定性能(减少抖动)。 操作系统核心交换成本很高(如果具有不同的可伸缩性，这可能不太可行，但是是一个很好理解的点)。</li><li>将客户端线程移动到不同的 NUMA 对性能有很大影响(40K-34K)。 我没有想到会这样，因为真正的工作是由服务器工作线程完成的，所以移动客户端线程不应该影响服务器性能到这个程度(17%)。</li></ul><p>因此，从实验中我们了解到，如果客户机和服务器线程位于相同的 NUMA 上，并且使用一定工具减少OS核心交换(直到真的需要扩展核心数或者整体性能) ，则有助于实现最佳性能。</p><p>但是等等！ 我们的目标是在 NUMA 节点上平衡客户端和服务器线程的分布，以获得最佳性能。</p><h3 id="跨-NUMA-平衡客户端和服务器的线程"><a href="#跨-NUMA-平衡客户端和服务器的线程" class="headerlink" title="跨 NUMA 平衡客户端和服务器的线程"></a><strong>跨 NUMA 平衡客户端和服务器的线程</strong></h3><p>让我们应用上面获得的知识和数据来平衡 NUMA配置</p><table><thead><tr><th>client-threads</th><th>server-threads</th><th>tps</th><th>remark</th></tr></thead><tbody><tr><td>Client-Threads bounded to vCPU: 0, 1, 12, 13</td><td>Server Thread bounded to vCPUs: (2-11, 14-23)</td><td>35188, 37426, 35140, 37640, 37625, 35574, 35709, 37680</td><td>多核心交换</td></tr><tr><td>Client thread bounded to specific vCPU across NUMA (0,12)</td><td>Server Thread bounded to specific vCPU across NUMA (4,16)</td><td>30001, 36160, 24403, 24354 37708, 24478, 36323, 24579</td><td>限制核心交换</td></tr></tbody></table><p>Oops，结果比预期的还要糟糕。波动增加了。让我们来看看到底出了什么问题</p><ul><li>24K: OS 选择了threads倾斜的分布，其中 NUMA-x 运行客户端线程，而 NUMA-y 运行两个服务器线程</li><li>37K: OS 选择在每个 NUMA 运行1个客户端和1个服务器线程的情况下很好地平衡了线程分布</li></ul><p><em>(所有其他数字都是排列组合测试得到的)</em></p><p>让我们尝试一个可能的提示。 平衡 NUMA。你可以在这里<a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/sect-virtualization_tuning_optimization_guide-numa-auto_numa_balancing">here</a>了解更多</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 0 &gt; &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;numa_balancing</span><br></pre></td></tr></table></figure><table><thead><tr><th>client-threads</th><th>server-threads</th><th>tps</th><th>remark</th></tr></thead><tbody><tr><td>Client thread bounded to specific vCPUs across NUMA (0,12)</td><td>Server Thread bounded to specific vCPUs across NUMA (4,16)</td><td>33628, 34190, 35380, 37572</td><td>限制核心交换 + 禁用NUMA平衡。抖动仍然存在，但肯定比上面提到的24K 情况要好</td></tr></tbody></table><ul><li>如果我们将客户端线程绑定到特定的 NUMA上的核心并平衡服务器线程会怎样？</li></ul><table><thead><tr><th>client-threads</th><th>server-threads</th><th>tps</th><th>remark</th></tr></thead><tbody><tr><td>Client thread bounded to specific vCPU NUMA (0)</td><td>Server Thread bounded to specific vCPUs across NUMA (4,16)</td><td>36742, 36326, 36701, 36570</td><td>限制核心交换 + 禁用NUMA平衡。 看起来很平均。</td></tr><tr><td>Client thread bounded to specific vCPU NUMA (12)</td><td>Server Thread bounded to specific vCPUs across NUMA (4,16)</td><td>35440, 35667, 35748, 35578</td><td>限制核心交换 + 禁用NUMA平衡。看起来很平均。</td></tr></tbody></table><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过上面的多个实验，我们看到了给定的测试用例如何因为运行客户机和服务器线程的位置和方式得到了从24K 到40K 不等的不同性能数据。</p><p>如果你的基准测试真的只关心较低的可伸缩性，那么你应该注意核心分配。</p><p>常用的降噪策略有运行测试 n 次的平均值、 n 次的中位数、 n 次的最优值等。但是如果方差是很大的话，没有一个是最有效的。 我倾向于使用平均 n 次最小时间运行测试的策略，因此概率上的数据趋于稳定。我不确定这是否是最好的方法，但似乎它有助于将噪音到一定的程度。 较小的样本(n 值较小)会增加噪声，所以我建议 n =9至少每次运行(60 + 10(tc-warmup))秒，所以630秒的测试用例运行时间足以减少抖动。</p><p>如果你有更好的方案，请与社区分享。</p><p>另外，随着 NUMA 节点的增加和 ARM 上核心的增加，情况变得更加复杂。如果有人研究过它，我会很乐意去理解它。</p><p><em>如果你有疑问，请让我知道，我会尽力回答。</em></p></div><div id="English" class="tab-content"><h2 id="Scheduling-threads-on-NUMA-enabled-VM-Machine"><a href="#Scheduling-threads-on-NUMA-enabled-VM-Machine" class="headerlink" title="Scheduling threads on NUMA enabled VM/Machine"></a>Scheduling threads on NUMA enabled VM/Machine</h2><p>NUMA is often looked upon from a memory allocation perspective but the article tries to explore how booting thread on different vCPUs can affect performance in a big way. During our experiment we have seen performance swing upto 66%.</p><p>MySQL has an option named <code>innodb_numa_interleave</code> that if enabled will try to uniformly allocate the buffer pool across the NUMA node. This is good but what about the worker threads. Are these worker threads too uniformly allocated across the NUMA node? Cross NUMA access is costlier and so having a worker thread closer to the data is always prefered but given the generic nature of these worker threads shouldn’t they be uniformly distributed.</p><p>Say I am booting 12 worker threads on a 24 vCPU machine with 2 NUMA nodes then uniform distribution would expect 6 worker threads bound to vCPUs from NUMA-node-0 and remaining 6 to vCPUs from NUMA-node-1.</p><p>The OS (Linux) scheduler doesn’t work it that way. It would try to exhaust vCPUs from one of the NUMA-nodes and then proceed to another.</p><p>All this could affect performance big-way till your worker threads (scalability) &lt; number-of-cores. Even beyond this point one may see varying performances for the same test-case due to core-switches.</p><h2 id="Understanding-the-setup"><a href="#Understanding-the-setup" class="headerlink" title="Understanding the setup:"></a>Understanding the setup:</h2><p>Let’s now try to see how the throughput can change based on where the worker threads are located.</p><p>I am using the same machine to run client (sysbench) and server so few cores are reserved for clients too. We also consider the position of client threads as it is an important aspect while running benchmark (unless you plan to use some dedicated machine for it).</p><ul><li>24 vCPU/48 GB VM with 2 NUMA nodes.<ul><li>NUMA-1: 0-11 vCPU/24GB</li><li>NUMA-2: 12-23 vCPU/24GB</li></ul></li><li>x86 (Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz) VM has 2 threads per physical core so 24 vCPU = 12 physical cores. So we would also explore scenarios when both worker threads are located on different vCPU but have the same physical core.</li><li>Test-Case: oltp-point-select with 2 threads. Purposely limiting it to 2 threads to keep other cores open to allow the OS to do core-switches (this has its own sweet effect). Also, all data is in-memory and using point-select means no IO being done so IO bottlenecks or background threads are mostly idle. All iterations are timed for 60 seconds.</li><li>vCPUs/cores are bound to sysbench and mysqld using numactl (vs taskset) given the flexibility it provides.</li><li>For server configuration please check <a href="https://github.com/mysqlonarm/benchmark-suites/blob/master/sysbench/conf/96tx1.5m_cpubound.cnf">here</a>.<br>Data-Size: 34G and BP: 36G. Complete data in memory and equally distributed so 50% of data on numa-0 and remaining 50% of numa-1. Sysbench uses range-type=uniform that should touch most of the varied parts of the table.</li></ul><table><thead><tr><th>client-threads</th><th>server-threads</th><th>tps</th></tr></thead><tbody><tr><td>Client-Threads bounded to vCPU: (0, 1, 12, 13)</td><td>Server Thread bounded to vCPUs: (2-11, 14-23)</td><td><strong>35188, 37426, 35140, 37640 37625, 35574, 35709, 37680</strong></td></tr></tbody></table><p>Naturally the tps is fluctuating. Some closer look revealed that OS continues to do core-switch that causes TPS to fluctuate (range of 7% is too high for small test-case like this). Also, OS continues to switch client threads cores too.</p><p>That prompted me to explore more about server core binding. As part of completeness I also explored client thread positioning.</p><h3 id="Position-of-Client-and-Server-Threads"><a href="#Position-of-Client-and-Server-Threads" class="headerlink" title="Position of Client and Server Threads"></a>Position of Client and Server Threads</h3><table><thead><tr><th></th><th>Server-Threads: (Numa Node: 0, Physical Core: 2-5, vCPU: 4-11)</th><th>Server-Threads: (Numa Node: 0, Physical Core: 2, 3, vCPU: 4, 6)</th><th>Server-Threads: (Numa Node: 0, Physical Core: 2, vCPU: 4, 5)</th></tr></thead><tbody><tr><td>Client Threads</td><td></td><td></td><td></td></tr><tr><td>(Numa Node: 0, Physical Core: 0, vCPU: 0,1)</td><td>- Client+Server threads on same NUMA - Server threads may do core-switch (OS-scheduler dependent) - Client threads on the same physical core <strong>TPS: 39570, 38656, 39633</strong></td><td>- Client+Server threads on same NUMA - Server threads are less likely to do core-switch. - Client threads on the same physical core. <strong>TPS: 39395, 39481, 39814</strong></td><td>- Client+Server threads on same NUMA - Server threads less likely to do core-switch (on same physical core) - Client threads on the same physical core. <strong>TPS: 39889, 40270, 40457</strong></td></tr><tr><td>(Numa Node: 0, Physical Core: 0,1, vCPU: 0,2)</td><td>- Client+Server threads on same NUMA - Server threads may do core-switch (OS-scheduler dependent) - Client threads on different physical core <strong>TPS: 39890, 38698, 40005</strong></td><td>- Client+Server threads on same NUMA - Server threads are less likely to do core-switch. - Client threads on different physical core. <strong>TPS: 40068, 40309, 39961</strong></td><td>- Client+Server threads on same NUMA - Server threads less likely to do core-switch (on same physical core) - Client threads on different same physical core. <strong>TPS: 40680, 40571, 40481</strong></td></tr><tr><td>(Numa Node: 0, Physical Core: 0, vCPU: 0)</td><td>- Client+Server threads on same NUMA - Server threads may do core-switch (OS-scheduler dependent) - Client threads on same physical core and same vCPU <strong>TPS: 37642, 39730, 35984</strong></td><td>- Client+Server threads on same NUMA - Server threads are less likely to do core-switch. - Client threads on same physical core and same vCPU <strong>TPS: 40426, 40063, 40200</strong></td><td>- Client+Server threads on same NUMA - Server threads less likely to do core-switch (on same physical core) - Client threads on same physical core and same vCPU <strong>TPS: 40292, 40158, 40125</strong></td></tr><tr><td>(Numa Node: 1, Physical Core: 6, vCPU: 12,13)</td><td>- Client+Server threads on different NUMA - Server threads may do core-switch (OS-scheduler dependent) -Client threads on the same physical core <strong>TPS: 34224, 34463, 34295</strong></td><td>- Client+Server threads on different NUMA - Server threads are less likely to do core-switch. - Client threads on the same physical core. <strong>TPS: 34518, 34418, 34436</strong></td><td>- Client+Server threads on different NUMA - Server threads less likely to do core-switch (on same physical core) - Client threads on the same physical core. <strong>TPS: 34282, 34512, 34583</strong></td></tr><tr><td>(Numa Node: 1, Physical Core: 6,7, vCPU: 12,14)</td><td>- Client+Server threads on different NUMA - Server threads may do core-switch (OS-scheduler dependent) -Client threads on different physical core <strong>TPS: 34462, 34127, 34620</strong></td><td>- Client+Server threads on different NUMA - Server threads are less likely to do core-switch. - Client threads on different physical core. <strong>TPS: 34438, 34379, 34419</strong></td><td>- Client+Server threads on different NUMA - Server threads less likely to do core-switch (on same physical core) - Client threads on different same physical core. <strong>TPS: 34804,34453,34729</strong></td></tr><tr><td>(Numa Node: 1, Physical Core: 6, vCPU: 12)</td><td>- Client+Server threads on different NUMA - Server threads may do core-switch (OS-scheduler dependent) - Client threads on same physical core and same vCPU <strong>TPS: 34989, 35162, 35245</strong></td><td>- Client+Server threads on different NUMA - Server threads are less likely to do core-switch. - Client threads on same physical core and same vCPU <strong>TPS: 35503, 35455, 35632</strong></td><td>- Client+Server threads on same NUMA - Server threads less likely to do core-switch (on same physical core) - Client threads on different physical core and same vCPU <strong>TPS: 35572, 35481, 35692</strong></td></tr></tbody></table><h3 id="Observations"><a href="#Observations" class="headerlink" title="Observations:"></a>Observations:</h3><ul><li>Limiting cores for server threads helps stabilize the performance (reduce jitter). OS-core switches are costly (with varying scalability this may not be feasible but a good parameter to understand).</li><li>Moving client thread to different NUMA affects performance in a big way (40K -&gt; 34K). I was not expecting this since the real work is done by server worker threads so moving client threads should not affect server performance to this level (17%).</li></ul><p>So from the experiment we learned that client and server threads if co-located on the same NUMA and technique to reduce core-switch (till it is really needed with increased scalability) helps achieve optimal performance.</p><p>But wait! Our goal is to have balance distribution of client and server threads across the NUMA node to get optimal performance.</p><h3 id="Balance-Client-and-Server-Threads-across-NUMA"><a href="#Balance-Client-and-Server-Threads-across-NUMA" class="headerlink" title="Balance Client and Server Threads across NUMA"></a>Balance Client and Server Threads across NUMA</h3><p>Let’s apply the knowledge gained above to balance numa configuration</p><table><thead><tr><th>client-threads</th><th>server-threads</th><th>tps</th><th>remark</th></tr></thead><tbody><tr><td>Client-Threads bounded to vCPU: 0, 1, 12, 13</td><td>Server Thread bounded to vCPUs: (2-11, 14-23)</td><td>35188, 37426, 35140, 37640, 37625, 35574, 35709, 37680</td><td>Lot of core switches</td></tr><tr><td>Client thread bounded to specific vCPU across NUMA (0,12)</td><td>Server Thread bounded to specific vCPU across NUMA (4,16)</td><td>30001, 36160, 24403, 24354 37708, 24478, 36323, 24579</td><td>Limit core switches</td></tr></tbody></table><p>Oops it turned out to be worse than expected. Fluctuation increased. Let’s understand what went wrong</p><ul><li>24K: OS opted for skewed distribution with NUMA-x running both client threads and NUMA-y running both server threads.</li><li>37K: OS opted for well balance distribution with each NUMA running 1 client and 1 server thread.</li></ul><p><em>(All other numbers are mix of combinations)</em></p><p>Let’s try a possible hint. NUMA balancing. You can read more about it <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/sect-virtualization_tuning_optimization_guide-numa-auto_numa_balancing">here</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 0 &gt; &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;numa_balancing</span><br></pre></td></tr></table></figure><table><thead><tr><th>client-threads</th><th>server-threads</th><th>tps</th><th>remark</th></tr></thead><tbody><tr><td>Client thread bounded to specific vCPUs across NUMA (0,12)</td><td>Server Thread bounded to specific vCPUs across NUMA (4,16)</td><td>33628, 34190, 35380, 37572</td><td>Limit core switches + NUMA balancing disabled. Jitter is still there but surely better than 24K case above.</td></tr></tbody></table><ul><li>What if we bind client thread to specific numa cores and balance server threads</li></ul><table><thead><tr><th>client-threads</th><th>server-threads</th><th>tps</th><th>remark</th></tr></thead><tbody><tr><td>Client thread bounded to specific vCPU NUMA (0)</td><td>Server Thread bounded to specific vCPUs across NUMA (4,16)</td><td>36742, 36326, 36701, 36570</td><td>Limiting core switches + NUMA balancing disabled. Looks well balanced now.</td></tr><tr><td>Client thread bounded to specific vCPU NUMA (12)</td><td>Server Thread bounded to specific vCPUs across NUMA (4,16)</td><td>35440, 35667, 35748, 35578</td><td>Limit core switches + NUMA balancing disabled. Looks well balanced now.</td></tr></tbody></table><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Through multiple experiments above we saw how the given test-case can help produce different results ranging from 24K -&gt; 40K based on where and how you run client and server threads.</p><p>If your benchmark really cares about the lower scalability then you should watch out for the core allocation.</p><p>Usual strategies to reduce noise are average of N runs, median of N runs, best of N runs, etc… But if the variance is that high none of them will work best. I tend to use strategy of averaging N smaller time runs so with probabilty things could stablize. Not sure if this is best approach but seems like it help reduce the noise to quite some level. Lesser sample (smaller value of N) would increase noise so I would recommend N = 9 at-least with each run of (60+10 (tc-warmup)) secs so 630 seconds run of test-case is good enough to reduce the jitter.</p><p>If you have better alternative, please help share it with community.</p><p>BTW, story is more complex with increasing NUMA nodes and more cores on ARM. Topic of future. If anyone has studied it, would love to understand it.</p><p><em>If you have more questions/queries do let me know. Will try to answer them.</em></p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者: bzhaoopenstack&lt;br&gt;作者: Krunal Bauskar&lt;br&gt;原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/Benchmarking-Mind-Your-Core/&quot;&gt;https://mysqlonarm.github.io/Benchmarking-Mind-Your-Core/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;最近，我们在运行基准测试时发现 MySQL 吞吐量的抖动。 即使对于普通用户来说也是如此，但是还有很多其他事情需要注意(尤其是 IO 性能瓶颈) ，以至于我们今天计划讨论的一些方面可能会被暂时省略。 在本文中，我们将讨论可能影响 MySQL 性能的一个原因。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>ARM, 真的来了！</title>
    <link href="https://kunpengcompute.github.io/2020/04/24/arm-zhen-de-lai-liao/"/>
    <id>https://kunpengcompute.github.io/2020/04/24/arm-zhen-de-lai-liao/</id>
    <published>2020-04-24T09:10:49.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>译者：bzhaoopenstack<br>作者: Krunal Bauskar<br>原文链接: <a href="https://mysqlonarm.github.io/Why-ARM/">https://mysqlonarm.github.io/Why-ARM/</a></p><a id="more"></a><div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>ARM处理器无处不在。 很有可能你们中的一些人在阅读这篇博客时，使用的是 基于ARM 驱动的设备。 电话，物联网设备，家用电器，医疗设备，都是由 ARM 处理器驱动的。 ARM处理器以节能著称，因此大多数需要较长充电周期且处理器能力较低的设备一直在使用ARM芯片。</p><p>但在过去的几年里，这种情况已经发生了变化。 越来越多的 ARM 处理器被用于数据库服务器、 web 服务器、应用服务器、大数据场景等高端应用。 它们已经作为服务器芯片进入了数据中心。 在云上运行应用程序时，ARM芯片被视为一种具有成本效益的最佳选择。</p><h2 id="ARM-生态之进化"><a href="#ARM-生态之进化" class="headerlink" title="ARM 生态之进化"></a>ARM 生态之进化</h2><p>几年前，很难想象 ARM 会被用于运行在一些高端的服务器级的应用程序。 我可以想出两个主要原因:</p><ul><li>Arm 最适合小型手持设备</li><li>Arm 生态受到它所支持的特定产品的限制</li></ul><p>一些主要的操作系统提供商对 ARM 生态系统提供了支持，包括 RedHat (CentOS)、 Ubuntu、 Debian 和 Windows。 这使得主流软件可以轻松地移植到 ARM。 社区推动ARM以确保大多数主流软件都可以在 ARM架构上使用。 例如IDE、 DB-server、 Hadoop 及其衍生软件，包括 Apache 基金会、 CI/CD软件、容器、虚拟化软件等等。 .</p><p>授权其他厂商和开发自己的 ARM 处理器的 ARM 商业模式进一步推动了其受欢迎的程度，吸引了更多的芯片设计师参与其中，相互合作和创新优化。</p><p>随着亚马逊等主要云供应商开始提供基于 ARM 处理器的 EC2实例(目前只允许邀请) ，这意味着现在每个人都可以启动一个 ARM 实例，并开始在 ARM 上开发 / 移植他们想要的软件。 这非常有助于ARM生态的进一步发展。</p><h2 id="那…还缺什么"><a href="#那…还缺什么" class="headerlink" title="那…还缺什么?"></a>那…还缺什么?</h2><p>虽然大部分主流软件已经移植到 ARM 上，但它们还没有针对 ARM 进行优化。 Arm 具有弱内存模型，可以在更小的空间内安置更多的内核，底层指令的不同(软件会用这些指令)等等。 </p><p>这是 ARM 生态第二阶段的开始，社区 / 开发人员 / 用户开始从“在 ARM 上运行软件”导向“在 ARM 上优化软件”。 我认为当用户开始认真思考 ARM 并开始在 ARM 上优化他们的软件时，这是 ARM 社区乃至ARM生态的一个重大胜利(大大的里程碑)。</p><p>这(尤其是优化)是一个永无止境的过程，我看的第一个目标是至少与 x86的部分性能相当。 我故意说“ 部分性能” ，因为每个架构都有自己的 USP，所以如果你将一个企业级应用程序移植到 ARM，并且你可以以 50% 的成本(运营成本 + 初始投资)外加x86芯片的75% 性能的ARM芯片提供给客户 ，我认为这对大多数客户来说仍然是极具吸引力的(特尤其是应用程序为可伸缩的场景)。 当然，这并不意味着所有的应用程序在 ARM 上运行的速度都比较低下，事实上，有些应用程序在 ARM 上运行的速度会比 x86快，而且由于ARM生态的优化阶段在未来几年才逐步开始，经过各个玩家的努力，可以推断许多应用程序在 ARM 上运行的速度将比其他架构更快、更猛。</p><h2 id="ARM节能环保-！"><a href="#ARM节能环保-！" class="headerlink" title="ARM节能环保 ！"></a>ARM节能环保 ！</h2><p>ARM无处不在，尤其是数据中心运营商(无论大小)最关心的问题。 与其他架构相比，ARM 可以节省大约50% 的电力。 这有助于支持绿色环保倡议。</p><h2 id="ARM是下一代处理器！"><a href="#ARM是下一代处理器！" class="headerlink" title="ARM是下一代处理器！"></a>ARM是下一代处理器！</h2><p>有趣的是，我为什么这样称呼ARM。 下一代的芯片正在积极地使用 Andrino、 Raspberry Pi、 Odroid、 Banana Pi、 Asus tinker board 等工具包来构建下一代系统。 这些芯片将定义下一代计算机。 你看到它们正在使用 ARM，并且已经围绕着 ARM 成长起来，变得更加通用，在未来的几年里，将会有一大批 ARM 用户 / 开发者簇拥在一起，共建ARM生态。</p><p>一旦大量的ARM用户/开发者的支持到位，现阶段围绕 ARM正在紧锣密鼓进行的所有基础工作和有助于用户的事物将会迎来新的面貌。</p><h2 id="ARM上台式机-笔记本！"><a href="#ARM上台式机-笔记本！" class="headerlink" title="ARM上台式机 / 笔记本！"></a>ARM上台式机 / 笔记本！</h2><p>如果我们开始看到基于 ARM 的办公桌面 / PC工作站 / 笔记本电脑(已经有了！)被大家普遍使用时，ARM挤占台式机/笔记本市场也就不足为奇了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Arm 生态系统看起来非常迷人，其中充满了新的挑战和机遇。 畅想一下近十年，从微型可穿戴设备到高端电影体验，从自动驾驶助力车 / 汽车到大型喷气式飞机 / 太空飞行器，都会被基于 ARM 的处理器主宰，它将无处不在。 现阶段据估计，每人有35个基于 ARM的设备。 这就是机会的海洋，不是吗?!</p><p><em>如果您有想法，请随时发送邮件交流</em></p></div><div id="English" class="tab-content"><p>ARM processors are everywhere. It is quite likely some of you may be reading this blog from an ARM powered device. Phone, IoT devices, consumer and home appliances, health-care devices, all are powered by ARM processors. ARM processors are known to be power efficient and so most of these devices that demands a long recharge cycle but less processing power started using them.</p><p>But this has changed in the past few years. More and more ARM processors are being used for high-end applications like database server, web server, application server, big data use-cases. They have already made their way to the data-centers as a server class machines. They are being looked upon as a cost effective option while running applications in cloud.</p><h2 id="ARM-ecosystem-evolution"><a href="#ARM-ecosystem-evolution" class="headerlink" title="ARM ecosystem evolution"></a><span style="color:#4885ed">ARM ecosystem evolution</span></h2><p>Few years back it was difficult to imagine that ARM would be used for running some high-end server class applications. There were 2 major reasons that I could think off:</p><ul><li>ARM were best suited for small handheld devices.</li><li>ARM ecosystem was limited around the specific product it supported.</li></ul><p>ARM ecosystem has really picked up well after some major OS providers added support for it including RedHat (CentOS), Ubuntu, Debian, Windows. This eased out porting of the major softwares to ARM. ARM community gave it a push to make sure most of the standard softwares are available on ARM viz. IDE, DB-server, Hadoop and all its variants from Apache Foundation, CI/CD software, Container, Virtualization, etc…</p><p>The ARM model that allows other vendors to license and develop their own ARM processors further helped fueled its popularity with more chip designers joining, collaborating and innovating.</p><p>Break-through came with major cloud providers like Amazon started providing ec2 instances (currently invitation only) based on ARM processors this means now everyone can boot an ARM instance and start developing/porting their software on ARM. This helped further grow the ecosystem.</p><h2 id="What-was-missing"><a href="#What-was-missing" class="headerlink" title="What was missing?"></a><span style="color:#4885ed">What was missing?</span></h2><p>Though most of these software have been ported to ARM they were not yet optimized for ARM. ARM has a weak memory model, can fit more cores in smaller space, difference in low-level instruction (for software that uses them), etc..</p><p>This was the start of the 2nd phase of ARM where the community/developer/user started moving from “running software on arm” -&gt; “optimizing software on arm”. I think this was a major win for the arm community when users started to think ARM seriously and started spending efforts on optimizing their software on ARM.</p><p>This (especially optimization) is a never ending process but I see first goal is to at-least be on par with x86. I purposely say “onpar” because each of architecture has its own USP so say if you port an enterprise class application to ARM and you can offer it to customer @ 50% of the cost (operating cost + initial investment) for 75% of the performance of x86 I think that would be still be attractive fit for most of the customers (especially given application are horizontally scalable). Of-course that doesn’t mean all applications run on ARM at reduced speed, in fact there are applications that run on ARM faster than x86 and since the optimization phase has just started in next few years a lot of applications would be running on ARM faster than other architectures.</p><h2 id="Go-Green"><a href="#Go-Green" class="headerlink" title="Go Green"></a><span style="color:#4885ed">Go Green</span></h2><p>It is everywhere and especially a matter of major concern for data-center operators (small or big). ARM being power efficient can save approximately 50% of the power compared to other architecture. This makes it help support Go-Green initiative.</p><h2 id="ARM-is-Next-Gen-processor"><a href="#ARM-is-Next-Gen-processor" class="headerlink" title="ARM is Next-Gen processor"></a><span style="color:#4885ed">ARM is Next-Gen processor</span></h2><p>It is interesting why I referred to it this way. Next generation kids are actively using kits like Andrino, Raspberry Pi, Odroid, Banana Pi, Asus tinker board, etc…. to build some of the next-gen system. These kids will be defining the next generation of computing. Given they started with ARM their social community has grown around ARM in the next few years there would be an army of ARM users/developers with a very active community.</p><p>All the groundwork and good things that are being built at this stage around ARM will be pushed to the next level once this workforce becomes active.</p><h2 id="ARM-in-Desktop-Laptop"><a href="#ARM-in-Desktop-Laptop" class="headerlink" title="ARM in Desktop/Laptop"></a><span style="color:#4885ed">ARM in Desktop/Laptop</span></h2><p>This is catching up fast and no wonder if we start seeing ARM based Desktop/PC workstation/Laptop (there are already few) commonly being used.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><span style="color:#4885ed">Conclusion</span></h2><p>The ARM Ecosystem looks a lot more fascinating and full of new challenges and opportunities. Current decade will be ruled by ARM based processors and it will be everywhere from tiny wearable devices to high-end movie experience, from auto-driving cycle/car to jumbo jet/space-craft. It is estimated that there would be 35 active ARM power devices per person. That’s Ocean of Opportunity.</p><p><em>If you have any comments feel free to drop an email (check about section)</em></p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者：bzhaoopenstack&lt;br&gt;作者: Krunal Bauskar&lt;br&gt;原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/Why-ARM/&quot;&gt;https://mysqlonarm.github.io/Why-ARM/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>ARM: 需要了解的背景和问题</title>
    <link href="https://kunpengcompute.github.io/2020/04/24/arm-xu-yao-liao-jie-de-bei-jing-he-wen-ti/"/>
    <id>https://kunpengcompute.github.io/2020/04/24/arm-xu-yao-liao-jie-de-bei-jing-he-wen-ti/</id>
    <published>2020-04-24T07:21:27.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>译者： bzhaoopenstack<br>作者： Amit Dattatray Khandekar<br>原文链接：<a href="https://amitdkhan-pg.blogspot.com/">https://amitdkhan-pg.blogspot.com/</a></p><p>Arm 的故事始于1993年，当时苹果与 ARM (当时是 Acorn RISC Machines)组建了一家合资企业，推出了“ Apple Newton”掌上电脑。 这个故事今天还在继续，有消息称苹果将把他们的 Mac换成 ARM 处理器，在这里 ARM 仍然以节能处理器的角色出现。 这是它在智能手机中如此受欢迎，也是它进入智能汽车、无人机和其他物联网设备的主要原因，在这些设备中，电池寿命保持和硬件尽量减少热量的产生是至关重要的。 今天，甚至是数据中心也可以在 ARM 上运行。 由于市场的巨大变更，在此，我考虑抛出关于主角ARM的一些观点，这些观点对于刚刚开始接触 ARM 生态系统的用户和软件开发人员是十分有益的..</p><a id="more"></a><p>Arm 能耗较低的原因与 其所基于的 RISC的硬件架构有关。 RISC 指令非常简单，每个指令执行只需要一个CPU时钟周期; 因此它们需要更少的晶体管，更少的功率，从而产生更少的热量。</p><p>OK，是什么促使ARM 处理器开始进入数据中心市场呢？ 毕竟，移动电话和数据中心没有任何共同点。 不是吗？</p><p>好吧，两者都消耗电力，并且都需要在拟定的成本下表现良好。 尽管数据中心与手机相比是巨大的，同时它们对 CPU需求和 使用量也是巨大的。 对于两个场景功率效率非常重要，能达到市场需求的性能成本也是一样。 </p><p><strong>分而治之</strong></p><p>那么，我们就用更多的，又便宜的 ARM 处理器来替换现有的数据中心中昂贵的处理器不就好了，这样总的 CPU 功耗不就等于现有的功耗了吗？ 是的，这确实有用。 假设有4个 cpu 承载16个并行进程，那么最好的办法是将4个高性能CPU替换为8到16个性能较差的 cpu ，这样总吞吐量可能会更高。</p><p>但是，当有一个数据库长查询，需要消耗高性能 CPU 的能力时？ 即便在这个时候，这个数据库查询请求也可以使用多个 较低性能的cpu 来并行查询。 这里我们看到，即使是软件也需要适应这种范式转换: 就是尽可能地将任务划分为多个并行任务。 我们需要理解这样一个事实: 重要的不仅仅是单个 CPU 的能力，而是所有 CPU 的总能力。</p><p><strong>big.LITTLE架构</strong></p><p>在 ARM 的big.LITTLE架构中，在同一个 SoC 中可以有两个或多个具有不同性能的核。 如果其中一个处理的工作负载发生变化，如果另一个核更适合已变化的工作负载的话，那么它可以动态地接管工作负载。 这种方式避免消耗不必要的电力和热量的产生，因为ARM定位是低功耗处理器，成本成本！！ 在 linux 内核中，这种调度已经得到了支持，特别是针对 big.LITTLE架构。</p><p><strong>ARM的商业许可模式</strong></p><p>正如我们许多人知道的那样，ARM 公司本身并不生产芯片，而是设计芯片。 它的客户购买了基于 ARM 架构设计的芯片制造许可证。 目前为止，有两种这样的许可证。</p><p>一种是核心许可证。 当一家公司购买核心许可证时，它必须使用 ARM 的内部核心设计制造完整的 CPU 核心，而不能做任何更改。 Arm 授权的核心系列名为 Cortex-A * * 。 例如，在高通的 Snapdragon 855芯片组中，所有的 CPU 核心都基于 Cortex-A 系列; 这意味着它们使用了 ARM 的核心许可证。</p><p>另一个是 ARM 架构许可证。 当一家公司购买这个许可证而不是核心许可证时，它必须设计自己的核心，但核心的设计必须与 ARM 指令集兼容。 这样的核心通常被称为定制核心，因为它们有自己的微结构，而不是由 ARM 设计的。 这为某些大公司根据自己的需要生产核心提供了灵活的条件。 高通(Qualcomm)、华为(huawei)、苹果(Apple)和三星(Samsung)等公司都制造了这种定制的核心。</p><p>这种许可模式的优点在于: 现成的核心设计可供任何人使用(当然需要购买许可证)。 因此，有许多厂商都制造了兼容的芯片。 从而进一步推动了芯片创新和竞争。</p><p><strong>基于ARM的应用程序</strong></p><p>移动设备的应用程序已经基于ARM 处理器上编写了。 但是在服务器上运行的软件呢？ 好吧，Linux 内核支持 ARM，所以像 Ubuntu，CentOS 和 Debian 这样的操作系统已经正式支持 ARM 镜像。 另外，如果你使用的是 Ubuntu，在Ubuntu 官方库中几乎所有常用的 x86依赖库都已经能安装在 ARM 上了，至少对于 ARMv8来说已经做了该做的porting。 我能够安装 PostgreSQL 数据库包，并且一直在高竞争的情况下运行 pgbench，并且运行得很好。 (可能在以后的博客中，我将进一步阐述 PostgreSQL)此外，gcc / g + + 等编译器已经针对 ARM 架构进行了调优，因此大多数硬件专用编译器都已完成了ARM架构相关的优化。</p><p>但是，当涉及到那些运行在数据服务器的软件时，可能需要进行大量的适配工作才能获得比较nice的性能。 例如，在做代码同步的时候，应用程序必须特别注意ARM 的弱内存模型。 其次，应用程序应该利用内置的 ARM 硬件方面的能力，比如 NEON (这是 ARM 对 SIMD 的品牌名称) 指令，以便在多数据流上并行操作等等。</p><p>对此，目前已经有很多开发者和团队正投身于大量的研究和分析，以优化整个 ARM 生态系统中的软件。 同时我们已经看到了这个ARM生态系统正在逐渐过渡、完善和适应市场。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者： bzhaoopenstack&lt;br&gt;作者： Amit Dattatray Khandekar&lt;br&gt;原文链接：&lt;a href=&quot;https://amitdkhan-pg.blogspot.com/&quot;&gt;https://amitdkhan-pg.blogspot.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Arm 的故事始于1993年，当时苹果与 ARM (当时是 Acorn RISC Machines)组建了一家合资企业，推出了“ Apple Newton”掌上电脑。 这个故事今天还在继续，有消息称苹果将把他们的 Mac换成 ARM 处理器，在这里 ARM 仍然以节能处理器的角色出现。 这是它在智能手机中如此受欢迎，也是它进入智能汽车、无人机和其他物联网设备的主要原因，在这些设备中，电池寿命保持和硬件尽量减少热量的产生是至关重要的。 今天，甚至是数据中心也可以在 ARM 上运行。 由于市场的巨大变更，在此，我考虑抛出关于主角ARM的一些观点，这些观点对于刚刚开始接触 ARM 生态系统的用户和软件开发人员是十分有益的..&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>ARM: Points to be noted</title>
    <link href="https://kunpengcompute.github.io/2020/04/24/arm-points-to-be-noted/"/>
    <id>https://kunpengcompute.github.io/2020/04/24/arm-points-to-be-noted/</id>
    <published>2020-04-24T07:19:47.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>作者： Amit Dattatray Khandekar<br>原文链接：<a href="https://amitdkhan-pg.blogspot.com/">https://amitdkhan-pg.blogspot.com/</a></p><p>  The story of ARM began in 1993 with a joint venture of Apple with ARM (then Acorn RISC Machines) to launch the “Apple Newton” handheld PC. And the story continues today with news that Apple is going to switch their MACs to ARM processors.  What has not changed in the story is ARM’s reputation as a power-efficient processor. This is the primary reason why it is so popular in smarthphones, and why it has made its way into smart cars, drones and other internet-of-things devices where it is crucial to preserve battery life and minimize heat generation. Today even data centers can run on ARM. Due to such widespread market disruption happening, I thought about putting some specific points which I think are good-to-know for users and software developers who have just begun using the ARM ecosystem …</p><a id="more"></a><p>The reason why ARM power consumption is less has to do with the inherent nature of RISC architecture on which ARM is based. RISC instructions are so simple that each of them requires only one clock cycle to execute; so they require less transistors, and hence less power is required and less heat is generated.</p><p>Ok, but then why ARM processors started making their way into data centers? After all, mobile phones and data centers don’t have anything in common. Or do they?</p><p>Well, both consume power, and both need to perform well for a given price. Even though data centers are huge as compared to the size of a mobile phone, their CPU usage is also huge. So power efficiency is equally important. And so is the price for a given performance.</p><p><strong>Divide and conquer</strong></p><p>So, just replace the existing expensive processors with more number of cheaper ARM processors, so that the total CPU power will be equal to the existing power ? Yes, this does work. Suppose, there are 4 CPUs serving 16 parallel processes, it’s better for them to be instead served by 8 or 16 lower performing CPUs.  Overall throughput will likely be higher.</p><p>But what if there is a single long database query which needs high CPU power ? Even here, the database query can make use of multiple CPUs to run a parallelized query.  Here we see that even the software needs to adapt to this paradigm shift: divide the task into number of parallel tasks wherever possible. We need to understand the fact that more than the power of a single CPU, what counts is the total power of all the CPUs.</p><p>Another thing is that, the worloads are not always high. For instance, cloud service workloads are always mixed, frequently with numerous small tasks, where again a server with large number of low power CPUs fits well.</p><p><strong>big.LITTLE</strong></p><p>In the ARM’s big.LITTLE architecture, there can be two or more cores of different performance capacity in the same SoC. And if the workload processed by one of them changes, the other one can take over that workload on the fly if it is more suitable for the changed workload. This way unnecessary power usage and heat generation is prevented because the low-power processor type gets chosen. There has been support for doing such scheduling particularly for big.LITTLE in the linux kernel.</p><p><strong>ARM’s licensing model</strong></p><p>As many of you might know, ARM does not manufacture chips; it designs them. And it’s clients buy its license to manufacture chips based on ARM’s design. Now, there are two kinds of licenses.</p><p>One is the core license.  When a company buys the core license, it has to manufacture the complete CPU core using ARM’s in-house core design without modifying it. The ARM’s family of core designs that it licenses, are named Cortex-A**. E.g. in Qualcomm’s Snapdragon 855 chipset, all CPU cores are based on Cortex-A series; it means they used the ARM core license.</p><p>The other is the ARM architecture license. When a company buys this license and not the core license, it has to design it’s own core, but the core design has to be compatible with the ARM instruction set. Such cores are often called custom cores, because they have their own micro-architecture that is not designed by ARM. This provides flexibility to the big companies to build cores as per their own needs. Companies like Qualcomm, Huawei, Apple and Samsung have built such custom cores.</p><p>The beauty of this licensing model is : the ready-made core design is available to just anybody (of course a license has to be bought). And hence there are a number of vendors who all have manufactured compatibile chips. This drives innovation and competition.</p><p><strong>Applications</strong></p><p>Applications for mobile devices were already written from scratch on ARM processers. But what about the software running on servers ? Well, Linux kernel has support for ARM, so OSes like Ubuntu, CentOS and Debian already have officially supported ARM images. Furthermore, if you are running on, say Ubuntu, almost all the usual x86 packages that are present in the Ubuntu repository are already there for ARM as well, at least for ARMv8. I was able to install the PostgreSQL database package, and have been running pgbench with high contention, and it runs just fine. (Probably in later blogs, I will elaborate on PostgreSQL further) Also, the compilers like gcc/g++ are already tuned for ARM architecture, so most of the hardware-specific compiler optimizations are transparently done for ARM.</p><p>But when it comes to running software meant for data servers, a lot of adaptation might be required to have a reasonable performance. For instance, applications have to be aware of the implications of the ARM’s weak memory model, especially for code synchronizatoin. Secondly, they should leverage in-built ARM capabilities like NEON (which is the ARM’s brand name for SIMD) to parallelize same operation on multiple data; and so on.</p><p>A lot of research and analysis is going on to optimize sofware running in the ARM ecosystem as a whole. But we are already seeing a gradual transition and adaptation to this ecosystem.  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者： Amit Dattatray Khandekar&lt;br&gt;原文链接：&lt;a href=&quot;https://amitdkhan-pg.blogspot.com/&quot;&gt;https://amitdkhan-pg.blogspot.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;  The story of ARM began in 1993 with a joint venture of Apple with ARM (then Acorn RISC Machines) to launch the “Apple Newton” handheld PC. And the story continues today with news that Apple is going to switch their MACs to ARM processors.  What has not changed in the story is ARM’s reputation as a power-efficient processor. This is the primary reason why it is so popular in smarthphones, and why it has made its way into smart cars, drones and other internet-of-things devices where it is crucial to preserve battery life and minimize heat generation. Today even data centers can run on ARM. Due to such widespread market disruption happening, I thought about putting some specific points which I think are good-to-know for users and software developers who have just begun using the ARM ecosystem …&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>ARM vs X86 Hadoop 性能大比拼</title>
    <link href="https://kunpengcompute.github.io/2020/04/24/arm-vs-x86-hadoop-xing-neng-da-bi-pin/"/>
    <id>https://kunpengcompute.github.io/2020/04/24/arm-vs-x86-hadoop-xing-neng-da-bi-pin/</id>
    <published>2020-04-24T02:59:28.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>参与者：<a href="https://github.com/ayushtkn">Ayush Saxena</a>、<a href="https://github.com/ZhengZhenyu">郑振宇</a>、<a href="https://github.com/liusheng">刘胜</a></p><p><em>背景：2019年，本团队在 Apache Hadoop 开源社区上游完成了 ARM 平台适配并使能了 Hadoop ARM CI 以便保证 Hadoop 在后续的版本开发中均保证 ARM 平台适配，并且团队成员正在推动 Hadoop 开源社区发布 ARM 平台的软件包，以便用户可以直接下载使用。在 ARM 的基本适配工作完成后，性能调优成了我们下一步的工作重心。</em></p><p>近期，华为计算开源生态部技术团队进行了 Apache Hadoop 在 ARM 及 X86 平台上的初步性能比拼测试及简要分析，可以为关心 ARM 数据中心大数据场景的读者提供一些参考。</p><a id="more"></a><h2 id="测试环境及工具"><a href="#测试环境及工具" class="headerlink" title="测试环境及工具"></a>测试环境及工具</h2><p>本次测试使用华为云，我们在华为云上搭建了两套三节点测试集群（一套ARM，一套X86），个节点均为8vCPU 16GB RAM，但在CPU指标上，由于硬件限制，X86集群使用 <code>Intel Xeon Gold 6266C</code> 主频为  <code>3.0 GHz</code> 而 ARM 节点使用 <code>Huawei Kunpeng 920</code> 主频为 <code>2.6 GHz</code>，<strong>但从成本角度来看，ARM 节点的单价仅为 X86 节点单价的 70%</strong>，在参考性能比拼结果时，需要考虑上述指标。硬盘方面，我们使用了IOPS上限4,200，IOPS突发上限5,000的500G硬盘作为系统盘；网络方面，为虚拟机配置了5Mbit/s的带宽。</p><p>本次测试分别对上述Hadoop集群进行了业界常用的<code>TeraSort</code>测试，结合Hadoop支持的各种压缩算法，对50GB数据进行<code>TeraSort</code>对比测试，并对测试结果进行简要分析。</p><h2 id="测试数据及简要分析"><a href="#测试数据及简要分析" class="headerlink" title="测试数据及简要分析"></a>测试数据及简要分析</h2><p>首先，需要再次明确的是，本次测试所使用的CPU在主频方面并不相同，大约有13%的性能差距(2.6G Hz vs 3.0GHz)，我们可以对性能指标乘以一定的系数来进行平衡，但由于ARMv8以及X86指令集自身的不同，简单的系数并不能完全反映出实际情况。这里我们先通过简单系数的方式来进行初步的理论分析，来看一看Apache Hadoop在ARM运行在ARM数据中心上是否与运行在X86数据中心上有可比性，为读者提供一定的参考；后面我们也会继续探索更为合理和精确的对比方式。</p><p><img src="https://user-images.githubusercontent.com/10849016/80173098-5d904480-8621-11ea-9d4b-f894c5a4c1d9.png" alt="image"></p><p>从上面得测试数据可以发现，ARM平台上的表现整体比X86稍差，性能差距大约在10%-20%之间；将我们上面提到的CPU主频大约有13%的性能差距考虑在内，ARM平台上的整体表现与X86平台上的性能表现之间的差距大约在10%以内。</p><p>针对这一情况，我们进行了更进一步的分析，分析结果表明，ARM平台在<strong>Mapper</strong>阶段（包括QuickSort和压缩）表现的较X86平台要差；而在<strong>Reducer</strong>阶段（包括MergeSort和解压缩）ARM的表现较X86平台要更好；通过优化<strong>Mapper</strong>阶段的实现，ARM平台的性能有可能超越X86平台。在<strong>压缩</strong>环节，ARM耗时较X86落后的比较多，同时<code>ZSTD</code>是当前在ARM平台上表现的最好的压缩算法，与X86之间的性能差距最小。</p><p>除了Hadoop本身代码上的区别，我们还发现在ARM平台和X86平台上启动JVM的性能有所差距，这里我们单独编写了一份脚本来进行测试，在这个脚本中我们将启动<code>Mapper</code>和<code>Reducer</code>各1500个，然后退出，用于观察启动JVM的性能差距：</p><p><img src="https://user-images.githubusercontent.com/10849016/80352005-d0115680-88a5-11ea-81b8-d6ec532fb4b0.png" alt="image"></p><p>从结果可以看出，在这项对比中，ARM与X86相比也有15%-18%的性能差距，同样，CPU主频是否对该时间有影响并且有多大的影响也是需要进行进一步分析的。但是，从这个测试我们可以得到一个结论，那就是如果可以重用JVM，那么可以一定程度上缩小ARM和X86之间的性能差距。</p><h2 id="总结及后续工作"><a href="#总结及后续工作" class="headerlink" title="总结及后续工作"></a>总结及后续工作</h2><ol><li>总体来说，Apache Hadoop运行在ARM数据中心上与运行在X86数据中心上在性能上约有10%到20%的性能差距，将CPU主频差别考虑进去后，ARM与X86的整体性能差距基本在10%以内。</li><li>ARM在Mapper阶段性能较X86差，但是在Reducer阶段较X86性能好；更进一步来看，ARM在Mapper阶段的数据压缩阶段性能与X86相比差距较为明显，其中ZSTD的差距最小。</li><li>ARM和X86目前在启动JVM方面约有15%-18%的性能差距，如果可以在MR中重用JVM，那么可以一定程度上缩小ARM和X86之间的性能差距。</li><li>从性价比角度考虑，当前测试所使用的ARM集群成本仅为X86集群成本的70%，但性能差距只有10%-20%。可以预见，同等成本的ARM集群将能够提供与X86集群同等性能、甚至更好的性能，因此ARM数据中心在大数据领域还是十分有实用价值的。</li></ol><p>后续，我们将持续进行更为细致的测试，并且根据测试结果推动社区上游进行有针对性的改进，逐步缩小ARM与X86之间的性能差距。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参与者：&lt;a href=&quot;https://github.com/ayushtkn&quot;&gt;Ayush Saxena&lt;/a&gt;、&lt;a href=&quot;https://github.com/ZhengZhenyu&quot;&gt;郑振宇&lt;/a&gt;、&lt;a href=&quot;https://github.com/liusheng&quot;&gt;刘胜&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;背景：2019年，本团队在 Apache Hadoop 开源社区上游完成了 ARM 平台适配并使能了 Hadoop ARM CI 以便保证 Hadoop 在后续的版本开发中均保证 ARM 平台适配，并且团队成员正在推动 Hadoop 开源社区发布 ARM 平台的软件包，以便用户可以直接下载使用。在 ARM 的基本适配工作完成后，性能调优成了我们下一步的工作重心。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;近期，华为计算开源生态部技术团队进行了 Apache Hadoop 在 ARM 及 X86 平台上的初步性能比拼测试及简要分析，可以为关心 ARM 数据中心大数据场景的读者提供一些参考。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://kunpengcompute.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="https://kunpengcompute.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Building Linux packages for different CPU architectures with Docker and QEMU</title>
    <link href="https://kunpengcompute.github.io/2020/04/23/building-linux-packages-for-different-cpu-architectures-with-docker-and-qemu/"/>
    <id>https://kunpengcompute.github.io/2020/04/23/building-linux-packages-for-different-cpu-architectures-with-docker-and-qemu/</id>
    <published>2020-04-23T03:00:37.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>作者: Martin Grigorov</p><p>原文链接: <a href="https://medium.com/@martin.grigorov/building-linux-packages-for-different-cpu-architectures-with-docker-and-qemu-d29e4ebc9fa5">https://medium.com/@martin.grigorov/building-linux-packages-for-different-cpu-architectures-with-docker-and-qemu-d29e4ebc9fa5</a></p><p>Many Linux open source projects provide only source code releases. To be able to use them the users need to download the source code and to build it, usually by executing steps like: <code>./configure</code>,<code>make</code> and <code>make install</code>.</p><p>Some users prefer this way because they have the chance to configure the software by passing specific arguments to the <code>./configure</code> script. It is also the preferred way from security point of view — the person responsible for managing the system is certain that this is the original version of the source code and no one added anything on top.</p><a id="more"></a><p>Still many users prefer to download a binary package and install it, or to use the package management software of their favorite Linux distribution, e.g. yum for RedHat/CentOS/Fedora or apt for Debian/Ubuntu flavors. Here the benefit is that the dependencies are installed automatically for you.</p><p>Some of the open source projects provide binary packages for download themselves. Others delegate the packaging task to their community or to the Linux distributions to package the software following the best practices for the specific package type. They do this for different reasons but most often because:</p><p>1) it is an extra burden — the software developers do not want to deal with “bureaucracy” different their domain of expertise</p><p>2) there are many Linux distributions with their specific packaging types, e.g. .deb, .rpm, .apk, etc. One needs to read a lot of documentation to understand each of them</p><p>3) another reason is because one may need specific hardware to be able to build a package for not so common CPU architectures. Usually developers work on Intel or AMD based computers known as x86_64 CPU architecture. But if your software needs to run on mobile phones or tables and Internet of Things (IoT) devices then you need to produce a binary for ARM architecture, also known as AARCH. ARMv7 and before is 32-bit. ARMv8, also known as aarch64, is 64-bit. Lately even more and more cloud providers recommend ARM64 CPUs because they have similar performance to the x86_64 ones but consume less electricity, so they are cheaper to rent and environment friendlier.</p><p>In the rest of this article I’m going to show you how to build and package your software for ARM on x86_64 computer by using Docker and QEMU.</p><h2 id="What-is-Docker"><a href="#What-is-Docker" class="headerlink" title="What is Docker ?"></a>What is Docker ?</h2><p>From Wikipedia: Docker is a set of platform as a service (PaaS) products that uses OS-level virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels. All containers are run by a single operating system kernel and therefore use fewer resources than virtual machines.</p><h2 id="What-is-QEMU"><a href="#What-is-QEMU" class="headerlink" title="What is QEMU ?"></a>What is QEMU ?</h2><p>From Wikipedia: QEMU (short for Quick EMUlator) is a free and open-source emulator that performs hardware virtualization. QEMU is a hosted virtual machine monitor: it emulates the machine’s processor through dynamic binary translation and provides a set of different hardware and device models for the machine, enabling it to run a variety of guest operating systems. It also can be used with KVM to run virtual machines at near-native speed (by taking advantage of hardware extensions such as Intel VT-x). QEMU can also do emulation for user-level processes, allowing applications compiled for one architecture to run on another.</p><p>Most of the cloud based Continuous Integration (CI) providers (e.g. TravisCI, CircleCI, DroneCI, Github Actions, and more) use Docker to provide you with a throw-away Docker container (a Linux instance) which you can modify the way you need, for example by installing required dependencies of your software or by even changing kernel settings, and then to build/test/package your software. Once your CI job finishes the docker container is discarded and the resources freed for the new CI job. The jobs are fully isolated from each other and this makes your build reproducible because they always start from the same state and there is nothing left from a previous job.</p><h2 id="Building"><a href="#Building" class="headerlink" title="Building"></a>Building</h2><p>The process of building your software consists of two main steps:</p><h3 id="1-register-QEMU-binfmt"><a href="#1-register-QEMU-binfmt" class="headerlink" title="1) register QEMU/binfmt"></a>1) register QEMU/binfmt</h3><p>If you try to run a Docker container that is built for a different CPU architecture than the host’s it will fail with this error:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -it — rm arm64v8&#x2F;centos:8 uname -m</span><br><span class="line">standard_init_linux.go:211: exec user process caused “exec format error”</span><br></pre></td></tr></table></figure><p>To be able to run such foreign architectures one may use QEMU! Someone made the installation step as simple as executing:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -it — rm — privileged multiarch&#x2F;qemu-user-static — credential yes — persistent yes</span><br></pre></td></tr></table></figure><p>What this does is:</p><p> 1.2) run a Docker container that modifies the host. If it is executed inside a Docker container then it will modify the outer container.<br> 1.3) The — privileged argument gives permissions to the Docker container to modify the host. In case it is run in a CI server then the host is the outer Docker container, the one allocated for our CI job.<br> 1.4) The — credential yes argument is needed only if you need to use sudo later in step 2).<br> 1.5) The — persistent yes argument tells it to load the interpreter when binfmt is configured and remains in memory. All future uses clone the interpreter from memory.</p><p>If the execution of the command above is successful you should see output similar to the following:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-alpha-static as binfmt interpreter for alpha</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-arm-static as binfmt interpreter for arm</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-armeb-static as binfmt interpreter for armeb</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-sparc-static as binfmt interpreter for sparc</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-sparc32plus-static as binfmt interpreter for sparc32plus</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-sparc64-static as binfmt interpreter for sparc64</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-ppc-static as binfmt interpreter for ppc</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-ppc64-static as binfmt interpreter for ppc64</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-ppc64le-static as binfmt interpreter for ppc64le</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-m68k-static as binfmt interpreter for m68k</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-mips-static as binfmt interpreter for mips</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-mipsel-static as binfmt interpreter for mipsel</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-mipsn32-static as binfmt interpreter for mipsn32</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-mipsn32el-static as binfmt interpreter for mipsn32el</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-mips64-static as binfmt interpreter for mips64</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-mips64el-static as binfmt interpreter for mips64el</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-sh4-static as binfmt interpreter for sh4</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-sh4eb-static as binfmt interpreter for sh4eb</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-s390x-static as binfmt interpreter for s390x</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-aarch64-static as binfmt interpreter for aarch64</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-aarch64_be-static as binfmt interpreter for aarch64_be</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-hppa-static as binfmt interpreter for hppa</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-riscv32-static as binfmt interpreter for riscv32</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-riscv64-static as binfmt interpreter for riscv64</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-xtensa-static as binfmt interpreter for xtensa</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-xtensaeb-static as binfmt interpreter for xtensaeb</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-microblaze-static as binfmt interpreter for microblaze</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-microblazeel-static as binfmt interpreter for microblazeel</span><br><span class="line">Setting &#x2F;usr&#x2F;bin&#x2F;qemu-or1k-static as binfmt interpreter for or1k</span><br></pre></td></tr></table></figure><p>Now if we try to run the foreign Docker image it will succeed:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -it — rm arm64v8&#x2F;centos:8 uname -m</span><br><span class="line">aarch64</span><br></pre></td></tr></table></figure><p>The above tells us that uname -m executed inside arm64v8/centos:8 container returns that the CPU architecture is aarch64!</p><p>If you want to understand how QEMU/binfmt works you can read its documentation but it is not required to know more for the purpose of this article.</p><h3 id="2-Build-your-software"><a href="#2-Build-your-software" class="headerlink" title="2) Build your software"></a>2) Build your software</h3><p>All we need to do now is to run the usual build steps (e.g. ./configure, make, make test, etc.) inside the foreign Docker container.</p><p>2.1) Create a Dockerfile that uses as a base image any image with foreign architecture, like arm64v8/centos:8 above.<br>2.2) run the scripts</p><p>One can use Docker’s RUN commands, e.g.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RUN make</span><br><span class="line">RUN make test</span><br></pre></td></tr></table></figure><p>but this may get wild if you need to execute many steps!</p><p>I prefer to put all these commands in a Shell script, copy it to the custom Docker image and finally execute it.</p><p>The script may look like this:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;env bash</span><br><span class="line">apt install -y dependency1 dependency2 dependencyN</span><br><span class="line">.&#x2F;configure</span><br><span class="line">make</span><br><span class="line">make test</span><br></pre></td></tr></table></figure><p>The Dockerfile will look something like:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FROM arm64v8&#x2F;centos:8</span><br><span class="line">ADD build-test-and-package.sh &#x2F;usr&#x2F;bin</span><br><span class="line">CMD [“build-test-and-package.sh”]</span><br></pre></td></tr></table></figure><h3 id="2-3-Build-the-custom-image"><a href="#2-3-Build-the-custom-image" class="headerlink" title="2.3) Build the custom image"></a>2.3) Build the custom image</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker build -t my-arm-centos:8 .</span><br></pre></td></tr></table></figure><h3 id="2-4-Run-it"><a href="#2-4-Run-it" class="headerlink" title="2.4) Run it"></a>2.4) Run it</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run — rm -it -v $(pwd):&#x2F;my-software my-arm-centos:8</span><br></pre></td></tr></table></figure><p>Here we mount the current folder into /my-software folder inside the Docker container. build-test-and-package.sh needs to know this location to cd into it!</p><p>Any result files, like the binary packages, could be saved in this folder or another mounted folder so that they can be consumed at the end of the CI workflow, e.g. to store them as artifacts of the build and copy them to AWS S3 or elsewhere.</p><h2 id="In-action"><a href="#In-action" class="headerlink" title="In action"></a>In action</h2><p>You can see all this in action at Varnish Cache GitHub repository.</p><p>It is a Pull Request suggesting to build, test and package Varnish Cache for CentOS 7 &amp; 8, Ubuntu 16.04 &amp; 18.04, Debian 8, 9 &amp; 10, and Alpine 3, for both x86_64 and aarch64.</p><p>Expending it for more distros, versions or CPU architectures is as easy as adding an additional CircleCI job with the proper parameters.</p><p>The build graph looks like:<br><img src="https://user-images.githubusercontent.com/1736354/80073339-33377c00-857a-11ea-869e-4abbc3949086.png" alt="graph"></p><p>The dist and tar_pkg_tools jobs run first in parallel. The dist job packages the source distribution and tar_pkg_tools gets the packaging recipes for RPM, DEB and APK from here. Those are stored in CircleCI’s workflow workspace and made available for any following jobs.</p><p>Once both of them finish the next jobs that run in parallel are the distcheck and the package jobs. distcheck jobs build Varnish Cache on different distros and the package jobs build the respective binary packages for each arch/distro/version triple.</p><p>If everything is successful finally the collect_packages job exports all binary packages as CircleCI artifacts which are later copied to Package Cloud.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Using stable tools like Docker and QEMU makes it easier to build and test our software for different CPU architectures.</p><p>There are few drawbacks though:</p><p>1) it is an emulation of the foreign CPU architecture, so it is slower than doing it on a real hardware<br><strong>Note:</strong> Some cloud CI services like TravisCI and DroneIO provide support for ARM/ARM64. I have experience only with TravisCI and it is not faster than QEMU. I’ve had some small issues with it but it was easy to work them around. Hopefully it will become even better in the near future!</p><p>2) you need to find a good base Docker image for the CPU architecture you need to support. There are many images at DockerHub but depending on how exotic your needs are it may be harder to find one.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: Martin Grigorov&lt;/p&gt;
&lt;p&gt;原文链接: &lt;a href=&quot;https://medium.com/@martin.grigorov/building-linux-packages-for-different-cpu-architectures-with-docker-and-qemu-d29e4ebc9fa5&quot;&gt;https://medium.com/@martin.grigorov/building-linux-packages-for-different-cpu-architectures-with-docker-and-qemu-d29e4ebc9fa5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Many Linux open source projects provide only source code releases. To be able to use them the users need to download the source code and to build it, usually by executing steps like: &lt;code&gt;./configure&lt;/code&gt;,&lt;code&gt;make&lt;/code&gt; and &lt;code&gt;make install&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Some users prefer this way because they have the chance to configure the software by passing specific arguments to the &lt;code&gt;./configure&lt;/code&gt; script. It is also the preferred way from security point of view — the person responsible for managing the system is certain that this is the original version of the source code and no one added anything on top.&lt;/p&gt;
    
    </summary>
    
    
      <category term="虚拟化" scheme="https://kunpengcompute.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="https://kunpengcompute.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
      <category term="Packaging" scheme="https://kunpengcompute.github.io/tags/Packaging/"/>
    
  </entry>
  
  <entry>
    <title>ARM上跑Mysql, 能行吗？</title>
    <link href="https://kunpengcompute.github.io/2020/04/22/arm-shang-pao-mysql-neng-xing-ma/"/>
    <id>https://kunpengcompute.github.io/2020/04/22/arm-shang-pao-mysql-neng-xing-ma/</id>
    <published>2020-04-22T05:00:46.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>译者：bzhaoopenstack</p><p>作者: Krunal Bauskar </p><p>原文链接: <a href="https://mysqlonarm.github.io/Running-MySQL-on-ARM/">https://mysqlonarm.github.io/Running-MySQL-on-ARM/</a></p><p>我相信在座的大多数都会有这个问题。事实上，在我开始主动开始#mysqlonarm 相关工作之前，我也有过同样的经历。在ARM上运行MySQL需要什么？真的有用吗？包依赖关系呢？它有什么样的表现？支持什么功能呢？社区支持这么玩吗？还有很多悬而未决的问题。。</p><p>让我们试着用简单的问答形式来回答这些问题。</p><a id="more"></a><h4 id="Q-Mysql支持ARM架构吗"><a href="#Q-Mysql支持ARM架构吗" class="headerlink" title="Q: Mysql支持ARM架构吗?"></a>Q: Mysql支持ARM架构吗?</h4><p>A:支持，Mysql官方支持ARM。可以从mysql.com站点下载可用的软件包。</p><h4 id="Q-支持啥OS"><a href="#Q-支持啥OS" class="headerlink" title="Q: 支持啥OS?"></a>Q: 支持啥OS?</h4><p>A: 目前支持RHEL-7 &amp; 8/Oracle-Linux- 7 &amp; 8，还没有发现发布其他OS的包。</p><h4 id="Q-我们能在ARM架构下从源代码在OS上-如ubuntu-构建Mysql吗"><a href="#Q-我们能在ARM架构下从源代码在OS上-如ubuntu-构建Mysql吗" class="headerlink" title="Q: 我们能在ARM架构下从源代码在OS上(如ubuntu)构建Mysql吗?"></a>Q: 我们能在ARM架构下从源代码在OS上(如ubuntu)构建Mysql吗?</h4><p>A: 可以，能玩。我一直在源码构建二进制包，使用的是当前mysql的release tag mysql-8.0.19。同样也可以在CentOS上玩。这也意味着所有需要的包依赖性问题都得到了解决，或已经可用。</p><h4 id="Q-ARM上的工具链可用吗"><a href="#Q-ARM上的工具链可用吗" class="headerlink" title="Q: ARM上的工具链可用吗?"></a>Q: ARM上的工具链可用吗?</h4><p>A: 因为软件包是可用的，而且我能够从源代码构建它，所以默认的应用程序工具，如mysql shell/mysqladmin/mysqlslap/mysqldump/etc…，以及大量其他的默认程序都随二进制文件一起发布了。如果你关心某个特定的工具，告诉我，我会检查它们。现在，我尝试了一些我认为比较重要的工具，它们工作正常。</p><h4 id="Q-MariaDB和Percona在ARM上是否支持各自的服务器规格"><a href="#Q-MariaDB和Percona在ARM上是否支持各自的服务器规格" class="headerlink" title="Q: MariaDB和Percona在ARM上是否支持各自的服务器规格?"></a>Q: MariaDB和Percona在ARM上是否支持各自的服务器规格?</h4><p>A: MariaDB Community Server软件包（来自MariaDB公司）在ARM (CentOS7/Ubuntu-16.04/18.04)上可用，MariaDB服务器工具在ARM上暂未正式发布。 Percona尚未正式支持ARM ，但我能够从源代码构建它（ MyRocks / TokuDB不可用）。</p><h4 id="Q-如果工具不可用。这能阻止我在ARM上尝试MySQL-或其衍生软件-吗？"><a href="#Q-如果工具不可用。这能阻止我在ARM上尝试MySQL-或其衍生软件-吗？" class="headerlink" title="Q: 如果工具不可用。这能阻止我在ARM上尝试MySQL(或其衍生软件)吗？"></a>Q: 如果工具不可用。这能阻止我在ARM上尝试MySQL(或其衍生软件)吗？</h4><p>A: 不会，因为大多数工具都是符合mysql标准的，不是所谓的强绑定特定平台。所以你当然既可以把它们安装在X86，同时安装在ARM上(除非工具还没有移植到ARM上)。</p><h4 id="Q-社区支持这么玩吗"><a href="#Q-社区支持这么玩吗" class="headerlink" title="Q: 社区支持这么玩吗?"></a>Q: 社区支持这么玩吗?</h4><p>A: MySQL on ARM 已经有一段时间了。ARM、高通、华为等多家厂商积极贡献，Mysql社区发展迅速。在对Mysql ARM优化方面，社区呈现很大的兴趣，非常多的开发者想参与其中。挑战极具吸引力，然而最麻烦的是ARM硬件资源短缺。如果你有兴趣参与，和我聊聊（给我发一封电子邮件）。</p><h4 id="Q-看起来都还行。性能咋样"><a href="#Q-看起来都还行。性能咋样" class="headerlink" title="Q: 看起来都还行。性能咋样?"></a>Q: 看起来都还行。性能咋样?</h4><p>A: 这是一个大话题，所以我会在未来几天内发布多个关于这个话题的博文，但把它在一定范围内的表现是可比的。另一方面，ARM实例提供更好的价格比。</p><h4 id="Q-可以从哪里获取帮助"><a href="#Q-可以从哪里获取帮助" class="headerlink" title="Q:可以从哪里获取帮助?"></a>Q:可以从哪里获取帮助?</h4><p>由于软件包可以从MySQL官方下载，我假定他们的服务提供也应涵盖ARM。和MariaDB一样。当然，除了官方支持之外，还有普通团体和独立开发者。</p><h4 id="Command-to-build-MySQL-on-ARM"><a href="#Command-to-build-MySQL-on-ARM" class="headerlink" title="Command to build MySQL on ARM"></a>Command to build MySQL on ARM</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cmake .. -DWITH_NUMA&#x3D;1 -DDOWNLOAD_BOOST&#x3D;1 -DWITH_BOOST&#x3D;&lt;boost-dir&gt; -DCMAKE_INSTALL_PREFIX&#x3D;&lt;dir-to-install&gt;</span><br><span class="line">make -j &lt;num-of-cores&gt;</span><br></pre></td></tr></table></figure><p>因此，在ARM上构建Mysql不需要什么特殊的flag。(假设你已经安装了标准依赖). 它会默认使用”CMAKE_BUILD_TYPE=RelWithDebInfo”来编译。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>MySQL on ARM已为事实，并且得到了日益增长的生态系统/社区来支持。可以来试试。当你不考虑性能或功能时，它可以成为你节约成本这个大目标的必选项。</p><p><em>如果你还有问题/疑问，请告诉我。我会尝试回答</em></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者：bzhaoopenstack&lt;/p&gt;
&lt;p&gt;作者: Krunal Bauskar &lt;/p&gt;
&lt;p&gt;原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/Running-MySQL-on-ARM/&quot;&gt;https://mysqlonarm.github.io/Running-MySQL-on-ARM/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我相信在座的大多数都会有这个问题。事实上，在我开始主动开始#mysqlonarm 相关工作之前，我也有过同样的经历。在ARM上运行MySQL需要什么？真的有用吗？包依赖关系呢？它有什么样的表现？支持什么功能呢？社区支持这么玩吗？还有很多悬而未决的问题。。&lt;/p&gt;
&lt;p&gt;让我们试着用简单的问答形式来回答这些问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>ARM vs X86 Mysql性能大比拼</title>
    <link href="https://kunpengcompute.github.io/2020/04/22/arm-vs-x86-mysql-xing-neng-da-bi-pin/"/>
    <id>https://kunpengcompute.github.io/2020/04/22/arm-vs-x86-mysql-xing-neng-da-bi-pin/</id>
    <published>2020-04-22T04:25:50.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>译者：bzhaoopenstack</p><p>原文链接：<a href="https://mysqlonarm.github.io/MySQL-on-x86-vs-ARM/">https://mysqlonarm.github.io/MySQL-on-x86-vs-ARM/</a></p><p>作者: Krunal Bauskar</p><p>在开始探索这一领域之前，我相信大家对这个话题都会非常感兴趣，包括本人在内。另外，在深入讨论具体的数据前，让我们先来了解一下两种架构之间的基本差异。除了CISC和RISC之外，让我们从Mysql的角度来看待这些重要的差异。</p><ul><li>强 vs  弱内存模型 (弱内存模型在无锁写场景下需要适当的内存屏障).</li><li>底层硬件特定专用指令。例如：两者现在都支持crc32c硬件指令，但是底层调用它们的方式不同。更多指令差异请参看x86-SSE/ARM-ACLE。</li><li>Cache Line 差异。 大多数ARM处理器倾向于使用更大的cacheline size (所有缓存以128 bytes为单位或者 64/128字节混合的方式)。</li><li>其他系统调用级别的差异，如：ARM的PAUSE指令缺失和具有非常低延迟的替代指令无法引导硬件达到所需的延迟，sched_getcpu在ARM上引入了使用无锁构造的挑战，内存操作似乎带来更高的延迟等。</li></ul><p>Mysql社区已经为这个领域的问题贡献了多个补丁（另一个博客的主题）。自从MySQL刚刚宣称开始支持ARM架构以来，几乎没有什么优化，大部分工作仍然没有完成。</p><a id="more"></a><h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><p>现在让我们来看看最重要的方面：性能</p><p>我们在x86和ARM上测试了MySQL（当前版本8.0.19）的性能。试验步骤和机器详情如下。</p><h3 id="测试步骤"><a href="#测试步骤" class="headerlink" title="测试步骤"></a>测试步骤</h3><ul><li>24核/48GB Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz，用于在x86上运行MySQL数据库软件。</li><li>24核/48GB ARM @ 2.60GHz，用于在ARM上运行MySQL</li><li>sysbench运行在一台专用的机器，且位于相同的数据中心。</li><li>sysbench步骤:<ul><li>加载测试表. (相同的数据库被多次运行重用，所以需要预热).</li><li>Checksum预热。对所有表执行checksum。对于checksum,流程中需要获取bufferpool中的行记录。</li><li>Query 预热。 如果您使用的是自适应哈希索引，可以跳过此节，但它确实有些效果。</li><li>执行测试套件 (oltp-read-write/oltp-update-index/oltp-update-non-index/oltp-read-only/oltp-point-select)</li><li>每个测试套件以多种不同的扩展性来执行。如对于24 vCPU，尝试多threads 1/2/4/8/16/32/128/256.</li><li>在切换测试套件中间，引入一些睡眠操作确保之前的测试套件完全运行完毕。虽然这不能保证所有的数据库刷新都完成了，但是X秒的睡眠可以保证对后续测试套件的影响最小。</li><li>MySQL-Server Configuration:<ul><li>足够大的BP(bufferpool) ，以保证可以存储完整的数据</li><li>了解更多配置详情，参看 <a href="https://github.com/mysqlonarm/benchmark-suites/blob/master/sysbench/conf/96tx1.5m_cpubound.cnf">以下配置</a></li></ul></li></ul></li></ul><p>运行脚本和调用sysbench的自动化测试脚本细节在 <a href="https://github.com/mysqlonarm/benchmark-suites">这里</a></p><h3 id="测试运行具体细节"><a href="#测试运行具体细节" class="headerlink" title="测试运行具体细节:"></a>测试运行具体细节:</h3><ul><li>Table: 96-tables * 150万数据 (data-size= 34GB)</li><li>Buffer Pool: 36GB</li><li>Redo-Log: 4GB*2</li><li>TC-run-time: 300 secs</li><li>TC-warmup: 60 (sysbench –warmup-time)</li><li>workload-query-based warmup: 600</li><li>change-over-sleep: 180</li><li>checksum-based-warmup: enabled</li><li>data-storage: 300GB (支持16500 IOPS(对Burst IOPS无效果).)</li></ul><p><em>注: Frequency Scaling (FS)频率缩放. 所述ARM 主频规格2.6 GHz vs x86主频规格3.0 GHz。直接比较它们的数据是不公平的。为了补偿频率差，下面的图还为ARM添加了频率调整 tps/qps（ARM-fscaled简单地按(3/2.6）的算法基于原始数据算出ARM tps/qps数据)。在现实生活中，考虑到CPU频率的增加会影响争用曲线图和等待周期，这个影响因素可能会稍微高一些。</em></p><hr><h3 id="1-Point-Select"><a href="#1-Point-Select" class="headerlink" title="1. Point Select:"></a>1. Point Select:</h3><p><a href="https://camo.githubusercontent.com/929493df33b60738e09eee164e57181f3945e935/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d70732e706e67"><img src="https://camo.githubusercontent.com/929493df33b60738e09eee164e57181f3945e935/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d70732e706e67" alt="img"></a></p><table><thead><tr><th>threads</th><th>ARM (qps)</th><th>x86 (qps)</th><th>ARM (qps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>6696</td><td>6439</td><td>7726</td><td>4</td><td>20</td></tr><tr><td>2</td><td>12482</td><td>11774</td><td>14402</td><td>6</td><td>22</td></tr><tr><td>4</td><td>23881</td><td>21308</td><td>27555</td><td>12</td><td>29</td></tr><tr><td>8</td><td>45993</td><td>42110</td><td>53069</td><td>9</td><td>26</td></tr><tr><td>16</td><td>88517</td><td>81239</td><td>102135</td><td>9</td><td>26</td></tr><tr><td>32</td><td>142974</td><td>136724</td><td>164970</td><td>5</td><td>21</td></tr><tr><td>64</td><td>198839</td><td>212484</td><td>229430</td><td>-6</td><td>8</td></tr><tr><td>128</td><td>217778</td><td>241555</td><td>251282</td><td>-10</td><td>4</td></tr><tr><td>256</td><td>209797</td><td>224009</td><td>242073</td><td>-6</td><td>8</td></tr></tbody></table><h4 id="分析"><a href="#分析" class="headerlink" title="分析:"></a>分析:</h4><ul><li>ARM在较低扩展性上性能比X86要好，但是在扩展到与cpu数目相近的threads时，性能就开始下降了</li><li>在频率缩放下，尽管存在扩展性问题，ARM仍然击败X86</li></ul><hr><h3 id="2-Read-Only"><a href="#2-Read-Only" class="headerlink" title="2. Read Only:"></a>2. Read Only:</h3><p><a href="https://camo.githubusercontent.com/e5b5e8d60dd3a1e636246b3d11f9e19602c755ce/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d726f2e706e67"><img src="https://camo.githubusercontent.com/e5b5e8d60dd3a1e636246b3d11f9e19602c755ce/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d726f2e706e67" alt="img"></a></p><table><thead><tr><th>threads</th><th>ARM (qps)</th><th>x86 (qps)</th><th>ARM (qps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>5222</td><td>5259</td><td>6025</td><td>-1</td><td>15</td></tr><tr><td>2</td><td>10333</td><td>10200</td><td>11923</td><td>1</td><td>17</td></tr><tr><td>4</td><td>19176</td><td>19349</td><td>22126</td><td>-1</td><td>14</td></tr><tr><td>8</td><td>36881</td><td>37035</td><td>42555</td><td>0</td><td>15</td></tr><tr><td>16</td><td>70337</td><td>67065</td><td>81158</td><td>5</td><td>21</td></tr><tr><td>32</td><td>109207</td><td>113210</td><td>126008</td><td>-4</td><td>11</td></tr><tr><td>64</td><td>139294</td><td>164148</td><td>160724</td><td>-15</td><td>-2</td></tr><tr><td>128</td><td>151382</td><td>175872</td><td>174672</td><td>-14</td><td>-1</td></tr><tr><td>256</td><td>149136</td><td>164382</td><td>172080</td><td>-9</td><td>5</td></tr></tbody></table><h4 id="分析-1"><a href="#分析-1" class="headerlink" title="分析:"></a>分析:</h4><ul><li>在低扩展性上，ARM与X86基本持平，在较高扩展性败于X86</li><li>应用频率缩放，ARM在大多数场景下继续击败X86</li></ul><hr><h3 id="3-Read-Write"><a href="#3-Read-Write" class="headerlink" title="3. Read Write:"></a>3. Read Write:</h3><p><a href="https://camo.githubusercontent.com/252ecf14b5b65667b2cfece150fdf9b7e42d36ca/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d72772e706e67"><img src="https://camo.githubusercontent.com/252ecf14b5b65667b2cfece150fdf9b7e42d36ca/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d72772e706e67" alt="img"></a></p><table><thead><tr><th>threads</th><th>ARM (tps)</th><th>x86 (tps)</th><th>ARM (tps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>137</td><td>149</td><td>158</td><td>-8</td><td>6</td></tr><tr><td>2</td><td>251</td><td>273</td><td>290</td><td>-8</td><td>6</td></tr><tr><td>4</td><td>462</td><td>502</td><td>533</td><td>-8</td><td>6</td></tr><tr><td>8</td><td>852</td><td>920</td><td>983</td><td>-7</td><td>7</td></tr><tr><td>16</td><td>1539</td><td>1678</td><td>1776</td><td>-8</td><td>6</td></tr><tr><td>32</td><td>2556</td><td>2906</td><td>2949</td><td>-12</td><td>1</td></tr><tr><td>64</td><td>3770</td><td>5158</td><td>4350</td><td>-27</td><td>-16</td></tr><tr><td>128</td><td>5015</td><td>8131</td><td>5787</td><td>-38</td><td>-29</td></tr><tr><td>256</td><td>5676</td><td>8562</td><td>6549</td><td>-34</td><td>-24</td></tr></tbody></table><h4 id="分析-2"><a href="#分析-2" class="headerlink" title="分析:"></a>分析:</h4><ul><li>在read-write测试套件中情况有些不同。ARM开始落后了。在低扩展性情况下，频率缩放会减缓这种落后，在高扩展性则持续增大了差距。</li></ul><hr><h3 id="4-Update-Index"><a href="#4-Update-Index" class="headerlink" title="4. Update Index:"></a>4. Update Index:</h3><p><a href="https://camo.githubusercontent.com/114ccd28ff574e5becf7723a62f24a0e2e599ef4/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d75692e706e67"><img src="https://camo.githubusercontent.com/114ccd28ff574e5becf7723a62f24a0e2e599ef4/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d75692e706e67" alt="img"></a></p><table><thead><tr><th>threads</th><th>ARM (tps)</th><th>x86 (tps)</th><th>ARM (tps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>328</td><td>373</td><td>378</td><td>-12</td><td>1</td></tr><tr><td>2</td><td>623</td><td>768</td><td>719</td><td>-19</td><td>-6</td></tr><tr><td>4</td><td>1060</td><td>1148</td><td>1223</td><td>-8</td><td>7</td></tr><tr><td>8</td><td>1905</td><td>2028</td><td>2198</td><td>-6</td><td>8</td></tr><tr><td>16</td><td>3284</td><td>3590</td><td>3789</td><td>-9</td><td>6</td></tr><tr><td>32</td><td>5543</td><td>6275</td><td>6396</td><td>-12</td><td>2</td></tr><tr><td>64</td><td>9138</td><td>10381</td><td>10544</td><td>-12</td><td>2</td></tr><tr><td>128</td><td>13879</td><td>16868</td><td>16014</td><td>-18</td><td>-5</td></tr><tr><td>256</td><td>19954</td><td>25459</td><td>23024</td><td>-22</td><td>-10</td></tr></tbody></table><h4 id="分析-3"><a href="#分析-3" class="headerlink" title="分析:"></a>分析:</h4><ul><li>频率缩放下，ARM继续与X86处于同等/更好的状态 (除了高竞争用例)</li></ul><hr><h3 id="5-Update-Non-Index"><a href="#5-Update-Non-Index" class="headerlink" title="5. Update Non-Index:"></a>5. Update Non-Index:</h3><p><a href="https://camo.githubusercontent.com/6d578230e925ce5afdae131d46ce484cfb647e1b/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d756e692e706e67"><img src="https://camo.githubusercontent.com/6d578230e925ce5afdae131d46ce484cfb647e1b/68747470733a2f2f6d7973716c6f6e61726d2e6769746875622e696f2f696d616765732f626c6f67332f41524d2d76732d7838362d756e692e706e67" alt="img"></a></p><table><thead><tr><th>threads</th><th>ARM (tps)</th><th>x86 (tps)</th><th>ARM (tps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>328</td><td>373</td><td>378</td><td>-12</td><td>1</td></tr><tr><td>2</td><td>588</td><td>686</td><td>678</td><td>-14</td><td>-1</td></tr><tr><td>4</td><td>1075</td><td>1118</td><td>1240</td><td>-4</td><td>11</td></tr><tr><td>8</td><td>1941</td><td>2043</td><td>2240</td><td>-5</td><td>10</td></tr><tr><td>16</td><td>3367</td><td>3662</td><td>3885</td><td>-8</td><td>6</td></tr><tr><td>32</td><td>5681</td><td>6438</td><td>6555</td><td>-12</td><td>2</td></tr><tr><td>64</td><td>9328</td><td>10631</td><td>10763</td><td>-12</td><td>1</td></tr><tr><td>128</td><td>14158</td><td>17245</td><td>16336</td><td>-18</td><td>-5</td></tr><tr><td>256</td><td>20377</td><td>26367</td><td>23512</td><td>-23</td><td>-11</td></tr></tbody></table><h4 id="分析-4"><a href="#分析-4" class="headerlink" title="分析:"></a>分析:</h4><ul><li>频率缩放下，ARM继续与X86处于同等/更好的状态 (除了高竞争用例)</li></ul><hr><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>基于以上的数据，我们能够得到如下结论:</p><ul><li>在只读场景中测试Mysql, ARM和X86性能几乎一致。</li><li>在涉及写场景中测试Mysql，ARM开始有些落后，但是如果我们考虑频率缩放，情况可能会好一点。</li><li>在生活里，我们通常不会考虑频率缩放，而会考虑他们的性价比。这是一个话题，但通常是事实：ARM实例比X86实例便宜34%（我们在测试中用相同的flavor 24U48G）。</li><li>有一点我们需要持续观察，就是， ARM工作负载在达到CPU限制之前具有很好的可伸缩性。随着可伸缩性的增强，争用增加，ARM开始滞后。这是因为互斥/竞争热点都是针对x86调优的（例如：自旋锁 spin-lock）。现在MySQL正式支持ARM，并且ARM的社区和来自各地开发者的兴趣也不断增长，所以Mysql社区也会针对ARM进行调优，让我们拭目以待把。</li></ul><p>总之，Mysql on ARM是一个值得从成本和性能角度来尝试的选择。</p><p><em>如果你还有问题/疑问，请告诉我。我会试着去回答。</em></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者：bzhaoopenstack&lt;/p&gt;
&lt;p&gt;原文链接：&lt;a href=&quot;https://mysqlonarm.github.io/MySQL-on-x86-vs-ARM/&quot;&gt;https://mysqlonarm.github.io/MySQL-on-x86-vs-ARM/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者: Krunal Bauskar&lt;/p&gt;
&lt;p&gt;在开始探索这一领域之前，我相信大家对这个话题都会非常感兴趣，包括本人在内。另外，在深入讨论具体的数据前，让我们先来了解一下两种架构之间的基本差异。除了CISC和RISC之外，让我们从Mysql的角度来看待这些重要的差异。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;强 vs  弱内存模型 (弱内存模型在无锁写场景下需要适当的内存屏障).&lt;/li&gt;
&lt;li&gt;底层硬件特定专用指令。例如：两者现在都支持crc32c硬件指令，但是底层调用它们的方式不同。更多指令差异请参看x86-SSE/ARM-ACLE。&lt;/li&gt;
&lt;li&gt;Cache Line 差异。 大多数ARM处理器倾向于使用更大的cacheline size (所有缓存以128 bytes为单位或者 64/128字节混合的方式)。&lt;/li&gt;
&lt;li&gt;其他系统调用级别的差异，如：ARM的PAUSE指令缺失和具有非常低延迟的替代指令无法引导硬件达到所需的延迟，sched_getcpu在ARM上引入了使用无锁构造的挑战，内存操作似乎带来更高的延迟等。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mysql社区已经为这个领域的问题贡献了多个补丁（另一个博客的主题）。自从MySQL刚刚宣称开始支持ARM架构以来，几乎没有什么优化，大部分工作仍然没有完成。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>源于鲲鹏，回归社区：GNU Glibc的ARM优化小记</title>
    <link href="https://kunpengcompute.github.io/2020/04/17/yuan-yu-kun-peng-hui-gui-she-qu-gnu-glibc-de-arm-you-hua-xiao-ji/"/>
    <id>https://kunpengcompute.github.io/2020/04/17/yuan-yu-kun-peng-hui-gui-she-qu-gnu-glibc-de-arm-you-hua-xiao-ji/</id>
    <published>2020-04-17T03:44:54.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>作者：姜逸坤 张学磊</p><p>从2019年10月初开始，我们团队开始着手Glibc在aarch64(64)架构下的优化工作，并且在2019年年底，将我们的全部优化贡献给上游开源社区。本文分享我们在Glibc的版本完成的优化以及性能测试结果，同时我们也尝试着将优化的思路进行总结，希望对其他项目的优化提供一些思路。</p><a id="more"></a><h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><h3 id="1-1-什么是Glibc"><a href="#1-1-什么是Glibc" class="headerlink" title="1.1 什么是Glibc?"></a>1.1 什么是Glibc?</h3><p>我们先看看官方的解释：</p><blockquote><p>The GNU C Library project provides the core libraries for the GNU system and GNU/Linux systems, as well as many other systems that use Linux as the kernel.</p></blockquote><p>Glibc的全名是The GNU C Library，它为GNU系统、GNU/Linux系统以及提供了核心的底层库。比如，我们平常使用的<code>memset</code>，<code>strlen</code>等等这些非常常用的接口都由这个库提供。</p><h3 id="1-2-为什么要优化？"><a href="#1-2-为什么要优化？" class="headerlink" title="1.2 为什么要优化？"></a>1.2 为什么要优化？</h3><p>在计算领域的水平场景，例如大数据、数据库、Web等领域都直接或者间接地依赖着Glibc，举个简单的例子，在数据库的代码中，我们经常使用memcpy接口，对变量进行复制，调用频次也异常的高。如果在数据复制的过程中，性能能够有所提升，那么对上层软件的性能提升也是显而易见的。</p><h3 id="1-3-做了什么优化？"><a href="#1-3-做了什么优化？" class="headerlink" title="1.3 做了什么优化？"></a>1.3 做了什么优化？</h3><p>根据我们的分析，字符、内存和锁操作是最基础也是最重要的基本接口，因此，我们选择了对这三种类型的接口优先进行优化。在实现优化中，我们利用了Glibc的<a href="https://sourceware.org/glibc/wiki/GNU_IFUNC">indirect function</a>这一机制，即会根据CPU、CPU arch去自动选择匹配的函数。这一机制让我们的实现，更加灵活，也对现有系统影响最小。</p><p>下图为我们这次优化主要接口：</p><p><img src="https://user-images.githubusercontent.com/1736354/82533405-c5c93c00-9b75-11ea-8c71-f37f557febee.png" alt="image"></p><p>在上游社区的推进过程中，我们始终坚持<strong>Upstream First</strong>的原则，希望能够将鲲鹏优化的收益共享给整个生态，真正做到<strong>源于鲲鹏，回归社区</strong>。</p><p>所以，可以看到我们的优化大部分（橙色部分）都贡献到了上游社区的AArch64的generic实现中，从而使得整个生态都能够受益，而小部分（绿色部分）针对于Kunpeng CPU的特殊优化则保持了单独实现。</p><h2 id="2-优化"><a href="#2-优化" class="headerlink" title="2. 优化"></a>2. 优化</h2><p>我们知道在一般的开发中，小字节数据操作的使用频率，是远远的大于大字节数据操作的使用频率，而对于大数据和数据库的场景，则有可能会出现很多大字节数据操作的使用。因此，其实我们的一个优化原则是：<strong>在保证中小字节没有负优化的前提下，提升大字节数据操作的性能</strong>。</p><p>本节我们将一一解析在我们贡献的过程中，每个接口优化的关键点，并且尽可能的写的通俗易懂，希望能通过这些干货，给大家在其他的优化中带来启发。</p><h3 id="2-1-memcmp，每次做更多，总时间更少"><a href="#2-1-memcmp，每次做更多，总时间更少" class="headerlink" title="2.1 memcmp，每次做更多，总时间更少"></a>2.1 memcmp，每次做更多，总时间更少</h3><p>Patch链接：<a href="http://patchwork.ozlabs.org/patch/1182191/">aarch64: Optimized implementation of memcmp</a></p><h4 id="2-1-1-优化思路"><a href="#2-1-1-优化思路" class="headerlink" title="2.1.1 优化思路"></a>2.1.1 优化思路</h4><p>对于memcmp的优化，我们的核心思路是通过<strong>循环展开</strong>让每个周期内做的事情更多，从而减少循环本身的开销。下图可以直观的看出，循环展开带来的性能提升：<br><img src="https://user-images.githubusercontent.com/1736354/82417438-212ff700-9aae-11ea-941b-ee98d33a9df6.png" alt="image"><br>具体如下：</p><ol><li>扩展循环间隔长度<br>memcmp的aarch64原实现是以16bit的长度作为循环的周期长度，在无形中增加了很多次循环的消耗，尤其是在进行大字节数据比较中，有较大的性能损失。因此，我们这次优化的核心思路是：<strong>将16bit的循环扩展的64bit的循环</strong>，简单的说就是现在一次循环会比较64bit的数据。</li><li>寻址方式优化<br>除此之外，我们还改变了LDP的寻址方式，从原来的后变址寻址（Post Index Addressing）变成了偏移寻址（Base Plus index）。</li></ol><h4 id="2-1-2-性能测试"><a href="#2-1-2-性能测试" class="headerlink" title="2.1.2 性能测试"></a>2.1.2 性能测试</h4><p><img src="https://user-images.githubusercontent.com/1736354/79631884-7f577a80-818e-11ea-8dba-b5d4d92718e1.png" alt="image"></p><p>可以从我们实际的测试结果看到整体在中大字节的性能有不错的提升，尤其是在128字节以上的场景，性能提升更是达到了18%。</p><h3 id="2-2-memcpy，他山之石，可以为玉"><a href="#2-2-memcpy，他山之石，可以为玉" class="headerlink" title="2.2 memcpy，他山之石，可以为玉"></a>2.2 memcpy，他山之石，可以为玉</h3><p>Patch链接：<a href="http://patchwork.ozlabs.org/patch/1215732/">add default memcpy version for kunpeng920</a><br>memcpy优化，因为社区的falkor版本在大、小字节的性能表现，已经很完善，因此最终，我们直接使用了Flakor版本作为优化版本。</p><p>Falkor版本的将字符分为3种场景：</p><ol><li>对于small(&lt; 32)的场景，优先处理，避免过多判断，影响性能。</li><li>对于medium(33-128)的场景，做展开，避免多次循环带来的性能损失。</li><li>对于large(&gt;128)的场景，4字节对齐处理，并做循环展开每次循环处理64字节。</li></ol><p>有兴趣的可以看看源码的实现<a href="http://patchwork.ozlabs.org/project/glibc/patch/1502134812-31816-1-git-send-email-siddhesh@sourceware.org/">链接</a>，整体性能提升13-18%。</p><h3 id="2-3-memrchr，站在巨人的肩上"><a href="#2-3-memrchr，站在巨人的肩上" class="headerlink" title="2.3 memrchr，站在巨人的肩上"></a>2.3 memrchr，站在巨人的肩上</h3><p>Patch链接：<a href="http://patchwork.ozlabs.org/patch/1178706/">aarch64: Optimized implementation of memrchr</a></p><h4 id="2-3-1-优化思路"><a href="#2-3-1-优化思路" class="headerlink" title="2.3.1 优化思路"></a>2.3.1 优化思路</h4><p>memrchr整体的优化思路是，参考memchr设计的魔鬼数字算法，通过汇编实现逻辑适配，实现对特定字符逆向查找的功能，替代原有的C语言实现方案达到优化，具体实现见上链接。</p><h4 id="2-3-2-性能测试"><a href="#2-3-2-性能测试" class="headerlink" title="2.3.2 性能测试"></a>2.3.2 性能测试</h4><p><img src="https://user-images.githubusercontent.com/1736354/82403228-ed92a400-9a90-11ea-9a35-9ec4899cc9ea.png" alt="image"><br>最终，我们获得了58%的性能提升，最终在大字节的场景，比generic版本提升了4倍左右。</p><h3 id="2-4-memset，定向优化，更懂硬件"><a href="#2-4-memset，定向优化，更懂硬件" class="headerlink" title="2.4 memset，定向优化，更懂硬件"></a>2.4 memset，定向优化，更懂硬件</h3><p>Patch链接：<a href="http://patchwork.ozlabs.org/patch/1188834/">aarch64: Optimized memset for Kunpeng processor. </a></p><h4 id="2-4-1-优化思路"><a href="#2-4-1-优化思路" class="headerlink" title="2.4.1 优化思路"></a>2.4.1 优化思路</h4><p>我们进行了通过循环展开和特殊的定制优化来更好的适配硬件分支预测的特性，从而达到优化的效果。</p><p>特别说明的是，对于memset来说，置零场景是非常常用的场景，我们发现原有的实现使用DZ_ZVA指令并未在置零场景有显著效果，反而增加了许多条件分支，因此我们使用set_long代替了置零，由于set_long本身有更少的分支及更少的预测，所以性能与原实现比也有所提升。</p><h4 id="2-4-2-性能测试"><a href="#2-4-2-性能测试" class="headerlink" title="2.4.2 性能测试"></a>2.4.2 性能测试</h4><p><img src="https://user-images.githubusercontent.com/1736354/82403298-19158e80-9a91-11ea-8b50-9542edcc15ca.png" alt="image"></p><h3 id="2-5-strcpy，加速的武器，vector-loads"><a href="#2-5-strcpy，加速的武器，vector-loads" class="headerlink" title="2.5 strcpy，加速的武器，vector loads"></a>2.5 strcpy，加速的武器，vector loads</h3><p>Patch链接：<a href="http://patchwork.ozlabs.org/patch/1181183/">aarch64: Optimized implementation of strcpy</a><br>strlen使用了neon寄存器，通过vector operations对函数进行了优化，对比原有的汇编实现，在64字节以上的场景，获得了5%-18%的提升：</p><p><img src="https://user-images.githubusercontent.com/1736354/82403315-2763aa80-9a91-11ea-82f4-441c90b52962.png" alt="image"></p><h3 id="2-6-strlen-strnlen-循环展开，判断更少，性能更优"><a href="#2-6-strlen-strnlen-循环展开，判断更少，性能更优" class="headerlink" title="2.6 strlen/strnlen 循环展开，判断更少，性能更优"></a>2.6 strlen/strnlen 循环展开，判断更少，性能更优</h3><p>strlen Patch链接：<a href="https://patches-gcc.linaro.org/patch/25209/">aarch64: Optimized strlen for strlen_asimd</a><br>strnlen Patch链接：<a href="http://patchwork.ozlabs.org/patch/1181184/">aarch64: Optimized implementation of strnlen</a></p><p>strlen和strnlen同样使用了vector operations和循环展开，对主循环仅行了改造</p><p>strlen有7%-18%的提升:<br><img src="https://user-images.githubusercontent.com/1736354/82403331-321e3f80-9a91-11ea-9703-ab92470b2178.png" alt="image"></p><p>strnlen有11%-24%的提升:<br><img src="https://user-images.githubusercontent.com/1736354/82403342-39dde400-9a91-11ea-9fc8-5f68df97887b.png" alt="image"></p><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h2><p>经过上面的介绍，相信大家已经了解了我们是怎么去优化这些函数的版本的，虽说大部分的优化都是比较晦涩的汇编语言，但是其实实际原理还是非常易懂的。</p><p>最后，我们再总结下我们应该从哪些方面考虑，去完成优化：</p><ul><li>使用Neon汇编指令提高指令速度</li><li>使用Prefetch机制充分利用cache</li><li>避免非对齐的内存访问</li><li>指令重排，减少数据依赖</li><li>循环展开，减少高频判断</li><li>结合硬件特性，用软件补齐硬件缺陷</li></ul><h2 id="4-写在最后"><a href="#4-写在最后" class="headerlink" title="4. 写在最后"></a>4. 写在最后</h2><p>本书所提及的所有代码，均已贡献到Glibc上游社区，并且随着<a href="https://sourceware.org/legacy-ml/libc-announce/2020/msg00001.html">Glibc 2.31</a>已经在社区完成发布，有需要的可以直接从社区上游获取使用，有任何问题也可以在本文留言。</p><p>另外，Glibc优化，也全部<a href="https://gitee.com/src-openeuler/glibc/pulls/17">合入到</a>集成在当前版本的openeuler中，有兴趣的，也可以直接使用openEuler最新版本进行体验。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：姜逸坤 张学磊&lt;/p&gt;
&lt;p&gt;从2019年10月初开始，我们团队开始着手Glibc在aarch64(64)架构下的优化工作，并且在2019年年底，将我们的全部优化贡献给上游开源社区。本文分享我们在Glibc的版本完成的优化以及性能测试结果，同时我们也尝试着将优化的思路进行总结，希望对其他项目的优化提供一些思路。&lt;/p&gt;
    
    </summary>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/categories/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/tags/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>让Github Action在你的ARM机器上跑起来</title>
    <link href="https://kunpengcompute.github.io/2020/04/15/rang-github-action-zai-ni-de-arm-ji-qi-shang-pao-qi-lai/"/>
    <id>https://kunpengcompute.github.io/2020/04/15/rang-github-action-zai-ni-de-arm-ji-qi-shang-pao-qi-lai/</id>
    <published>2020-04-15T08:22:00.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>作者：<a href="https://github.com/Yikun">姜逸坤</a></p><p>Github在2019年8月，宣布推出了一项新的功能——<a href="https://github.blog/2019-08-08-github-actions-now-supports-ci-cd/">Github Action</a>，让成千上万的开源项目可以利用Github提供的计算资源完成构建、测试、部署等CI/CD，并且提供<a href="https://github.blog/2019-11-05-self-hosted-runners-for-github-actions-is-now-in-beta/">Self Hosted Runners</a>功能，让开发者们可以将自己的机器接入到Github中来。</p><p>最近，我们利用这一功能，将搭载着<a href="https://openeuler.org/zh/">openEuler 20.03 (LTS) 操作系统</a>，跑在<a href="http://www.hisilicon.com/en/Products/ProductList/Kunpeng">Kunpeng 920 处理器</a>的ARM环境接入进来，在近期华为与阿里合作的<a href="https://github.com/kunpengcompute/kunpeng-mpam">MPAM</a>项目，也将充分的利用这些资源利用Github Action的能力完成构建与测试。</p><p>本篇文章将接入方法分享给大家，希望能够帮助更多同学们把<strong>自己的ARM环境</strong>也在Github上用起来。</p><a id="more"></a><h2 id="1-接入资源"><a href="#1-接入资源" class="headerlink" title="1. 接入资源"></a>1. 接入资源</h2><p><img src="https://user-images.githubusercontent.com/1736354/79320175-f21ce780-7f3b-11ea-8802-d0be06455e70.png" alt="image"><br>资源的接入流程比较简单：</p><ol><li><p>依次点击项目的<code>Settings</code>–<code>Actions</code>进入资源接入页面，点击<code>Add Runner</code>。</p></li><li><p>根据弹出的提示，下载和运行脚本<br><img src="https://user-images.githubusercontent.com/1736354/79411221-e8938e00-7fd4-11ea-8e8e-7a1b576c2fa2.png" alt="image"></p></li><li><p>完成后我们可以看到接入的资源：<br><img src="https://user-images.githubusercontent.com/1736354/79411190-d4e82780-7fd4-11ea-952c-06f43dbfa5e2.png" alt="image"></p></li></ol><h2 id="2-使用资源"><a href="#2-使用资源" class="headerlink" title="2. 使用资源"></a>2. 使用资源</h2><p><img src="https://user-images.githubusercontent.com/1736354/79320076-c4d03980-7f3b-11ea-8822-7e724fd8743a.png" alt="image"><br>我们为接入的项目增加一个Action：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">Run</span> <span class="string">some</span> <span class="string">script</span> <span class="string">in</span> <span class="string">Kunpeng</span> <span class="string">env</span></span><br><span class="line"></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span> <span class="string">[</span> <span class="string">master</span> <span class="string">]</span></span><br><span class="line">  <span class="attr">pull_request:</span></span><br><span class="line">    <span class="attr">branches:</span> <span class="string">[</span> <span class="string">master</span> <span class="string">]</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">build:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">self-hosted</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">    <span class="comment"># Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uses:</span> <span class="string">actions/checkout@v2</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Run</span> <span class="string">`uname</span> <span class="string">-a`</span> <span class="string">in</span> <span class="string">Kunpeng</span> <span class="string">env</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line">        <span class="string">uname</span> <span class="string">-a</span></span><br><span class="line">        <span class="string">cat</span> <span class="string">/etc/os-release</span></span><br><span class="line">        <span class="string">lscpu</span> <span class="string">|</span> <span class="string">grep</span> <span class="string">-E</span> <span class="string">"Architecture|Model name|CPU\(s\):"</span></span><br></pre></td></tr></table></figure><p>这样，这个workflow是展示所接入的环境上内核、操作系统、处理器信息，我们可以从结果看到job的结果：<br><img src="https://user-images.githubusercontent.com/1736354/79411452-71122e80-7fd5-11ea-83db-e4f001c5f796.png" alt="image"></p><p>点击<code>Details</code>可以进入详情页面：</p><p><img src="https://user-images.githubusercontent.com/1736354/79414159-7aeb6000-7fdc-11ea-86d5-60dce06abe43.png" alt="image"></p><p>可以看到，我们在资源上执行的指令，已经运行成功，可以看到这台资源的系统为<code>openEuler 20.03 (LTS)</code>，CPU为aarch64 128核的<code>Kunpeng 920</code>。</p><h2 id="3-结语"><a href="#3-结语" class="headerlink" title="3.结语"></a>3.结语</h2><p>本文介绍了我们是如何将搭载着鲲鹏920处理器、openEuler操作系统的计算资源接入到Github Action的。可以看到Github Action的自定义资源接入，在ARM64下还是很顺滑的。</p><p>希望这篇文章能够帮助到大家，大家也可以尝试着将你们自己ARM资源接入进来，有问题可以留言一起讨论，玩的开心！：）</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：&lt;a href=&quot;https://github.com/Yikun&quot;&gt;姜逸坤&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Github在2019年8月，宣布推出了一项新的功能——&lt;a href=&quot;https://github.blog/2019-08-08-github-actions-now-supports-ci-cd/&quot;&gt;Github Action&lt;/a&gt;，让成千上万的开源项目可以利用Github提供的计算资源完成构建、测试、部署等CI/CD，并且提供&lt;a href=&quot;https://github.blog/2019-11-05-self-hosted-runners-for-github-actions-is-now-in-beta/&quot;&gt;Self Hosted Runners&lt;/a&gt;功能，让开发者们可以将自己的机器接入到Github中来。&lt;/p&gt;
&lt;p&gt;最近，我们利用这一功能，将搭载着&lt;a href=&quot;https://openeuler.org/zh/&quot;&gt;openEuler 20.03 (LTS) 操作系统&lt;/a&gt;，跑在&lt;a href=&quot;http://www.hisilicon.com/en/Products/ProductList/Kunpeng&quot;&gt;Kunpeng 920 处理器&lt;/a&gt;的ARM环境接入进来，在近期华为与阿里合作的&lt;a href=&quot;https://github.com/kunpengcompute/kunpeng-mpam&quot;&gt;MPAM&lt;/a&gt;项目，也将充分的利用这些资源利用Github Action的能力完成构建与测试。&lt;/p&gt;
&lt;p&gt;本篇文章将接入方法分享给大家，希望能够帮助更多同学们把&lt;strong&gt;自己的ARM环境&lt;/strong&gt;也在Github上用起来。&lt;/p&gt;
    
    </summary>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/categories/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/tags/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>理解InnoDB rw-lock的统计数据</title>
    <link href="https://kunpengcompute.github.io/2020/04/15/li-jie-innodb-rw-lock-de-tong-ji-shu-ju/"/>
    <id>https://kunpengcompute.github.io/2020/04/15/li-jie-innodb-rw-lock-de-tong-ji-shu-ju/</id>
    <published>2020-04-15T03:46:30.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>译者：bzhaoopenstack</p><p>原文链接：<a href="https://mysqlonarm.github.io/Understanding-InnoDB-rwlock-stats/">https://mysqlonarm.github.io/Understanding-InnoDB-rwlock-stats/</a></p><p>作者: Krunal Bauskar</p><p>InnoDB使用互斥锁进行独占访问，使用rw-locks进行共享访问。rw-locks用于控制对缓冲池页、表空间、自适应搜索系统、数据字典、informaton_schema等公共共享资源的访问。总之，rw-locks在InnoDB系统中扮演着非常重要的角色，因此跟踪和监视它们也很重要。</p><a id="more"></a><p>InnoDB 提供了一种简单的方式来跟踪它们， “SHOW ENGINE INNODB STATUS”.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 38667, rounds 54868, OS waits 16539</span><br><span class="line">RW-excl spins 6353, rounds 126218, OS waits 3936</span><br><span class="line">RW-sx spins 1896, rounds 43888, OS waits 966</span><br><span class="line">Spin rounds per wait: 1.42 RW-shared, 19.87 RW-excl, 23.15 RW-sx</span><br></pre></td></tr></table></figure><p>在本文中，我们将了解这些统计数据是如何计算的，以及每个数据的意义。我们还将尝试使用不同的用例来描述一些推论，并且接触一下基础又重要的统计数据， 这个<a href="https://bugs.mysql.com/bug.php?id=99171">bug</a>使当前状态的统计几乎无法进行调优。</p><h2 id="rw-lock-自旋算法"><a href="#rw-lock-自旋算法" class="headerlink" title="rw-lock 自旋算法"></a>rw-lock 自旋算法</h2><p>rw-locks有三种类型:</p><ul><li>Shared: 提供资源的共享访问。允许多个共享锁。</li><li>Exclusive: 提供对资源的独占访问。共享锁等待排他锁。</li><li>Shared-Exclusive (SX): 对读不一致的资源提供写访问（relaxed exclusive）。</li></ul><p>首先我们先尝试理解流程，然后讨论一些调优步骤。</p><p>(为了便于讨论，假设 spins=0, rounds=0, os-waits=0).</p><h3 id="Locking-步骤"><a href="#Locking-步骤" class="headerlink" title="Locking 步骤"></a>Locking 步骤</h3><ul><li>Step-1:</li></ul><p>  尝试获取所需的锁</p><ul><li>If SUCCESS then return immediately. (spins=0, rounds=0, os-waits=0)</li><li>If FAILURE then enter spin-loop.</li></ul><ul><li><strong>Step-2:</strong> 开始自旋回环(Spin-loop). 增加自旋计数 (spin-count). <em>(为什么需要自旋循环？如果我们的线程进入等待状态，那么操作系统将把CPU从给定的线程中带走，暂时不让它使用CPU，然后线程将不得不按照操作系统调度的次序等待CPU资源，从而进行下面的任务。更好的方法是在繁忙等待中（busy-wait）使用自旋循环(带有条件的检查确认锁是否被释放）以便保留CPU。由于这些锁大多数都是短时间使用，因此重新获得的机会可能性非常高。).</em></li></ul><ul><li><strong>Step-3:</strong> 开始N轮自旋。Start spinning for N rounds. (这里N的定义由<a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-parameters.html#sysvar_innodb_sync_spin_loops">innodb_sync_spin_loops</a>来控制)。默认为30轮。<ul><li><strong>Step-3a:</strong> 每一轮将调用一个PAUSE逻辑（见下面关于PAUSE逻辑的单独一节），这将导致CPU进入PAUSE的X个周期。</li><li><strong>Step-3b:</strong> 每轮检查（软实现）是否对应的锁已经可用(busy-wait)。<ul><li>如果它可用，那么自旋循环退出。（可能还有一些其他同样正在自旋的检查。我们将使用下面的信息）。</li></ul></li><li><strong>Step-3c:</strong> 再次尝试获取所需的锁。<ul><li>If SUCCESS then return. (spins=1, rounds=M (M &lt;= N), os-waits=0)</li><li>If FAILURE，并且此时仍有其他正在自旋，且悬而未决任务 (max=innodb_sync_spin_loops)还是继续自旋。（为什么循环被中断，锁失败。注：该锁被多个线程并行查看。而当多个线程试图获取锁时，它们收到了锁可用的信号。被其他线程取走，所以该线程现在仍在重新尝试）。</li></ul></li><li><strong>Step-3d:</strong> 当这个线程现在完成了它设置的spin-wait轮循次数，到现在它还没有获得锁。那么它会被认为浪费CPU周期，没有必要继续自旋。最好的选择是放弃挂起的CPU周期并交还给操作系统，让操作系统调度做其他有用的事情。此外，由于所述线程现在将要进入睡眠，它应该向一些公共基础设施注册自身，这些基础设施将帮助它在所述锁可用时发出恢复活动的信号。</li><li><strong>Step-3e:</strong> 这个将线程从唤醒的基础设施正式InnoDB中的同步阵列（sync-array）基础设施。<ul><li>所述线程通过在同步阵列中预留插槽来注册自身。</li><li>在开始等待之前，再试一次看锁是否可用。（因为预留可能很费时间，同时锁这个时候是可用的状态）。</li><li>如果仍然没有获得锁，则将等待同步阵列向该线程送回信号。</li><li>这种等待称为OS-wait，进入这个循环现在会导致OS-waits计数增加。</li></ul></li><li><strong>Step-3f:</strong> 如果该线程收到由同步阵列发送回的wait-event信号。它会重新尝试获取锁。<ul><li>If SUCCESS then return. (spins=1, rounds=N, os-waits=1)</li><li>If FAILURE ，则整个循环从旋转逻辑重新启动(返回Step-3，rounds-count重新初始化为0)。注意：自旋计数(spins count)不会重新递增。</li></ul></li></ul></li></ul><p>所以现在我们来给这些计数赋予意义</p><ul><li><strong>spins:</strong> 代表在第一次尝试中多少轮数而未能得到一个锁，并不得不进入自旋循环。</li><li><strong>rounds:</strong> 表示执行多少轮PAUSE逻辑。</li><li><strong>os-waits:</strong>自旋循环在多少轮自旋时仍未得到锁而导致os-waits。</li></ul><p>在获取所述锁流程中的自旋循环期间中，可能需要超过30轮(innodb_sync_spin_loops)PAUSE逻辑，并且还可能多次进入os-waits。这可能会导致os-waits &gt; spins-count。</p><h3 id="PAUSE-逻辑"><a href="#PAUSE-逻辑" class="headerlink" title="PAUSE 逻辑"></a>PAUSE 逻辑</h3><p>K = {取 (0 - <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-parameters.html#sysvar_innodb_spin_wait_delay">innodb_spin_wait_delay</a>)之间的随机数 * <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-parameters.html#sysvar_innodb_spin_wait_pause_multiplier">innodb_spin_wait_pause_multiplier</a>}</p><p>调用底层 PAUSE 指令 K 次.</p><p>并不是所有的架构都提供底层的PAUSE指令。x86有提供，但ARM没有。即使x86深藏了这个PAUSE指令，它也会随着处理器的不同系列而继续变化。老一代处理器的周期约为10-15次（pre-skylake）,Skylake系列的周期约为140次，然后CascadeLake系列的周期数又降下来了 (我看到在属于CascadeLake系列的Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz芯片上是13次). <em>(除了Cascadelake系列以外，我个人并没有在其他平台上对它进行基准测试) 但这些信息是可供参考的。</em>这意味着延迟引入PAUSE指令（按照周期计算）会持续不断的变化，所以针对每一代/类型的处理器调整PAUSE逻辑是非常重要的。 这里有两个可配置的变量可以解决这个问题，innodb_spin_wait_delay 和 innodb_spin_wait_pause_multiplier。</p><h2 id="统计数据解读"><a href="#统计数据解读" class="headerlink" title="统计数据解读"></a>统计数据解读</h2><p>现在我们已经了解了统计数据，让我们看看这个数字，并试着做出一些推断。</p><p>不过，在谈及进一步的细节之前，让我先看看这个<a href="https://bugs.mysql.com/bug.php?id=99171">bug</a>， 它描述了导致这些统计数据不一致和不正确的原因和修复方法。</p><p>为了得到一个公正的结论，我们将使用mysql对应版本，并打上补丁。 (正如bug中指出的，不使用修复补丁统计数据不能产生正确的数据，因此各种解释和调优都毫无用处).</p><h3 id="Use-Case-1"><a href="#Use-Case-1" class="headerlink" title="Use Case 1"></a>Use Case 1</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 338969, rounds 20447615, OS waits 592941</span><br><span class="line">RW-excl spins 50582, rounds 1502625, OS waits 56124</span><br><span class="line">RW-sx spins 12583, rounds 360973, OS waits 10484</span><br><span class="line">Spin rounds per wait: 60.32 RW-shared, 29.71 RW-excl, 28.69 RW-sx</span><br></pre></td></tr></table></figure><p>让我们分析一下共享自旋的情况:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 338969, rounds 20447615, OS waits 592941</span><br></pre></td></tr></table></figure><ul><li>在头一次尝试中，用了338K 次仍未获取到锁，迫使线程进去自旋锁状态(spin-lock)。</li><li>在每个自旋周期内，执行了60轮PAUSE周期（因此，所述自旋周期执行了2次）。</li><li>OS-waits/spins = 592/338 = 1.75表明大部分被分流进入了OS-wait（PAUSE的延迟不够）。</li><li>表明对于大多数自旋周期，单一的操OS-wait是不够的，因此这种操作是在重复进行的。</li></ul><p><strong>Conclusion:</strong> 该Use-case是高竞争情况。而且，诸如PAUSE循环无法产生所需的延迟来获得锁，导致每个自旋周期产生如此之多的PAUSE循环。</p><p><em>256 thread oltp-read-write workload on 24 vCPU ARM machine</em></p><h3 id="Use-Case-2"><a href="#Use-Case-2" class="headerlink" title="Use Case 2"></a>Use Case 2</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 35943, rounds 777178, OS waits 19051</span><br><span class="line">RW-excl spins 4269, rounds 121796, OS waits 4164</span><br><span class="line">RW-sx spins 13407, rounds 321954, OS waits 7346</span><br><span class="line">Spin rounds per wait: 21.62 RW-shared, 28.53 RW-excl, 24.01 RW-sx</span><br></pre></td></tr></table></figure><p>让我们分析一下共享自旋的情况:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 35943, rounds 777178, OS waits 19051</span><br></pre></td></tr></table></figure><ul><li>流程中，自旋循环35K次。</li><li>只有19K次(大约是自旋循环的一半)引起了OS-waits。</li><li>平均每个自旋周期也限制在21.62，这表明，对于每个自旋周期，平均有22轮PAUSE循环。</li></ul><p><strong>Conclusion:</strong> 该Use-case表示中度竞争情况。</p><p><em>16 thread oltp-read-write workload on 24 vCPU ARM machine</em></p><h3 id="Use-Case-3"><a href="#Use-Case-3" class="headerlink" title="Use Case 3"></a>Use Case 3</h3><p>让我举一个常见的例子，以供参考。这是16个线程的oltp-read-write工作负载在基于x86_64的16CPU虚拟机上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 39578, rounds 424553, OS waits 7329</span><br><span class="line">RW-excl spins 5828, rounds 78225, OS waits 1341</span><br><span class="line">RW-sx spins 11666, rounds 67297, OS waits 449</span><br><span class="line">Spin rounds per wait: 10.73 RW-shared, 13.42 RW-excl, 5.77 RW-sx</span><br></pre></td></tr></table></figure><ul><li>流程中自旋循环39K次。</li><li>只有7K(约占自旋循环的20%) 导致OS-waits。</li><li>每自旋周期平均数也限制为10。</li></ul><p><strong>Conclusion:</strong> 该Use-case表示低竞争情况。</p><p><em>16 thread oltp-read-write workload on 24 vCPU x86_64 machine</em></p><h3 id="调优注意事项"><a href="#调优注意事项" class="headerlink" title="调优注意事项"></a>调优注意事项</h3><p>记得我们在上面看到的高竞争案例。通过优化一些代码，可以显著减少共享自旋的争用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 318800, rounds 13856732, OS waits 374634</span><br><span class="line">RW-excl spins 35759, rounds 656955, OS waits 22310</span><br><span class="line">RW-sx spins 10750, rounds 226315, OS waits 5598</span><br><span class="line">Spin rounds per wait: 43.47 RW-shared, 18.37 RW-excl, 21.05 RW-sx</span><br></pre></td></tr></table></figure><p>每个自旋周期的轮数: 平均数从 60 降到 43</p><p>每个自旋周期的OS-wait次数: 从1.75 降到1.17</p><p>这性能好起来了吗？不是太好。有许多因素需要考虑。</p><ul><li>你看到TPS有改善吗？</li><li>有时，它可能会建议简单地增加PAUSE循环。但是，增加PAUSE循环超过某个点将会导致延长自旋周期，最终浪费宝贵的CPU周期，尤其是这会导致线程返回到OS-wait状态。(这种方式可能对HT案例和多核案例更有效)。</li><li>同样，如上所述，不同处理器的系列和类型会影响PAUSE循环延迟。</li></ul><p>有许多因素需要考虑。甚至我正在研究这个问题，看看我们如何为所有类型的CPU来优化它。一旦我在这个研究中发掘到一些非常好的通用的算法(或者我们可以开发一些自动的、自调整的或自适应的算法)，我会发布更多关于这个问题的博客，用户无需担心。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>正如我们在上文看到的，rw-locks统计数据可以帮助我们更好地理解系统中锁的争用。当然，它不是有关InnoDB争用的唯一说明，因为互斥锁没在这些统计数据里面。调优可能具有挑战性，因为以错误的方式过度调优也会影响性能。</p><p><em>如果你还有问题/疑问，请告诉我。会试着去回答他们。</em></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;译者：bzhaoopenstack&lt;/p&gt;
&lt;p&gt;原文链接：&lt;a href=&quot;https://mysqlonarm.github.io/Understanding-InnoDB-rwlock-stats/&quot;&gt;https://mysqlonarm.github.io/Understanding-InnoDB-rwlock-stats/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者: Krunal Bauskar&lt;/p&gt;
&lt;p&gt;InnoDB使用互斥锁进行独占访问，使用rw-locks进行共享访问。rw-locks用于控制对缓冲池页、表空间、自适应搜索系统、数据字典、informaton_schema等公共共享资源的访问。总之，rw-locks在InnoDB系统中扮演着非常重要的角色，因此跟踪和监视它们也很重要。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Understanding InnoDB rw-lock stats</title>
    <link href="https://kunpengcompute.github.io/2020/04/14/understanding-innodb-rw-lock-stats/"/>
    <id>https://kunpengcompute.github.io/2020/04/14/understanding-innodb-rw-lock-stats/</id>
    <published>2020-04-14T07:21:05.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>作者: Krunal Bauskar</p><p>InnoDB uses mutexes for exclusive access and rw-locks for the shared access of the resources. rw-locks are used to control access to the common shared resources like buffer pool pages, tablespaces, adaptive search systems, data-dictionary, informaton_schema, etc… In short, rw-locks play a very important role in the InnoDB system and so tracking and monitoring them is important too.</p><a id="more"></a><p>InnoDB provides an easy way to track them using “SHOW ENGINE INNODB STATUS”.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 38667, rounds 54868, OS waits 16539</span><br><span class="line">RW-excl spins 6353, rounds 126218, OS waits 3936</span><br><span class="line">RW-sx spins 1896, rounds 43888, OS waits 966</span><br><span class="line">Spin rounds per wait: 1.42 RW-shared, 19.87 RW-excl, 23.15 RW-sx</span><br></pre></td></tr></table></figure><p>In this article we will try to understand how these stats are calculated and what is the significance of each of these numbers. We will also try to draw inferences using different use-cases and touch base important stats <a href="https://bugs.mysql.com/bug.php?id=99171">bug</a> that makes the current state of the stats almost ineffective for tuning.</p><h2 id="rw-lock-spin-algorithm"><a href="#rw-lock-spin-algorithm" class="headerlink" title="rw-lock spin algorithm"></a><span style="color:#4885ed">rw-lock spin algorithm</span></h2><p>There are 3 types of rw-locks:</p><ul><li>Shared: offers shared access to the resource. Multiple shared locks are allowed.</li><li>Exclusive: offers exclusive access to the resource. Shared locks wait for exclusive locks.</li><li>Shared-Exclusive (SX): offer write access to the resource with inconsistent read. (relaxed exclusive).</li></ul><p>We will first try to understand the flow and then discuss some tuning steps. <br><br>(For sake of discussion, to start with, let’s assume <span style="background-color: #fff2e6; color: red">spins=0, rounds=0, os-waits=0</span>).</p><h3 id="Locking-Steps"><a href="#Locking-Steps" class="headerlink" title="Locking Steps"></a><span style="color:#1aa260">Locking Steps</span></h3><ul><li><p><strong>Step-1:</strong> Try to obtain the needed lock</p><ul><li>If <span style="color: green">SUCCESS</span> then return immediately. <span style="background-color: #fff2e6; color: red"> (spins=0, rounds=0, os-waits=0)</span></li><li>If <span style="color: red">FAILURE</span> then enter spin-loop.<p></p> </li></ul></li><li><p><strong>Step-2:</strong> Start Spin-loop. Increment spin-count. <em>(Why is spin-loop needed? If we enter wait then the OS will take away CPU from the given thread and then the thread will have to wait for its turns as per OS-scheduling. Better approach is to busy-wait using spin-loop (with condition check) so that the CPU is kept. Since most of these locks are short-duration likely chance of re-obtaining it very high).</em></p><p></p> </li><li><p><strong>Step-3:</strong> Start spinning for N rounds. (Here N is defined and controlled by <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-parameters.html#sysvar_innodb_sync_spin_loops">innodb_sync_spin_loops</a>). Default is 30 rounds. </p><ul><li><p><strong>Step-3a:</strong> Each round will invoke a PAUSE logic (see a separate section below about PAUSE logic) that will cause the CPU to go to PAUSE for X cycles.</p></li><li><p><strong>Step-3b:</strong> Post each round a soft check is done if the said lock is available (busy-wait).</p><ul><li>If it is available then spin-cycle exits. (There could still be some rounds pending. We will use this info below).</li></ul></li><li><p><strong>Step-3c:</strong> Again try to obtain the needed lock.</p><ul><li>If <span style="color: green">SUCCESS</span> then return.<span style="background-color: #fff2e6; color: red"> (spins=1, rounds=M (M &lt;= N), os-waits=0)</span></li><li>If <span style="color: red">FAILURE</span> and there are some pending rounds (max=innodb_sync_spin_loops) then resume spinning. <em>(How come the loop was interrupted and locking failed. Note: the said lock is being looked upon by multiple threads in parallel. While multiple threads got signals about lock availability by the time said thread tried to obtain the lock, some other thread took it. So the said thread is now back re-trying)</em>.</li></ul></li><li><p><strong>Step-3d:</strong> Say a thread now completes its set rounds of spin-wait and even now it failed to obtain the lock. There is no point in  spinning further and wasting CPU cycles. Better give up pending CPU cycles back to OS and let OS scheduling do the needful. Also, since the said thread is now going to go to sleep it should register itself with some common infrastructure that will help signal it back to active whenever the said lock is available.</p></li><li><p><strong>Step-3e:</strong> This infrastructure to signal it back to active is sync-array infrastructure in InnoDB.</p><ul><li>Said thread registers itself by reserving a slot in the said array.</li><li>Before starting the wait, give another try to see if the lock is available. (since reserving could be time consuming and lock could be available in meantime).</li><li>If still lock is not available then wait for sync-array infrastructure to signal back the said thread.</li><li>This wait is called OS-wait and entering this loop will now cause OS-waits count to increase.</li></ul></li><li><p><strong>Step-3f:</strong> Say the said thread is signaled by sync-array infrastructure for the wait-event. It retries to obtain the needed lock.</p><ul><li>If <span style="color: green">SUCCESS</span> then return. <span style="background-color: #fff2e6; color: red"> (spins=1, rounds=N, os-waits=1)</span></li><li>If <span style="color: red">FAILURE</span> then the whole loop restarts from spinning logic (Back to Step-3 with rounds-count re-intialize to 0). Note: spins count is not re-incremented.</li></ul></li></ul></li></ul><p>So let’s now assign the meaning to these counts</p><ul><li><strong>spins:</strong> represent how many times flow failed to get a lock in first go and had to enter spin-loop.</li><li><strong>rounds:</strong> represent how many rounds of PAUSE logic executed.</li><li><strong>os-waits:</strong> how many times spin-loop failed to grant a lock that resulted in os-waits.</li></ul><p>It is possible that during a given spin loop for acquiring said lock flow may need more than 30 (innodb_sync_spin_loops) rounds of PAUSE logic and have to multiple time enter os-waits. This can cause os-waits &gt; spins-count.</p><h3 id="PAUSE-logic"><a href="#PAUSE-logic" class="headerlink" title="PAUSE logic"></a><span style="color:#1aa260">PAUSE logic</span></h3><p>K = {random value from between  (0 - <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-parameters.html#sysvar_innodb_spin_wait_delay">innodb_spin_wait_delay</a>) * <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-parameters.html#sysvar_innodb_spin_wait_pause_multiplier">innodb_spin_wait_pause_multiplier</a>}</p><p>Invoke low-level PAUSE instruction K times.</p><p>Not all architectures provide low-level pause instruction. x86 does provide it but ARM doesn’t. Even with x86 latency of this pause instruction continues to change with different families of processors. It was around 10-15 cycles for old generation processors (pre-skylake). Went upto 140 cycles with Skylake and again came down with CascadeLake (I see 13 cycles with Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz which belongs to CascadeLake family). <em>(I have not personally benchmarked it on all platforms (except Cascadelake) but this is based on the references.</em> This means the delay (in terms of cycle) introduced by PAUSE continues to change and so tuning PAUSE logic for each generation/type of processor is important. 2 configurable variables viz. innodb_spin_wait_delay, innodb_spin_wait_pause_multiplier exactly help achieve this.</p><h2 id="Interpreting-stats"><a href="#Interpreting-stats" class="headerlink" title="Interpreting stats"></a><span style="color:#4885ed">Interpreting stats</span></h2><p>Now that we understand the stats let’s look at the number and try to draw some inferences.</p><p>But before we get into further details let me point out a <a href="https://bugs.mysql.com/bug.php?id=99171">bug</a> that makes these stats inconsistent and incorrect.</p><p>To get a fair idea we will use the version of mysql with the patch applied (as pointed in bug, stats w/o patch doesn’t help yield correct picture and so all kinds of interpretation and tuning is bound to fail).</p><h3 id="Use-Case-1"><a href="#Use-Case-1" class="headerlink" title="Use Case 1"></a><span style="color:#1aa260">Use Case 1</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RW-shared spins 338969, rounds 20447615, OS waits 592941</span><br><span class="line">RW-excl spins 50582, rounds 1502625, OS waits 56124</span><br><span class="line">RW-sx spins 12583, rounds 360973, OS waits 10484</span><br><span class="line">Spin rounds per wait: 60.32 RW-shared, 29.71 RW-excl, 28.69 RW-sx</span><br></pre></td></tr></table></figure><p>Let’s analyze the shared spins case:<br></p><figure class="highlight plain"><figcaption><span>spins 338969, rounds 20447615, OS waits 592941```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">* 338K times flow couldn’t find the lock in first go, forcing thread to enter spin-lock.</span><br><span class="line">* During each spin-cycle there were 60 rounds of PAUSE cycle executed (so the said spin-cycles were done 2 times).</span><br><span class="line">* OS-waits&#x2F;spins &#x3D; 592&#x2F;338 &#x3D; 1.75 suggest that majority of the flow entered OS-wait (delay from PAUSE was not sufficient).</span><br><span class="line">* It also suggests that for the majority of spin-cycles, single OS -wait was not enough so it was repeated.</span><br><span class="line"></span><br><span class="line">**Conclusion:** Use-case represents heavy contention. Also, it sounds like PAUSE loop is unable to introduce the needed delay that is causing so many ROUNDS of PAUSE loop per spin-cycle.</span><br><span class="line"></span><br><span class="line">&lt;em&gt;256 thread oltp-read-write workload on 24 vCPU ARM machine&lt;&#x2F;em&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### &lt;span style&#x3D;&quot;color:#1aa260&quot;&gt;Use Case 2&lt;&#x2F;span&gt;</span><br></pre></td></tr></table></figure><p>RW-shared spins 35943, rounds 777178, OS waits 19051<br>RW-excl spins 4269, rounds 121796, OS waits 4164<br>RW-sx spins 13407, rounds 321954, OS waits 7346<br>Spin rounds per wait: 21.62 RW-shared, 28.53 RW-excl, 24.01 RW-sx</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">Let’s analyze the shared spins case:&lt;br&gt;</span><br><span class="line">&#96;RW-shared spins 35943, rounds 777178, OS waits 19051&#96;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">* Flow invokes spin-loop 35K times.</span><br><span class="line">* Only 19K (that is approximately half of the spin-loop) caused OS-waits.</span><br><span class="line">* Per spin-cycle average too is limited to 21.62 that suggests that for each spin-cycle on average 22 rounds of PAUSE loop was </span><br><span class="line">invoked.</span><br><span class="line"></span><br><span class="line">**Conclusion:** Use-case represents medium contention.</span><br><span class="line"></span><br><span class="line">&lt;em&gt;16 thread oltp-read-write workload on 24 vCPU ARM machine&lt;&#x2F;em&gt;</span><br><span class="line"></span><br><span class="line">### &lt;span style&#x3D;&quot;color:#1aa260&quot;&gt;Use Case 3&lt;&#x2F;span&gt;</span><br><span class="line"></span><br><span class="line">Just for reference, let me put an example of very less contention. This is with 16 threads oltp-read-write workload on x86_64 based VM with 16 CPU.</span><br></pre></td></tr></table></figure><p>RW-shared spins 39578, rounds 424553, OS waits 7329<br>RW-excl spins 5828, rounds 78225, OS waits 1341<br>RW-sx spins 11666, rounds 67297, OS waits 449<br>Spin rounds per wait: 10.73 RW-shared, 13.42 RW-excl, 5.77 RW-sx</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* Flow invokes spin-loop 39K times.</span><br><span class="line">* Only 7K (that is approximately 20% of the spin-loop) caused OS-waits.</span><br><span class="line">* Per spin-cycle average too is limited to 10.</span><br><span class="line"></span><br><span class="line">**Conclusion:** Use-case represents low contention.</span><br><span class="line"></span><br><span class="line">&lt;em&gt;16 thread oltp-read-write workload on 24 vCPU x86_64 machine&lt;&#x2F;em&gt;</span><br><span class="line"></span><br><span class="line">### &lt;span style&#x3D;&quot;color:#1aa260&quot;&gt;Note on tuning&lt;&#x2F;span&gt;</span><br><span class="line"></span><br><span class="line">Remember the high contention case we saw above. By tuning some things + code changes I could reduce the contention for shared-spins significantly.</span><br></pre></td></tr></table></figure><p>RW-shared spins 318800, rounds 13856732, OS waits 374634<br>RW-excl spins 35759, rounds 656955, OS waits 22310<br>RW-sx spins 10750, rounds 226315, OS waits 5598<br>Spin rounds per wait: 43.47 RW-shared, 18.37 RW-excl, 21.05 RW-sx</p><p>```</p><p>Rounds per spins-cycles: average of 60 -&gt; 43 <br><br>OS-wait per spin-cycle: 1.75 -&gt; 1.17</p><p>Is this good.? Not really. There are multiple factors to consider.</p><ul><li>Do you see improvement in TPS?</li><li>Sometime it may be suggested to simply increase the PAUSE loop. But increasing PAUSE loop beyond some point can cause extended spin-cycle wasting precious CPU cycles especially if that causes it to land back in OS-wait. (Does this help more in HT cases vs multi-core case).</li><li>Also, as pointed above, processor generation and type affect the PAUSE loop latency.</li></ul><p>There are multiple factors to consider. Even I am exploring this to see how we can tune this for all kinds of CPUs. I will blog more about it once I get some good solid algorithms on this front (or maybe we can develop some automated, self-adjustable or adaptive algorithms) so users don’t need to worry about it.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><span style="color:#4885ed">Conclusion</span></h2><p>As we saw above rw-locks stats can help us get good insight on understanding the contention of the system. Of-course it is not the only in-sight about InnoDB contention as mutexes are not covered as part of these stats. Tuning could be challenging but over-tuning can affect performance in the wrong way too.</p><br><em>If you have more questions/queries do let me know. Will try to answer them.</em>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: Krunal Bauskar&lt;/p&gt;
&lt;p&gt;InnoDB uses mutexes for exclusive access and rw-locks for the shared access of the resources. rw-locks are used to control access to the common shared resources like buffer pool pages, tablespaces, adaptive search systems, data-dictionary, informaton_schema, etc… In short, rw-locks play a very important role in the InnoDB system and so tracking and monitoring them is important too.&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Web开源服务之ARM64现状</title>
    <link href="https://kunpengcompute.github.io/2020/04/10/web-kai-yuan-fu-wu-zhi-arm64-xian-zhuang/"/>
    <id>https://kunpengcompute.github.io/2020/04/10/web-kai-yuan-fu-wu-zhi-arm64-xian-zhuang/</id>
    <published>2020-04-10T03:03:00.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>作者: <a href="https://github.com/wangxiyuan">王玺源</a></p><p>社区核心参与者：<a href="https://github.com/martin-g">Martin Grigorov</a>、<a href="https://www.linkedin.com/in/mikerumph/">Michael Rumph</a></p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>开源界中Web服务众多,但其中很多软件对ARM64的支持并不理想。或是没有官方CI测试保证代码质量，或是在ARM64上的性能明显差于X86_64，甚至有的服务根本无法在ARM上运行。为了完善Web领域的ARM64生态，我们参与了主流的几个开源社区，旨在推动Web on ARM64。以下是我们近期的一些进展，以供大家参考。</p><a id="more"></a><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>我们目前参与了主流的共9个Web相关项目。如下所示</p><table><thead><tr><th>项目</th><th>主要语言</th></tr></thead><tbody><tr><td>Apache Httpd Server</td><td>C</td></tr><tr><td>Apache Tomcat</td><td>Java</td></tr><tr><td>Memcached</td><td>C</td></tr><tr><td>Nginx</td><td>C</td></tr><tr><td>Lighttpd</td><td>C</td></tr><tr><td>JBoss/WildFly</td><td>Java</td></tr><tr><td>HAProxy</td><td>C</td></tr><tr><td>Squid</td><td>C++</td></tr><tr><td>Varnish Cache</td><td>C</td></tr></tbody></table><p>针对这些项目，我们按照以下三个方面循序渐进的推动中:</p><ol><li>能不能在ARM上运行</li><li>如何稳定在ARM上运行</li><li>怎么更好的在ARM上运行</li></ol><h1 id="能不能在ARM上运行"><a href="#能不能在ARM上运行" class="headerlink" title="能不能在ARM上运行"></a>能不能在ARM上运行</h1><p>我们可以看到这9大项目主要由Java和C/C++编写。</p><p>首先，像Python、Java这种自带runtime的语言天生就是跨平台的。这样的项目在ARM64平台上至少可以保证程序的可运行。</p><p>而C/C++项目则需要先编译成ARM64平台的目标可执行文件。这样的项目则需要先进行编译测试。</p><p>经过我们的测试，这9个Web项目都可以在ARM64上成功编译并运行。</p><h1 id="如何稳定在ARM上运行"><a href="#如何稳定在ARM上运行" class="headerlink" title="如何稳定在ARM上运行"></a>如何稳定在ARM上运行</h1><p>所谓稳定，包含两个方面：</p><ol><li>软件在ARM64上是否和在X86_64上行为一致？</li><li>随着代码更新迭代，软件在ARM64上是否持续可用？</li></ol><h2 id="行为一致"><a href="#行为一致" class="headerlink" title="行为一致"></a>行为一致</h2><p>我们常遇到两类行为一致的问题：</p><ol><li>同样的代码，不同的结果</li><li>同样的功能，不同的支持</li></ol><p>很遗憾，由于架构不同、底层实现不同等原因，很多软件的某些行为在X86_64和ARM64上的行为并不一致。</p><p>例如，之前的<a href="https://kunpengcompute.github.io/2020/04/08/arm-you-hua-he-java-math-ku-you-guan-de-na-xie-keng/">文章</a>提到的Java中Math计算结果的差异。</p><p>又或者某些功能依赖独有的平台特性或者特殊的第三方库，导致在X86_64上可以运行的功能，在ARM64上却执行失败。</p><p>例如我们发现WildFly官方发布的源码包中缺少了个别ARM64平台的<code>.so</code>文件，这就导致个别调用<code>.so</code>的功能不可用。</p><p>针对这种问题，我们需要打开代码逐个分析、逐个修复。保证所有测试在ARM64上全部通过。</p><h2 id="持续可用"><a href="#持续可用" class="headerlink" title="持续可用"></a>持续可用</h2><p>CI/CD是保证软件持续可用的重要方法。主流软件的CI系统都有X86_64平台的测试。而ARM64平台的少之又少。</p><p>针对这个问题，我们推动了这9个项目的ARM CI支持。除Lighttpd还在推动中以外，其他8个项目目前都已支持了ARM CI。甚至其中4个项目已经官方声明了ARM64的支持（详见附录）。</p><p>其中Httpd、Tomcat、Memcached、HAProxy和Varnish Cache通过Travis CI支持了ARM64测试。Nginx使用内部CI，对外不可见。Squid使用自己的树莓派。而JBOSS使用了我们捐献的基于Kunpeng 920的ARM虚拟机。同时我们也计划捐献同样的测试机到Lighttpd社区中。</p><p>随着ARM CI的落地，我们将持续保证ARM CI的稳定。我们相信在不久的将来，这9大核心Web项目都会官方声明ARM64的支持，并满足用户在ARM64上稳定、高效使用Web服务的需求。</p><h1 id="怎么更好的在ARM上运行"><a href="#怎么更好的在ARM上运行" class="headerlink" title="怎么更好的在ARM上运行"></a>怎么更好的在ARM上运行</h1><p>我们不仅希望软件在ARM64上能用，还在不断探索如何让软件在ARM64上用的好。其中<strong>性能优化</strong>是重中之重，也是我们未来一段时间的主要投入点。</p><p>例如，有些软件只实现了X86_64的汇编实现，但缺少ARM64的汇编代码。</p><p>又或者有些在X86_64上纯软实现的功能，可以在ARM64上通过下沉至硬编码的方式提高性能。</p><p>甚至还可以考虑如何最大化利用ARM64的多核优势，或规避ARM64的锁劣势等等。</p><p>关于性能优化的内容，我们将在以后的文章中针对不同的软件一一细说。敬请期待。</p><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><p>最后附上我们参与Web社区的总览表格及相关链接，感兴趣的同学可以进一步详读，有任何问题，欢迎留言。</p><table><thead><tr><th></th><th>Official arm64 CI</th><th>CI tool</th><th>Package in Downloads</th><th>Official ARM support</th></tr></thead><tbody><tr><td><a href="https://tomcat.apache.org/">Apache Tomcat</a></td><td><a href="https://github.com/apache/tomcat/commit/f386fbb4abaa3fe8f3b3df1da7d14f756c729e2e">YES</a></td><td><a href="https://github.com/apache/tomcat/blob/master/.travis.yml">TravisCI</a></td><td><a href="https://tomcat.apache.org/download-90.cgi">Binary</a></td><td><a href="https://tomcat.apache.org/ci.html#TravisCI">YES</a></td></tr><tr><td><a href="https://memcached.org/">Memcached</a></td><td><a href="https://github.com/memcached/memcached/pull/593">YES</a></td><td>1. <a href="http://build.memcached.org:8010/">BuildBot</a> 2. <a href="https://github.com/memcached/memcached/blob/master/.travis.yml">TravisCI</a></td><td><a href="https://memcached.org/downloads">Source Code</a></td><td><a href="https://github.com/memcached/memcached/wiki/Hardware">YES</a></td></tr><tr><td><a href="https://httpd.apache.org/">Apache httpd</a></td><td><a href="https://markmail.org/message/ajm3eouaqfhm22ox">YES</a></td><td><a href="https://github.com/apache/httpd/blob/trunk/.travis.yml">TravisCI</a></td><td><a href="http://httpd.apache.org/download.cgi">Source Code</a></td><td><a href="https://github.com/apache/httpd/blob/2.4.x/CHANGES#L17-L20">YES</a></td></tr><tr><td><a href="https://nginx.org/">NGINX</a></td><td><a href="https://mailman.nginx.org/pipermail/nginx-devel/2020-January/012943.html">YES</a></td><td>Internal</td><td>Only for <a href="https://nginx.org/en/linux_packages.html">Ubuntu</a> LTSs</td><td><a href="https://nginx.org/en/linux_packages.html#Ubuntu">YES</a></td></tr><tr><td><a href="https://www.lighttpd.net/">Lighttpd</a></td><td>NO</td><td><a href="https://ci.lighttpd.net/view/lighttpd1.4/job/lighttpd1.4/">Jenkins</a></td><td><a href="https://www.lighttpd.net/download/">Source Code</a></td><td>NO</td></tr><tr><td><a href="https://www.wildfly.org/">JBoss/Wildfly</a></td><td><a href="https://ci.wildfly.org/viewType.html?buildTypeId=WF_MasterLinuxArm64OpenJ911">YES</a></td><td><a href="https://ci.wildfly.org/">TeamCity</a></td><td><a href="https://wildfly.org/downloads/">Source Code</a></td><td>NO</td></tr><tr><td><a href="https://www.haproxy.org/">HAProxy</a></td><td><a href="https://github.com/haproxy/haproxy/commit/9bf2a1be89a6eaddb00f07b9d069a9a16c24c037">YES</a></td><td>1. <a href="https://cirrus-ci.com/github/haproxy/haproxy">CirrusCI</a> <br> 2. <a href="https://github.com/haproxy/haproxy/blob/master/.travis.yml">TravisCI</a></td><td><a href="http://www.haproxy.org/">Source Code</a></td><td><a href="https://www.haproxy.org/#plat">YES</a></td></tr><tr><td><a href="http://www.squid-cache.org/">Squid</a></td><td><a href="http://build.squid-cache.org/computer/arm64-rpi/">YES</a></td><td><a href="http://build.squid-cache.org/">Jenkins</a></td><td><a href="http://squid-cache.org/Versions/">Source Code</a></td><td>NO</td></tr><tr><td><a href="https://github.com/varnishcache/varnish-cache/">Varnish Cache</a></td><td><a href="https://github.com/varnishcache/varnish-cache/pull/3195">YES</a></td><td><a href="https://github.com/varnishcache/varnish-cache/blob/master/.travis.yml">Travis</a></td><td>1. <a href="https://varnish-cache.org/releases/index.html">Source Code</a> <br> 2. <a href="https://packagecloud.io/varnishcache">Package</a></td><td>NO</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: &lt;a href=&quot;https://github.com/wangxiyuan&quot;&gt;王玺源&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;社区核心参与者：&lt;a href=&quot;https://github.com/martin-g&quot;&gt;Martin Grigorov&lt;/a&gt;、&lt;a href=&quot;https://www.linkedin.com/in/mikerumph/&quot;&gt;Michael Rumph&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h1&gt;&lt;p&gt;开源界中Web服务众多,但其中很多软件对ARM64的支持并不理想。或是没有官方CI测试保证代码质量，或是在ARM64上的性能明显差于X86_64，甚至有的服务根本无法在ARM上运行。为了完善Web领域的ARM64生态，我们参与了主流的几个开源社区，旨在推动Web on ARM64。以下是我们近期的一些进展，以供大家参考。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Web" scheme="https://kunpengcompute.github.io/categories/Web/"/>
    
    
      <category term="Web" scheme="https://kunpengcompute.github.io/tags/Web/"/>
    
  </entry>
  
  <entry>
    <title>MySQL on x86 vs ARM</title>
    <link href="https://kunpengcompute.github.io/2020/04/08/mysql-on-x86-vs-arm/"/>
    <id>https://kunpengcompute.github.io/2020/04/08/mysql-on-x86-vs-arm/</id>
    <published>2020-04-08T10:16:43.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>作者: Krunal Bauskar</p><p>By and large this would be a topic of interest for most of us including me when I started to explore this space. Before we dwell into the numbers let’s first understand some basic differences between 2 architectures. Beyond being CISC and RISC let’s look at the important differences from MySQL perspective.</p><a id="more"></a><ul><li>Strong vs Weak memory model (weak memory model needs proper memory barrier while writing lock-free code).</li><li>Underlying hardware specific specialized instructions. For example: both now support crc32c hardware instructions but being low-level they are different ways to invoke them. For more differences checkout for x86-SSE/ARM-ACLE.</li><li>Cache Line differences. Most of the ARM processors tend to use bigger cache lines (128 bytes for all caches or a mix of 64/128 bytes).</li><li>Other sys-call level differences like: absence of PAUSE instructions with ARM and substitute instruction with very low latency failing to induce needed delay, sched_getcpu is costlier on ARM introducing challenges with use of lock-free construct, memory operations seems to show higher latency, etc…</li></ul><p>Community has contributed multiple patches around this space (Topic for another blog). Since MySQL just started supporting MySQL on ARM there are  few optimizations but most of the work is yet to be done.</p><h2 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a><span style="color:#4885ed">Performance</span></h2><p>Now let’s look at the most important aspect: Performance</p><p>We tested the performance of MySQL (current release 8.0.19) on x86 and ARM. Details of the test and machine are given below.</p><h3 id="Test-Setup"><a href="#Test-Setup" class="headerlink" title="Test Setup"></a><span style="color:#1aa260">Test Setup</span></h3><ul><li>24 vCPU/48 GB Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz for running MySQL on x86.</li><li>24 vCPU/48 GB ARM @ 2.60GHz for running MySQL on ARM</li><li>sysbench is running on a dedicated machine located in the same data-center.</li><li>sysbench steps:<ul><li>Load Tables. (Same seed db is reused for multiple runs so warmup is needed).</li><li>Checksum based warmup. Run checksum on all tables. For checksum, flow needs to fetch the rows in the buffer pool there-by causing it to warm up.</li><li>Query based warm up. Can skip but helpful if you are using adaptive hash indexes.</li><li>Execute TC (oltp-read-write/oltp-update-index/oltp-update-non-index/oltp-read-only/oltp-point-select)</li><li>Each TC is executed for N different scalability. Given 24 vCPU tried it for 1/2/4/8/16/32/128/256.</li><li>Before switching TC, an intermediate sleep is introduced to help flush changes from previous TC. This can’t ensure all changes are flushed but sleep of X secs ensures least impact on followup TC.</li><li>MySQL-Server Configuration:<ul><li>BP is large enough to accomodate complete data in-memory</li><li>For more details please check the <a href="https://github.com/mysqlonarm/benchmark-suites/blob/master/sysbench/conf/96tx1.5m_cpubound.cnf">following configuration details</a></li></ul></li></ul></li></ul><p></p><br> Details of running the scripts and automated test-script to invoke sysbench are also [available here](https://github.com/mysqlonarm/benchmark-suites)<h3 id="Run-specific-details"><a href="#Run-specific-details" class="headerlink" title="Run specific details:"></a><span style="color: #1aa260">Run specific details:</span></h3><ul><li>Table: 96-tables * 1.5 million (data-size= 34GB)</li><li>Buffer Pool: 36GB</li><li>Redo-Log: 4GB*2</li><li>TC-run-time: 300 secs</li><li>TC-warmup: 60 (sysbench –warmup-time)</li><li>workload-query-based warmup: 600</li><li>change-over-sleep: 180</li><li>checksum-based-warmup: enabled</li><li>data-storage: 300GB (support for 16500 IOPS (nullify effect of Burst IOPS)).</li></ul><p><font size="3"><em>Note: Frequency Scaling (FS). Given ARM is running @ 2.6 GHz vs x86 is running @ 3.0 GHz. Comparing them directly is not fair. In order to compensate for the frequency difference, graphs also add frequency-scaled tps/qps for ARM (ARM-fscaled simply extrapolate original ARM tps/qps number by (3/2.6) factor). In real life, the factor could be a bit on the higher side given increasing CPU frequency can affect contention graphs and wait cycles.</em></font></p><br><hr><h3 id="1-Point-Select"><a href="#1-Point-Select" class="headerlink" title="1. Point Select:"></a><span style="color: #de5246"><ins>1. Point Select:</ins></span></h3><img src="https://mysqlonarm.github.io/images/blog3/ARM-vs-x86-ps.png" width="100%"/><table><thead><tr><th>threads</th><th>ARM (qps)</th><th>x86 (qps)</th><th>ARM (qps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>6696</td><td>6439</td><td>7726</td><td>4</td><td>20</td></tr><tr><td>2</td><td>12482</td><td>11774</td><td>14402</td><td>6</td><td>22</td></tr><tr><td>4</td><td>23881</td><td>21308</td><td>27555</td><td>12</td><td>29</td></tr><tr><td>8</td><td>45993</td><td>42110</td><td>53069</td><td>9</td><td>26</td></tr><tr><td>16</td><td>88517</td><td>81239</td><td>102135</td><td>9</td><td>26</td></tr><tr><td>32</td><td>142974</td><td>136724</td><td>164970</td><td>5</td><td>21</td></tr><tr><td>64</td><td>198839</td><td>212484</td><td>229430</td><td>-6</td><td>8</td></tr><tr><td>128</td><td>217778</td><td>241555</td><td>251282</td><td>-10</td><td>4</td></tr><tr><td>256</td><td>209797</td><td>224009</td><td>242073</td><td>-6</td><td>8</td></tr></tbody></table><h4 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis:"></a>Analysis:</h4><ul><li>ARM performs better than x86 for lower scalability but fails to scale at same rate with increasing scalability.</li><li>With frequency scaling applied ARM continues to beat x86 despite of the scalability issues.</li></ul><br><hr><h3 id="2-Read-Only"><a href="#2-Read-Only" class="headerlink" title="2. Read Only:"></a><span style="color: #de5246"><ins>2. Read Only:</ins></span></h3><img src="https://mysqlonarm.github.io/images/blog3/ARM-vs-x86-ro.png" width="100%"/><table><thead><tr><th>threads</th><th>ARM (qps)</th><th>x86 (qps)</th><th>ARM (qps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>5222</td><td>5259</td><td>6025</td><td>-1</td><td>15</td></tr><tr><td>2</td><td>10333</td><td>10200</td><td>11923</td><td>1</td><td>17</td></tr><tr><td>4</td><td>19176</td><td>19349</td><td>22126</td><td>-1</td><td>14</td></tr><tr><td>8</td><td>36881</td><td>37035</td><td>42555</td><td>0</td><td>15</td></tr><tr><td>16</td><td>70337</td><td>67065</td><td>81158</td><td>5</td><td>21</td></tr><tr><td>32</td><td>109207</td><td>113210</td><td>126008</td><td>-4</td><td>11</td></tr><tr><td>64</td><td>139294</td><td>164148</td><td>160724</td><td>-15</td><td>-2</td></tr><tr><td>128</td><td>151382</td><td>175872</td><td>174672</td><td>-14</td><td>-1</td></tr><tr><td>256</td><td>149136</td><td>164382</td><td>172080</td><td>-9</td><td>5</td></tr></tbody></table><h4 id="Analysis-1"><a href="#Analysis-1" class="headerlink" title="Analysis:"></a>Analysis:</h4><ul><li>ARM is almost on par with x86 for lower scalability but again fails to scale for higher scalability.</li><li>With frequency scaling applied ARM continues to beat x86 (in most cases).</li></ul><br><hr><h3 id="3-Read-Write"><a href="#3-Read-Write" class="headerlink" title="3. Read Write:"></a><span style="color: #de5246"><ins>3. Read Write:</ins></span></h3><img src="https://mysqlonarm.github.io/images/blog3/ARM-vs-x86-rw.png" width="100%"/><table><thead><tr><th>threads</th><th>ARM (tps)</th><th>x86 (tps)</th><th>ARM (tps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>137</td><td>149</td><td>158</td><td>-8</td><td>6</td></tr><tr><td>2</td><td>251</td><td>273</td><td>290</td><td>-8</td><td>6</td></tr><tr><td>4</td><td>462</td><td>502</td><td>533</td><td>-8</td><td>6</td></tr><tr><td>8</td><td>852</td><td>920</td><td>983</td><td>-7</td><td>7</td></tr><tr><td>16</td><td>1539</td><td>1678</td><td>1776</td><td>-8</td><td>6</td></tr><tr><td>32</td><td>2556</td><td>2906</td><td>2949</td><td>-12</td><td>1</td></tr><tr><td>64</td><td>3770</td><td>5158</td><td>4350</td><td>-27</td><td>-16</td></tr><tr><td>128</td><td>5015</td><td>8131</td><td>5787</td><td>-38</td><td>-29</td></tr><tr><td>256</td><td>5676</td><td>8562</td><td>6549</td><td>-34</td><td>-24</td></tr></tbody></table><h4 id="Analysis-2"><a href="#Analysis-2" class="headerlink" title="Analysis:"></a>Analysis:</h4><ul><li>Pattern is different with read-write workload. ARM starts lagging. Frequency scaling helps ease this lag for lower scalability but increasing scalability continues to increase the gap.</li></ul><br><hr><h3 id="4-Update-Index"><a href="#4-Update-Index" class="headerlink" title="4. Update Index:"></a><span style="color: #de5246"><ins>4. Update Index:</ins></span></h3><img src="https://mysqlonarm.github.io/images/blog3/ARM-vs-x86-ui.png" width="100%"/><table><thead><tr><th>threads</th><th>ARM (tps)</th><th>x86 (tps)</th><th>ARM (tps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>328</td><td>373</td><td>378</td><td>-12</td><td>1</td></tr><tr><td>2</td><td>623</td><td>768</td><td>719</td><td>-19</td><td>-6</td></tr><tr><td>4</td><td>1060</td><td>1148</td><td>1223</td><td>-8</td><td>7</td></tr><tr><td>8</td><td>1905</td><td>2028</td><td>2198</td><td>-6</td><td>8</td></tr><tr><td>16</td><td>3284</td><td>3590</td><td>3789</td><td>-9</td><td>6</td></tr><tr><td>32</td><td>5543</td><td>6275</td><td>6396</td><td>-12</td><td>2</td></tr><tr><td>64</td><td>9138</td><td>10381</td><td>10544</td><td>-12</td><td>2</td></tr><tr><td>128</td><td>13879</td><td>16868</td><td>16014</td><td>-18</td><td>-5</td></tr><tr><td>256</td><td>19954</td><td>25459</td><td>23024</td><td>-22</td><td>-10</td></tr></tbody></table><h4 id="Analysis-3"><a href="#Analysis-3" class="headerlink" title="Analysis:"></a>Analysis:</h4><ul><li>Frequency scaled ARM continues to perform on par/better with x86 (except for heavy contention use-cases).</li></ul><br><hr><h3 id="5-Update-Non-Index"><a href="#5-Update-Non-Index" class="headerlink" title="5. Update Non-Index:"></a><span style="color: #de5246"><ins>5. Update Non-Index:</ins></span></h3><img src="https://mysqlonarm.github.io/images/blog3/ARM-vs-x86-uni.png" width="100%"/><table><thead><tr><th>threads</th><th>ARM (tps)</th><th>x86 (tps)</th><th>ARM (tps - fscaled (FS))</th><th>% ARM-vs-x86</th><th>% ARM (FS)-vs-x86</th></tr></thead><tbody><tr><td>1</td><td>328</td><td>373</td><td>378</td><td>-12</td><td>1</td></tr><tr><td>2</td><td>588</td><td>686</td><td>678</td><td>-14</td><td>-1</td></tr><tr><td>4</td><td>1075</td><td>1118</td><td>1240</td><td>-4</td><td>11</td></tr><tr><td>8</td><td>1941</td><td>2043</td><td>2240</td><td>-5</td><td>10</td></tr><tr><td>16</td><td>3367</td><td>3662</td><td>3885</td><td>-8</td><td>6</td></tr><tr><td>32</td><td>5681</td><td>6438</td><td>6555</td><td>-12</td><td>2</td></tr><tr><td>64</td><td>9328</td><td>10631</td><td>10763</td><td>-12</td><td>1</td></tr><tr><td>128</td><td>14158</td><td>17245</td><td>16336</td><td>-18</td><td>-5</td></tr><tr><td>256</td><td>20377</td><td>26367</td><td>23512</td><td>-23</td><td>-11</td></tr></tbody></table><h4 id="Analysis-4"><a href="#Analysis-4" class="headerlink" title="Analysis:"></a>Analysis:</h4><ul><li>Frequency scaled ARM continues to perform on par/better with x86 (except for heavy contention use-cases).</li></ul><br><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><span style="color:#4885ed">Conclusion</span></h2><p>There are some important observations we can make:</p><ul><li>For read only workload MySQL on ARM continues to perform on-par with MySQL on x86. </li><li>For write involving workload MySQL on ARM starts lagging a bit but if we consider frequency scaling things start getting  better.</li><li>Frequency scaling is not a real life parameter so we should consider the price-per-performance ratio. This could be a topic in itself but just a quick fact: ARM instance is 66% cheaper than x86 (24U48G same one we used).</li><li>There is a pattern that we can observe. ARM workloads are very well scalable till it hits the CPU limits. With increasing scalability, contention increases and ARM starts lagging. This is expected since mutexes/contention hot-spots were all tuned for x86 (for example: spin-lock). But now that MySQL officially supports ARM and the growing ARM community and interest, it would be tuned for ARM too.</li></ul><p>To summarize, MySQL on ARM is a worth exploring option from a cost and performance perspective.</p><br><em>If you have more questions/queries do let me know. Will try to answer them.</em>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: Krunal Bauskar&lt;/p&gt;
&lt;p&gt;By and large this would be a topic of interest for most of us including me when I started to explore this space. Before we dwell into the numbers let’s first understand some basic differences between 2 architectures. Beyond being CISC and RISC let’s look at the important differences from MySQL perspective.&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>ARM优化和Java Math库有关的那些坑</title>
    <link href="https://kunpengcompute.github.io/2020/04/08/arm-you-hua-he-java-math-ku-you-guan-de-na-xie-keng/"/>
    <id>https://kunpengcompute.github.io/2020/04/08/arm-you-hua-he-java-math-ku-you-guan-de-na-xie-keng/</id>
    <published>2020-04-08T03:44:29.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>作者：<a href="https://github.com/Yikun">姜逸坤</a></p><h2 id="1-起初"><a href="#1-起初" class="headerlink" title="1. 起初"></a>1. 起初</h2><p>最近在进行ARM切换的过程中发现了很多因为Java Math库在不同的平台上的精度不同导致用例失败，我们以Math.log为例，做一下简单的分析。下面是一个简单的计算log(3)的示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hello</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Math.log(3): "</span> + Math.log(<span class="number">3</span>));</span><br><span class="line">        System.out.println(<span class="string">"StrictMath.log(3): "</span> + StrictMath.log(<span class="number">3</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><p>我们发现，在x86下，Math的结果为<code>1.0986122886681098</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> on x86</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> java Hello</span></span><br><span class="line">Math.log(3): 1.0986122886681098</span><br><span class="line">StrictMath.log(3): 1.0986122886681096</span><br></pre></td></tr></table></figure><p>而aarch64的结果为<code>1.0986122886681096</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> on aarch64</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> java Hello</span></span><br><span class="line">Math.log(3): 1.0986122886681096</span><br><span class="line">StrictMath.log(3): 1.0986122886681096</span><br></pre></td></tr></table></figure><p>而在Java 8的<a href="https://docs.oracle.com/javase/8/docs/api/java/lang/Math.html">官方文档</a>中，对此有明确说明：</p><blockquote><p>Unlike some of the numeric methods of class StrictMath, all implementations of the equivalent functions of class Math are not defined to return the bit-for-bit same results. This relaxation permits better-performing implementations where strict reproducibility is not required.</p></blockquote><p>因此，结论是：<strong>Math的结果有可能是不精确的，如果结果对精度有苛求，那么请使用StrictMath</strong>。</p><p>在此，我们留下2个疑问：</p><ol><li>为什么说Math的实现不是<code>the bit-for-bit same results</code>？</li><li>Math是怎么实现在各个架构下<code>better-performing implementations</code>的？</li></ol><h2 id="2-深度探索一下Math的实现"><a href="#2-深度探索一下Math的实现" class="headerlink" title="2. 深度探索一下Math的实现"></a>2. 深度探索一下Math的实现</h2><p>为了能够更清晰的看到StrictMath的实现，我们深入的看了下JDK的实现。</p><h3 id="2-1-Math和StrictMath的基本实现"><a href="#2-1-Math和StrictMath的基本实现" class="headerlink" title="2.1 Math和StrictMath的基本实现"></a>2.1 Math和StrictMath的基本实现</h3><p>我们从Math.log和StrictMath.log的实现为例，进行深入学习：</p><ol><li><a href="http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/8f8015daf928/src/share/classes/java/lang/Math.java#l293">Math.log的代码</a>表面上很简单，就是直接调用StrictMath.log。<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">double</span> <span class="title">log</span><span class="params">(<span class="keyword">double</span> a)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> StrictMath.log(a); <span class="comment">// default impl. delegates to StrictMath</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><a href="http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/8f8015daf928/src/share/classes/java/lang/StrictMath.java#l231">StrictMath的代码</a>，会调用<a href="http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/8f8015daf928/src/share/native/java/lang/StrictMath.c#l76">StrictMath.c</a>中的方法，最终会调用fdlibm的<a href="http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/8f8015daf928/src/share/native/java/lang/fdlibm/src/e_log.c">e_log.c</a>的实现。</li></ol><p>总体的实现和下图类似：<br><img src="https://user-images.githubusercontent.com/1736354/78893132-569ff880-7a9d-11ea-85dc-4652c9bf85f8.png" alt="image"></p><p>对于StrictMath来说，没有什么黑科技，最终的实现就是e_log.c的ieee754标准实现，是通过C语言实现的，所以在各个平台的表现是一样的，整个流程如图中蓝色部分。感兴趣的同学可以看<a href="http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/8f8015daf928/src/share/native/java/lang/fdlibm/src/e_log.c">e_log.c</a>的源码实现即可。</p><h3 id="2-2-Math的黑科技"><a href="#2-2-Math的黑科技" class="headerlink" title="2.2 Math的黑科技"></a>2.2 Math的黑科技</h3><p>回到我们最初的起点，再加上一个问题：</p><ol><li>为什么说Math的实现不是<code>the bit-for-bit same results</code>？</li><li>Math是怎么实现在各个架构下<code>better-performing implementations</code>的？</li><li>既然Math的实现，也是直接调用StrictMath，为什么结果确不一样呢？</li></ol><p>原来，JVM为了让各个arch的CPU能够充分的发挥自己CPU的优势，会根据架构不同，会通过Hotspot intrinsics替换掉Math函数的实现，我们可以从代码<a href="http://hg.openjdk.java.net/jdk8u/jdk8u/hotspot/file/dae2d83e0ec2/src/share/vm/classfile/vmSymbols.hpp#l598">vmSymbols.hpp</a>看到，Math的很多实现都被替换掉了。log的替换类似于：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">do_intrinsic(_dlog, java_lang_Math, log_name, double_double_signature, F_S)</span><br></pre></td></tr></table></figure><p>最终，Math的调用为下图红色部分：</p><p><img src="https://user-images.githubusercontent.com/1736354/78893234-8f3fd200-7a9d-11ea-903a-311c8c3cc836.png" alt="image"></p><p>log的实现:</p><ul><li>在x86下，最终其实调用的是<a href="http://hg.openjdk.java.net/jdk8u/jdk8u/hotspot/file/dae2d83e0ec2/src/cpu/x86/vm/assembler_x86.cpp#l4140">assembler_x86.cpp</a>中的<code>flog</code>实现:<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Assembler::flog</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  fldln2();</span><br><span class="line">  fxch();</span><br><span class="line">  fyl2x();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>而在aarch64下，我们可以从<a href="http://hg.openjdk.java.net/jdk/jdk/file/e53ec3b362f4/src/hotspot/cpu/">src/hotspot/cpu/</a>目录下看到，aarch64并未实现优化版本。因此，实际aarch64调用的就是标准的StrictMath。</li></ul><p>正因如此，x86汇编的计算结果的差异导致了x86和aarch64结果在Math.log差异。</p><p>当然，aarch64也在JDK 11中，对部分的Math接口做了加速实现，有兴趣可以看看<a href="https://bugs.openjdk.java.net/browse/JDK-8189104">JEP 315: Improve Aarch64 Intrinsics</a>的实现。</p><h2 id="3-toRadians的小插曲"><a href="#3-toRadians的小插曲" class="headerlink" title="3. toRadians的小插曲"></a>3. toRadians的小插曲</h2><p>在ARM优化过程中，有的是因为Math库和StrictMath不同的实现造成结果不同，所以我们如果对精度要求非常高，直接切到StrictMath即可。</p><p>但有的函数，由于在Java大版本升级的过程中，出现了一些实现的差异，先看一个简单的Java程序</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hello</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">System.out.println(<span class="string">"Math.toRadians(0.33): "</span> + Math.toRadians(<span class="number">0.33</span>));</span><br><span class="line">System.out.println(<span class="string">"StrictMath.toRadians(0.33): "</span> + StrictMath.toRadians(<span class="number">0.33</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们分别看看在Java11和Java8的结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ &#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;java-11-openjdk-amd64&#x2F;bin&#x2F;java Hello</span><br><span class="line">Math.toRadians(0.33): 0.005759586531581287</span><br><span class="line">StrictMath.toRadians(0.33): 0.005759586531581287</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ &#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;java-1.8.0-openjdk-amd64&#x2F;bin&#x2F;java Hello</span><br><span class="line">Math.toRadians(0.33): 0.005759586531581288</span><br><span class="line">StrictMath.toRadians(0.33): 0.005759586531581288</span><br></pre></td></tr></table></figure><p>最后一位很奇怪的差了1，我们继续深入进去看到toRadians的实现：</p><ul><li><a href="https://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/lang/Math.java#l236">Java8的实现</a>为：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Java 8 </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">double</span> <span class="title">toDegrees</span><span class="params">(<span class="keyword">double</span> angrad)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> angrad * <span class="number">180.0</span> / PI;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><a href="http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/java.base/share/classes/java/lang/Math.java#l253">Java11的实现</a>为：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">double</span> DEGREES_TO_RADIANS = <span class="number">0.017453292519943295</span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">double</span> <span class="title">toRadians</span><span class="params">(<span class="keyword">double</span> angdeg)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> angdeg * DEGREES_TO_RADIANS;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>原来在Java11的实现中，为了优化性能，将<code>* 180.0 / PI</code>提前算好了，这样每次只用乘以乘数即可，从而化简了计算。这也最终导致了，Java8和Java11在精度上有一些差别。</li></ul><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2><ul><li>Math在各个arch下的实现不同，精度也不同，如果对精度要求很高，可以使用StrictMath。</li><li>Java不同版本的优化，也有可能导致Math库的精度不同</li><li>Math库在实现时，利用intrinsics机制，把各个arch下Math的实现换掉了，从而充分的发挥各个CPU自身的优势。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：&lt;a href=&quot;https://github.com/Yikun&quot;&gt;姜逸坤&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;1-起初&quot;&gt;&lt;a href=&quot;#1-起初&quot; class=&quot;headerlink&quot; title=&quot;1. 起初&quot;&gt;&lt;/a&gt;1. 起初&lt;/h2&gt;&lt;p&gt;最近在进行ARM切换的过程中发现了很多因为Java Math库在不同的平台上的精度不同导致用例失败，我们以Math.log为例，做一下简单的分析。下面是一个简单的计算log(3)的示例：&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Hello&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        System.out.println(&lt;span class=&quot;string&quot;&gt;&quot;Math.log(3): &quot;&lt;/span&gt; + Math.log(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        System.out.println(&lt;span class=&quot;string&quot;&gt;&quot;StrictMath.log(3): &quot;&lt;/span&gt; + StrictMath.log(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/categories/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
    
      <category term="基础库" scheme="https://kunpengcompute.github.io/tags/%E5%9F%BA%E7%A1%80%E5%BA%93/"/>
    
      <category term="Java" scheme="https://kunpengcompute.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>ARM CPU Vendor 及 Part ID 映射关系（持续更新）</title>
    <link href="https://kunpengcompute.github.io/2020/04/03/arm-cpu-vendor-ji-part-id-ying-she-guan-xi-chi-xu-geng-xin/"/>
    <id>https://kunpengcompute.github.io/2020/04/03/arm-cpu-vendor-ji-part-id-ying-she-guan-xi-chi-xu-geng-xin/</id>
    <published>2020-04-03T04:51:40.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>作者：郑振宇</p><p>根据<a href="https://developer.arm.com/docs/ddi0595/b/aarch64-system-registers/midr_el1">ARM CPU官方技术手册</a>，ARM CPU的CPU型号、Vendor、版本等信息存于<code>MIDR_EL1</code>寄存器中:<br><img width="492" alt="MIDR" src="https://user-images.githubusercontent.com/10849016/78315745-79f41080-7590-11ea-97cd-787ed68d6933.PNG"><br>其中从低至高第0-3 bit表示<code>revision</code>，代表固件版本的小版本号，如r1p3中的p3；<br>第4-15 bit表示<code>part number(id)</code>，代表这款CPU在所在<code>vendor</code>产品中定义的产品代码，如在<code>HiSilicon</code>产品中，<code>part_id=0xd01</code>代表<code>Kunpeng-920</code>芯片；<br>第16-19 bit表示<code>architecture</code>，即架构版本，<code>0x8</code>即ARMv8；<br>第20-23 bit表示<code>variant</code>，即固件版本的大版本号，如r1p3中的r1；<br>第24-31 bit表示<code>implementer</code>，即<code>vendor id</code>，如<code>vendor_id=0x48</code>表示<code>HiSilicon</code>。</p><a id="more"></a><p>想要知道一款ARM CPU的具体型号，则需要首先解析<code>vendor_id(implementer)</code> 然后再在该Vendor的所有型号中匹配<code>part_id</code>，才能获取到具体的信息；这里列出目前系统中已有的Vendor列表和其ID对应关系，以及主流厂商的主要型号映射关系：</p><h1 id="Vendor映射关系："><a href="#Vendor映射关系：" class="headerlink" title="Vendor映射关系："></a>Vendor映射关系：</h1><table><thead><tr><th>Vendor Name</th><th>Vendor ID</th></tr></thead><tbody><tr><td>ARM</td><td>0x41</td></tr><tr><td>Broadcom</td><td>0x42</td></tr><tr><td>Cavium</td><td>0x43</td></tr><tr><td>DigitalEquipment</td><td>0x44</td></tr><tr><td>HiSilicon</td><td>0x48</td></tr><tr><td>Infineon</td><td>0x49</td></tr><tr><td>Freescale</td><td>0x4D</td></tr><tr><td>NVIDIA</td><td>0x4E</td></tr><tr><td>APM</td><td>0x50</td></tr><tr><td>Qualcomm</td><td>0x51</td></tr><tr><td>Marvell</td><td>0x56</td></tr><tr><td>Intel</td><td>0x69</td></tr></tbody></table><h1 id="型号映射关系"><a href="#型号映射关系" class="headerlink" title="型号映射关系"></a>型号映射关系</h1><h2 id="ARM"><a href="#ARM" class="headerlink" title="ARM"></a>ARM</h2><table><thead><tr><th>Part ID</th><th>Model Name</th></tr></thead><tbody><tr><td>0xd03</td><td>Cortex-a53</td></tr><tr><td>0xd07</td><td>Cortex-a57</td></tr><tr><td>0xd08</td><td>Cortex-a72</td></tr></tbody></table><h2 id="Broadcom"><a href="#Broadcom" class="headerlink" title="Broadcom"></a>Broadcom</h2><table><thead><tr><th>Part ID</th><th>Model Name</th></tr></thead><tbody><tr><td>0x0f</td><td>Brahma B15</td></tr><tr><td>0x100</td><td>Brahma B53</td></tr></tbody></table><h2 id="Cavium"><a href="#Cavium" class="headerlink" title="Cavium"></a>Cavium</h2><table><thead><tr><th>Part ID</th><th>Model Name</th></tr></thead><tbody><tr><td>0x0af</td><td>Thunder X2 29xx</td></tr></tbody></table><h2 id="Qualcomm"><a href="#Qualcomm" class="headerlink" title="Qualcomm"></a>Qualcomm</h2><table><thead><tr><th>Part ID</th><th>Model Name</th></tr></thead><tbody><tr><td>0xc00</td><td>Falkor</td></tr></tbody></table><h2 id="HiSilicon"><a href="#HiSilicon" class="headerlink" title="HiSilicon"></a>HiSilicon</h2><table><thead><tr><th>Part ID</th><th>Model Name</th></tr></thead><tbody><tr><td>0xd01</td><td>Kunpeng-920</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：郑振宇&lt;/p&gt;
&lt;p&gt;根据&lt;a href=&quot;https://developer.arm.com/docs/ddi0595/b/aarch64-system-registers/midr_el1&quot;&gt;ARM CPU官方技术手册&lt;/a&gt;，ARM CPU的CPU型号、Vendor、版本等信息存于&lt;code&gt;MIDR_EL1&lt;/code&gt;寄存器中:&lt;br&gt;&lt;img width=&quot;492&quot; alt=&quot;MIDR&quot; src=&quot;https://user-images.githubusercontent.com/10849016/78315745-79f41080-7590-11ea-97cd-787ed68d6933.PNG&quot;&gt;&lt;br&gt;其中从低至高第0-3 bit表示&lt;code&gt;revision&lt;/code&gt;，代表固件版本的小版本号，如r1p3中的p3；&lt;br&gt;第4-15 bit表示&lt;code&gt;part number(id)&lt;/code&gt;，代表这款CPU在所在&lt;code&gt;vendor&lt;/code&gt;产品中定义的产品代码，如在&lt;code&gt;HiSilicon&lt;/code&gt;产品中，&lt;code&gt;part_id=0xd01&lt;/code&gt;代表&lt;code&gt;Kunpeng-920&lt;/code&gt;芯片；&lt;br&gt;第16-19 bit表示&lt;code&gt;architecture&lt;/code&gt;，即架构版本，&lt;code&gt;0x8&lt;/code&gt;即ARMv8；&lt;br&gt;第20-23 bit表示&lt;code&gt;variant&lt;/code&gt;，即固件版本的大版本号，如r1p3中的r1；&lt;br&gt;第24-31 bit表示&lt;code&gt;implementer&lt;/code&gt;，即&lt;code&gt;vendor id&lt;/code&gt;，如&lt;code&gt;vendor_id=0x48&lt;/code&gt;表示&lt;code&gt;HiSilicon&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="虚拟化" scheme="https://kunpengcompute.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="https://kunpengcompute.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Linux下获取ARMv8-A CPU详情的3种方法</title>
    <link href="https://kunpengcompute.github.io/2020/04/03/linux-xia-huo-qu-armv8-a-cpu-xiang-qing-de-3-chong-fang-fa/"/>
    <id>https://kunpengcompute.github.io/2020/04/03/linux-xia-huo-qu-armv8-a-cpu-xiang-qing-de-3-chong-fang-fa/</id>
    <published>2020-04-03T01:49:34.000Z</published>
    <updated>2021-07-08T02:56:41.492Z</updated>
    
    <content type="html"><![CDATA[<p>作者：郑振宇</p><p>在ARM平台上进行软件适配时，经常遇到需要根据不同CPU的具体型号、额外属性等信息进行分支处理的需求，因而需要获取CPU的详情信息；ARM架构CPU与X86架构芯片在CPU详情信息的呈现上有很大不同。本文将简述ARM CPU与CPU详情相关的知识及在Linux下获取ARMv8-A CPU详情的三种方法。</p><a id="more"></a><h1 id="ARM-CPU中有关CPU详情的寄存器"><a href="#ARM-CPU中有关CPU详情的寄存器" class="headerlink" title="ARM CPU中有关CPU详情的寄存器"></a>ARM CPU中有关CPU详情的寄存器</h1><p>根据<a href="https://developer.arm.com/docs/ddi0595/b/aarch64-system-registers/midr_el1">ARM CPU官方技术手册</a>，ARM CPU的CPU型号、Vendor、版本等信息存于<code>MIDR_EL1</code>寄存器中:<br><img width="492" alt="MIDR" src="https://user-images.githubusercontent.com/10849016/78315745-79f41080-7590-11ea-97cd-787ed68d6933.PNG"><br>其中从低至高第0-3 bit表示<code>revision</code>，代表固件版本的小版本号，如r1p3中的p3；<br>第4-15 bit表示<code>part number(id)</code>，代表这款CPU在所在<code>vendor</code>产品中定义的产品代码，如在<code>HiSilicon</code>产品中，<code>part_id=0xd01</code>代表<code>Kunpeng-920</code>芯片；<br>第16-19 bit表示<code>architecture</code>，即架构版本，<code>0x8</code>即ARMv8；<br>第20-23 bit表示<code>variant</code>，即固件版本的大版本号，如r1p3中的r1；<br>第24-31 bit表示<code>implementer</code>，即<code>vendor id</code>，如<code>vendor_id=0x48</code>表示<code>HiSilicon</code>。</p><p>想要知道一款ARM CPU的具体型号，则需要首先解析<code>vendor_id(implementer)</code> 然后再在该Vendor的所有型号中匹配<code>part_id</code>，才能获取到具体的信息；这里列出目前系统中已有的Vendor列表和其ID对应关系</p><table><thead><tr><th align="center">Vendor Name</th><th align="center">Vendor ID</th></tr></thead><tbody><tr><td align="center">ARM</td><td align="center">0x41</td></tr><tr><td align="center">Broadcom</td><td align="center">0x42</td></tr><tr><td align="center">Cavium</td><td align="center">0x43</td></tr><tr><td align="center">DigitalEquipment</td><td align="center">0x44</td></tr><tr><td align="center">HiSilicon</td><td align="center">0x48</td></tr><tr><td align="center">Infineon</td><td align="center">0x49</td></tr><tr><td align="center">Freescale</td><td align="center">0x4D</td></tr><tr><td align="center">NVIDIA</td><td align="center">0x4E</td></tr><tr><td align="center">APM</td><td align="center">0x50</td></tr><tr><td align="center">Qualcomm</td><td align="center">0x51</td></tr><tr><td align="center">Marvell</td><td align="center">0x56</td></tr><tr><td align="center">Intel</td><td align="center">0x69</td></tr></tbody></table><p>而对于具体型号来说，对应关系则更为复杂，这里就不一一列举，可以参考<a href="https://kunpengcompute.github.io/2020/04/03/arm-cpu-vendor-ji-part-id-ying-she-guan-xi-chi-xu-geng-xin/">本站文章</a>或<a href="https://github.com/karelzak/util-linux/blob/master/sys-utils/lscpu-arm.c">util-linux/lscpu</a>工具中的相关具体实现来获取完整的映射关系，<code>lscpu</code>工具我们则将在后面的部分中进行介绍。</p><p>上面介绍过，除了CPU型号之外，我们通常还会关注CPU是否支持我们需要的特性(扩展指令集，CPU Flags, CPU features)；与X86相差较大(CPU features定义集中在EBX,ECX和EDX寄存器中)，ARM架构的这些特性分散于<a href="https://www.kernel.org/doc/Documentation/arm64/cpu-feature-registers.txt"><code>ID_PFR0_EL1</code>, <code>ID_PFR1_EL1</code>, <code>ID_DFR0_EL1</code>, <code>ID_ISAR0_EL1</code></a> 等等若干个专用寄存器中，解析起来难度较高，后面我们会详细讨论如何获取这些内容。</p><h1 id="在Linux下如何获取CPU详情信息"><a href="#在Linux下如何获取CPU详情信息" class="headerlink" title="在Linux下如何获取CPU详情信息"></a>在Linux下如何获取CPU详情信息</h1><p>在介绍具体的方法前，首先需要介绍一下ARMv8架构下的安全分层机制(Exception Level):<br><img src="https://user-images.githubusercontent.com/10849016/78317406-c9d4d680-7594-11ea-9a35-14158c262d1d.png" alt="image"><br>如上图所示，ARMv8架构是专为数据中心场景而设计的架构，相比较早的ARM架构，新增了<code>EL2</code>层用于实现硬件虚拟化；较高层的用户是无权直接限访问下一层的数据内容的，对于我们的场景来说，由上面介绍的内容中可以看到，前面所有介绍的寄存器都存在于<code>EL1</code>层，而我们通常使用的应用程序都处于<code>EL0</code>层，因此是无法直接访问到这些寄存器的。那么该如何读取这些内容呢？</p><h2 id="1-从文件节点获取"><a href="#1-从文件节点获取" class="headerlink" title="1. 从文件节点获取"></a>1. 从文件节点获取</h2><p>OS在启动时，会将底层硬件信息载入到相应的文件节点中，这样，位于<code>EL0</code>层的用户就可以通过读取这些文件节点来获取这些信息，比较常用的有两个：</p><ol><li><strong>/sys/devices/system/cpu:</strong><br>该文件夹下保存了较全的CPU信息文件，并按单个CPU进行区分，读取其中某一个的<code>regs</code>文件目录就可以获得相应的CPU详情信息，如我们尝试获取CPU0的相关信息：<br><img src="https://user-images.githubusercontent.com/10849016/78318946-9c8a2780-7598-11ea-8a31-3a76d422a055.png" alt="image"><br>可以看到，我们读取的仍然是<code>MIDR_EL1</code>寄存器相对应的信息，并且是未解析的数据，需要对应上文介绍的方法进行解析。并且目前没有在这个文件夹下找到<code>CPU Flags</code>的相关的信息，如果后续找到其所在位置，会刷新。</li><li><strong>/proc/cpuinfo:</strong><br><img src="https://user-images.githubusercontent.com/10849016/78318499-731ccc00-7597-11ea-8229-de07a2e0db92.png" alt="image"><br>通过读取cpuinfo可以看到，通过这种方法获取的CPU详情，是进行过解析的，对原始数据进行了拆分，并且是包含了<code>CPU Flag</code>信息的，但仍与X86下的结果有较大不同，各个key所对应的信息仍然需要根据表单进行解析才能转变为人为可读的信息。</li></ol><h2 id="2-使用LSCPU命令读取"><a href="#2-使用LSCPU命令读取" class="headerlink" title="2.使用LSCPU命令读取"></a>2.使用LSCPU命令读取</h2><p>Linux内核的开发者显然也发现了上文介绍的两种方法获取信息不够全面且需要二次解析的问题，因此在Linux外围工具组<code>util-linux/lscpu</code>(<a href="https://en.wikipedia.org/wiki/Util-linux">wiki</a>)中进行了改进，从<a href="https://lwn.net/Articles/749882/">2.32</a>版本开始增加了对ARM平台CPU信息的解析，从而提供人为可读的内容(由于2019年11月才合入相关Patch，HiSilicon芯片的解析需要手动编译最新主干代码才能实现)。<br><img src="https://user-images.githubusercontent.com/10849016/78319822-95fcaf80-759a-11ea-92b3-41367a2ed5da.png" alt="image"><br>从上图可以看到，<code>lscpu</code>提供了非常丰富且直观的内容。</p><h2 id="3-使用内联汇编和辅助向量直接解析"><a href="#3-使用内联汇编和辅助向量直接解析" class="headerlink" title="3.使用内联汇编和辅助向量直接解析"></a>3.使用内联汇编和辅助向量直接解析</h2><p>上文介绍的两种方法相对来说比较简单，但需要进行读取文件、运行外部命令等操作；当想在自己的程序中引用上述信息时，速度会相对较慢且会引入新的依赖(lscpu)。因此最快速的方法是通过内联汇编直接读取并解析相应的寄存器；上文中已经提到，用户在<code>EL0</code>无法直接读取到位于<code>EL1</code>的寄存器内的内容，那么该如何去做呢？</p><p>ARM已经为我们准备好了一切，用户可以通过<code>MRS</code>指令将程序状态寄存器的内容传送到通用寄存器中，再进行进一步的解析，因此我们可以这样做：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* read the cpuid data from MIDR_EL1 register */</span></span><br><span class="line"><span class="keyword">asm</span>(<span class="string">"mrs %0, MIDR_EL1"</span> : <span class="string">"=r"</span> (cpuid));</span><br><span class="line">VIR_DEBUG(<span class="string">"CPUID read from register:  0x%016lx"</span>, cpuid);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* parse the coresponding part_id bits */</span></span><br><span class="line">data-&gt;pvr = cpuid&gt;&gt;<span class="number">4</span>&amp;<span class="number">0xFFF</span>;</span><br><span class="line"><span class="comment">/* parse the coresponding vendor_id bits */</span></span><br><span class="line">data-&gt;vendor_id = cpuid&gt;&gt;<span class="number">24</span>&amp;<span class="number">0xFF</span>;</span><br></pre></td></tr></table></figure><p>这样就可以快速的获取CPUID相关的具体内容，在根据表格进行映射即可获得Vendor和Model信息；</p><p>对于CPU Flags，由于牵扯到的寄存器众多，读者可以根据<a href="https://www.kernel.org/doc/Documentation/arm64/cpu-feature-registers.txt">ARM64 CPU Feature Registers</a>中的示例程序进行依次进行寄存器读取，再根据相应的映射关系进行解析，也可以使用下面将要介绍的可读性更高的另一种方法。</p><p>Linux内核提供了<a href="http://man7.org/linux/man-pages/man3/getauxval.3.html"><strong>getauxval()</strong></a>方法，用于读取辅助向量(auxiliary vector, 一个从内核到用户空间的信息交流机制)，通过读取相应的辅助向量，我们就能获取相应的硬件信息；辅助向量有很多，感兴趣的读者可以查看上面的链接，对于我们读取CPU Flags来说，关心的是<strong>AT_HWCAP</strong>这个辅助向量，通过<code>getauxval()</code>读取这个向量的值，可以获得整合过的CPU Flags信息，其bit定位规则则在<a href="https://github.com/torvalds/linux/blob/master/arch/arm64/include/uapi/asm/hwcap.h">hwcap.h</a>，当然，每种架构下的对应关系不相同，需要根据需要进行查找。</p><p>那么，我们就可以采用下面的方法进行解析：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/auxv.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* 通过移位Bit Mask来读取相应的标志位 */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BIT_SHIFTS(n)(UL(1) &lt;&lt; (n))</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> hwcaps = getauxval(AT_HWCAP);</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="comment">/* 目前ARMv8架构只有32种CPU Flags */</span></span><br><span class="line">    <span class="keyword">char</span> *<span class="built_in">list</span>[<span class="number">32</span>] = &#123;<span class="string">"fp\n"</span>, <span class="string">"asimd\n"</span>, <span class="string">"evtstrm\n"</span>, <span class="string">"aes\n"</span>, <span class="string">"pmull\n"</span>, <span class="string">"sha1"</span>,</span><br><span class="line">                              <span class="string">"sha2\n"</span>, <span class="string">"crc32\n"</span>, <span class="string">"atomics\n"</span>, <span class="string">"fphp\n"</span>, <span class="string">"asimdhp\n"</span>,</span><br><span class="line">                              <span class="string">"cpuid\n"</span>, <span class="string">"asimdrdm\n"</span>,<span class="string">"jscvt\n"</span>, <span class="string">"fcma\n"</span>, <span class="string">"lrcpc\n"</span>,</span><br><span class="line">                              <span class="string">"dcpop\n"</span>, <span class="string">"sha3\n"</span>, <span class="string">"sm3\n"</span>, <span class="string">"sm4\n"</span>, <span class="string">"asimddp\n"</span> ,</span><br><span class="line">                              <span class="string">"sha512\n"</span>, <span class="string">"sve\n"</span>, <span class="string">"asimdfhm\n"</span>, <span class="string">"dit\n"</span>, <span class="string">"uscat\n"</span>,</span><br><span class="line">                              <span class="string">"ilrcpc\n"</span>, <span class="string">"flagm\n"</span>, <span class="string">"ssbs\n"</span>, <span class="string">"sb\n"</span>, <span class="string">"paca\n"</span>,<span class="string">"pacg\n"</span>,&#125;;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i&lt; <span class="number">32</span>; i++)&#123;</span><br><span class="line"><span class="keyword">if</span> (hwcaps &amp; BIT_SHIFTS(i)) &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%s\n"</span>,<span class="built_in">list</span>[i]);</span><br><span class="line">&#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>或者，可以直接通过<code>hwcap.h</code>中预先定义好的宏来做bit mask:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/auxv.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;asm/hwcap.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">long</span> hwcaps= getauxval(AT_HWCAP);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(hwcaps &amp; HWCAP_AES)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"AES instructions are available\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(hwcaps &amp; HWCAP_CRC32)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"CRC32 instructions are available\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(hwcaps &amp; HWCAP_PMULL)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"PMULL/PMULL2 instructions that operate on 64-bit data are available\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(hwcaps &amp; HWCAP_SHA1)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"SHA1 instructions are available\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(hwcaps &amp; HWCAP_SHA2)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"SHA2 instructions are available\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者：郑振宇&lt;/p&gt;
&lt;p&gt;在ARM平台上进行软件适配时，经常遇到需要根据不同CPU的具体型号、额外属性等信息进行分支处理的需求，因而需要获取CPU的详情信息；ARM架构CPU与X86架构芯片在CPU详情信息的呈现上有很大不同。本文将简述ARM CPU与CPU详情相关的知识及在Linux下获取ARMv8-A CPU详情的三种方法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="虚拟化" scheme="https://kunpengcompute.github.io/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
    
      <category term="虚拟化" scheme="https://kunpengcompute.github.io/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Running MySQL on ARM. Does it work?</title>
    <link href="https://kunpengcompute.github.io/2020/03/31/running-mysql-on-arm-does-it-work/"/>
    <id>https://kunpengcompute.github.io/2020/03/31/running-mysql-on-arm-does-it-work/</id>
    <published>2020-03-31T07:11:02.000Z</published>
    <updated>2021-07-08T02:56:41.676Z</updated>
    
    <content type="html"><![CDATA[<p>作者: Krunal Bauskar 原文链接: <a href="https://mysqlonarm.github.io/Running-MySQL-on-ARM/">https://mysqlonarm.github.io/Running-MySQL-on-ARM/</a></p><p>I am sure most of you may have this question. In fact, I too had it before I started working on #mysqlonarm initiative. What does it take to run MySQL on ARM? Does it really work? What about dependencies? What kind of performance does it have? What about support? Is there enough community support? This could go on…..</p><p>Let’s try to answer these questions in simple question answer format.</p><a id="more"></a><h4 id="Q-Is-MySQL-supported-on-ARM"><a href="#Q-Is-MySQL-supported-on-ARM" class="headerlink" title="Q: Is MySQL supported on ARM?"></a>Q: Is MySQL supported on ARM?</h4><p>A: Yes, MySQL is officially supported on ARM. There are packages available that you can download from mysql.com site.</p><h4 id="Q-Which-OS-are-supported"><a href="#Q-Which-OS-are-supported" class="headerlink" title="Q: Which OS are supported?"></a>Q: Which OS are supported?</h4><p>A: Currently support is enabled for RHEL-7 &amp; 8/Oracle-Linux- 7 &amp; 8. I don’t see direct package support for other OS.</p><h4 id="Q-Can-we-build-it-from-source-code-for-other-OS-like-say-Ubuntu"><a href="#Q-Can-we-build-it-from-source-code-for-other-OS-like-say-Ubuntu" class="headerlink" title="Q: Can we build it from source code for other OS (like say Ubuntu)?"></a>Q: Can we build it from source code for other OS (like say Ubuntu)?</h4><p>A: Yes. It works. I have been using binaries built from source code (using mysql-8.0.19 tag current release tag) on Ubuntu-18.04 (Bionic Beaver). (Also, build it on CentOS if you want to go the source code way). This also means all needed dependencies are taken care off or are already available.</p><h4 id="Q-Are-supporting-tools-available-on-ARM"><a href="#Q-Are-supporting-tools-available-on-ARM" class="headerlink" title="Q: Are supporting tools available on ARM?"></a>Q: Are supporting tools available on ARM?</h4><p>A: Since packages are available and I was able to build it from source too the default utilities like mysql shell/mysqladmin/mysqlslap/mysqldump/etc… and tons of other things that default ships along with binaries are available. If you care about a specific tools do let me know I will check them out. For now I have tried percona-toolkit some selective tools and they too work.</p><h4 id="Q-Does-MariaDB-and-Percona-too-support-their-respective-server-flavor-on-ARM"><a href="#Q-Does-MariaDB-and-Percona-too-support-their-respective-server-flavor-on-ARM" class="headerlink" title="Q: Does MariaDB and Percona too support their respective server flavor on ARM?"></a>Q: Does MariaDB and Percona too support their respective server flavor on ARM?</h4><p>A: MariaDB Community Server packages (from MariaDB corporation) are available for ARM (CentOS7/Ubuntu-16.04/18.04). Tools for MariaDB server are not yet officially available on ARM.<br>Percona doesn’t yet officially support ARM but I was able to build it from source (MyRocks/TokuDB are not available).</p><h4 id="Q-Non-availability-of-tools-Can-that-block-my-progress-of-trying-MySQL-or-its-variants-on-ARM"><a href="#Q-Non-availability-of-tools-Can-that-block-my-progress-of-trying-MySQL-or-its-variants-on-ARM" class="headerlink" title="Q: Non-availability of tools. Can that block my progress of trying MySQL (or its variants) on ARM?"></a>Q: Non-availability of tools. Can that block my progress of trying MySQL (or its variants) on ARM?</h4><p>A: No. Since most of these tools talk mysql protocol you can of-course install them on x86 with server running on ARM. (if tool is not yet ported to ARM)</p><h4 id="Q-Is-there-enough-community-support"><a href="#Q-Is-there-enough-community-support" class="headerlink" title="Q: Is there enough community support?"></a>Q: Is there enough community support?</h4><p>A: MySQL on ARM is there for quite some time. There are active contributions from multiple vendors including ARM, Qualcomm, Huawei etc… and the community is growing rapidly. There is a lot of interest from all sections on optimizing MySQL on ARM. Lot of developers wanted to connect with this initiative. There are few challenges, most importantly non-availability of the hardware. If you are interested in contributing please talk to me (shoot me an email).</p><h4 id="Q-All-that-looks-good-What-about-performance"><a href="#Q-All-that-looks-good-What-about-performance" class="headerlink" title="Q: All that looks good. What about performance?"></a>Q: All that looks good. What about performance?</h4><p>A: This is a wide topic so I will be publishing multiple posts on this topic in the coming days but to put it in short performance is comparable. On other hand ARM instances should provide better price performance.</p><h4 id="Q-What-about-Support"><a href="#Q-What-about-Support" class="headerlink" title="Q: What about Support?"></a>Q: What about Support?</h4><p>Since packages are available officially from MySQL I presume their service offering should also cover ARM. Same with MariaDB. And of-course beyond official support there are common groups and independent developers.</p><h4 id="Command-to-build-MySQL-on-ARM"><a href="#Command-to-build-MySQL-on-ARM" class="headerlink" title="Command to build MySQL on ARM"></a>Command to build MySQL on ARM</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cmake .. -DWITH_NUMA&#x3D;1 -DDOWNLOAD_BOOST&#x3D;1 -DWITH_BOOST&#x3D;&lt;boost-dir&gt; -DCMAKE_INSTALL_PREFIX&#x3D;&lt;dir-to-install&gt;</span><br><span class="line">make -j &lt;num-of-cores&gt;</span><br></pre></td></tr></table></figure><p>So no special flag is needed to build MySQL on ARM. (Assumes you have installed standard dependencies). It defaults compiles with “CMAKE_BUILD_TYPE=RelWithDebInfo”</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><span style="color:#4885ed">Conclusion</span></h2><p>MySQL on ARM is reality and it is now officially supported with ever growing eco-system/community. So give it a try. It could be your next cost-saving options without comprising performance or functionality.</p><p><em>If you have more questions/queries do let me know. Will try to answer them</em></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: Krunal Bauskar 原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/Running-MySQL-on-ARM/&quot;&gt;https://mysqlonarm.github.io/Running-MySQL-on-ARM/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I am sure most of you may have this question. In fact, I too had it before I started working on #mysqlonarm initiative. What does it take to run MySQL on ARM? Does it really work? What about dependencies? What kind of performance does it have? What about support? Is there enough community support? This could go on…..&lt;/p&gt;
&lt;p&gt;Let’s try to answer these questions in simple question answer format.&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>让大数据生态在ARM架构下更顺滑</title>
    <link href="https://kunpengcompute.github.io/2020/03/30/rang-da-shu-ju-sheng-tai-zai-arm-jia-gou-xia-geng-shun-hua/"/>
    <id>https://kunpengcompute.github.io/2020/03/30/rang-da-shu-ju-sheng-tai-zai-arm-jia-gou-xia-geng-shun-hua/</id>
    <published>2020-03-30T12:23:45.000Z</published>
    <updated>2021-07-08T02:56:41.680Z</updated>
    
    <content type="html"><![CDATA[<p>作者：郑振宇</p><p>受疫情影响Linaro Connect 2020改为线上直播的Linaro Tech Days，笔者所在团队在该活动上介绍了自19年Q4以来笔者团队在各主流开源社区推广ARM生态所做的工作以及所取得的成果。直播活动约有120+与会者。</p><p>视频回放：<a href="https://static.linaro.org/connect/ltd20/videos/ltd20-104.mp4">视频连接</a><br>PPT：<a href="https://kunpengcompute.github.io/presentations/Linaro-tech-days-2020-03.pdf">LTD20-104 Make life easier for Big Data users on ARM - Our efforts and future plans</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;作者：郑振宇&lt;/p&gt;
&lt;p&gt;受疫情影响Linaro Connect 2020改为线上直播的Linaro Tech Days，笔者所在团队在该活动上介绍了自19年Q4以来笔者团队在各主流开源社区推广ARM生态所做的工作以及所取得的成果。直播活动约有120+与会者。&lt;/p&gt;
&lt;
      
    
    </summary>
    
    
      <category term="大数据" scheme="https://kunpengcompute.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="https://kunpengcompute.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="会议" scheme="https://kunpengcompute.github.io/tags/%E4%BC%9A%E8%AE%AE/"/>
    
  </entry>
  
  <entry>
    <title>Why ARM?</title>
    <link href="https://kunpengcompute.github.io/2020/03/30/why-arm/"/>
    <id>https://kunpengcompute.github.io/2020/03/30/why-arm/</id>
    <published>2020-03-30T10:56:54.000Z</published>
    <updated>2021-07-08T02:56:41.680Z</updated>
    
    <content type="html"><![CDATA[<p>作者: Krunal Bauskar 原文链接: <a href="https://mysqlonarm.github.io/Why-ARM/">https://mysqlonarm.github.io/Why-ARM/</a></p><p>ARM processors are everywhere. It is quite likely some of you may be reading this blog from an ARM powered device. Phone, IoT devices, consumer and home appliances, health-care devices, all are powered by ARM processors. ARM processors are known to be power efficient and so most of these devices that demands a long recharge cycle but less processing power started using them.</p><a id="more"></a><p>But this has changed in the past few years. More and more ARM processors are being used for high-end applications like database server, web server, application server, big data use-cases. They have already made their way to the data-centers as a server class machines. They are being looked upon as a cost effective option while running applications in cloud.</p><h2 id="ARM-ecosystem-evolution"><a href="#ARM-ecosystem-evolution" class="headerlink" title="ARM ecosystem evolution"></a><span style="color:#4885ed">ARM ecosystem evolution</span></h2><p>Few years back it was difficult to imagine that ARM would be used for running some high-end server class applications. There were 2 major reasons that I could think off:</p><ul><li>ARM were best suited for small handheld devices.</li><li>ARM ecosystem was limited around the specific product it supported.</li></ul><p>ARM ecosystem has really picked up well after some major OS providers added support for it including RedHat (CentOS), Ubuntu, Debian, Windows. This eased out porting of the major softwares to ARM. ARM community gave it a push to make sure most of the standard softwares are available on ARM viz. IDE, DB-server, Hadoop and all its variants from Apache Foundation, CI/CD software, Container, Virtualization, etc…</p><p>The ARM model that allows other vendors to license and develop their own ARM processors further helped fueled its popularity with more chip designers joining, collaborating and innovating.</p><p>Break-through came with major cloud providers like Amazon started providing ec2 instances (currently invitation only) based on ARM processors this means now everyone can boot an ARM instance and start developing/porting their software on ARM. This helped further grow the ecosystem.</p><h2 id="What-was-missing"><a href="#What-was-missing" class="headerlink" title="What was missing?"></a><span style="color:#4885ed">What was missing?</span></h2><p>Though most of these software have been ported to ARM they were not yet optimized for ARM. ARM has a weak memory model, can fit more cores in smaller space, difference in low-level instruction (for software that uses them), etc..</p><p>This was the start of the 2nd phase of ARM where the community/developer/user started moving from “running software on arm” -&gt; “optimizing software on arm”. I think this was a major win for the arm community when users started to think ARM seriously and started spending efforts on optimizing their software on ARM.</p><p>This (especially optimization) is a never ending process but I see first goal is to at-least be on par with x86. I purposely say “onpar” because each of architecture has its own USP so say if you port an enterprise class application to ARM and you can offer it to customer @ 50% of the cost (operating cost + initial investment) for 75% of the performance of x86 I think that would be still be attractive fit for most of the customers (especially given application are horizontally scalable). Of-course that doesn’t mean all applications run on ARM at reduced speed, in fact there are applications that run on ARM faster than x86 and since the optimization phase has just started in next few years a lot of applications would be running on ARM faster than other architectures.</p><h2 id="Go-Green"><a href="#Go-Green" class="headerlink" title="Go Green"></a><span style="color:#4885ed">Go Green</span></h2><p>It is everywhere and especially a matter of major concern for data-center operators (small or big). ARM being power efficient can save approximately 50% of the power compared to other architecture. This makes it help support Go-Green initiative.</p><h2 id="ARM-is-Next-Gen-processor"><a href="#ARM-is-Next-Gen-processor" class="headerlink" title="ARM is Next-Gen processor"></a><span style="color:#4885ed">ARM is Next-Gen processor</span></h2><p>It is interesting why I referred to it this way. Next generation kids are actively using kits like Andrino, Raspberry Pi, Odroid, Banana Pi, Asus tinker board, etc…. to build some of the next-gen system. These kids will be defining the next generation of computing. Given they started with ARM their social community has grown around ARM in the next few years there would be an army of ARM users/developers with a very active community.</p><p>All the groundwork and good things that are being built at this stage around ARM will be pushed to the next level once this workforce becomes active.</p><h2 id="ARM-in-Desktop-Laptop"><a href="#ARM-in-Desktop-Laptop" class="headerlink" title="ARM in Desktop/Laptop"></a><span style="color:#4885ed">ARM in Desktop/Laptop</span></h2><p>This is catching up fast and no wonder if we start seeing ARM based Desktop/PC workstation/Laptop (there are already few) commonly being used.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><span style="color:#4885ed">Conclusion</span></h2><p>The ARM Ecosystem looks a lot more fascinating and full of new challenges and opportunities. Current decade will be ruled by ARM based processors and it will be everywhere from tiny wearable devices to high-end movie experience, from auto-driving cycle/car to jumbo jet/space-craft. It is estimated that there would be 35 active ARM power devices per person. That’s Ocean of Opportunity.</p><p><em>If you have any comments feel free to drop an email (check about section)</em></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作者: Krunal Bauskar 原文链接: &lt;a href=&quot;https://mysqlonarm.github.io/Why-ARM/&quot;&gt;https://mysqlonarm.github.io/Why-ARM/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ARM processors are everywhere. It is quite likely some of you may be reading this blog from an ARM powered device. Phone, IoT devices, consumer and home appliances, health-care devices, all are powered by ARM processors. ARM processors are known to be power efficient and so most of these devices that demands a long recharge cycle but less processing power started using them.&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://kunpengcompute.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>鲲鹏计算团队博客开张啦，欢迎投稿！</title>
    <link href="https://kunpengcompute.github.io/2020/03/27/kun-peng-ji-suan-tuan-dui-bo-ke-kai-zhang-la-huan-ying-tou-gao/"/>
    <id>https://kunpengcompute.github.io/2020/03/27/kun-peng-ji-suan-tuan-dui-bo-ke-kai-zhang-la-huan-ying-tou-gao/</id>
    <published>2020-03-27T03:37:41.000Z</published>
    <updated>2021-07-08T02:56:41.680Z</updated>
    
    <content type="html"><![CDATA[<div class="tabs is-toggle"><ul><li class="is-active"><a onclick="onTabClick(event)"><span>中文</span></a></li><li><a onclick="onTabClick(event)"><span>English</span></a></li></ul></div><div id="中文" class="tab-content" style="display: block;"><p>我们将在这里分享关于鲲鹏计算相关的技术、开源、生态的点滴。</p><p>欢迎关注！欢迎转发！欢迎投稿！</p></div><div id="English" class="tab-content"><p>We are share something about technology, opensource and ecosystem of Kunpeng.</p><p>Welcome to join us!</p></div><style type="text/css">.content .tabs ul { margin: 0; }.content .tabs ul li { margin: 0; }.tab-content { display: none; }</style><script>function onTabClick (event) {    var tabTitle = $(event.currentTarget).children('span:last-child').text();    $('.article .content .tab-content').css('display', 'none');    $('.article .content .tabs li').removeClass('is-active');    $('#' + tabTitle).css('display', 'block');    $(event.currentTarget).parent().addClass('is-active');}</script><h2 id="如何投稿？"><a href="#如何投稿？" class="headerlink" title="如何投稿？"></a>如何投稿？</h2><p>非常简单，仅需要两步：</p><ol><li>点击<a href="https://github.com/kunpengcompute/kunpengcompute.github.io/issues">这里</a>，进入博客提交页面，我们使用Issue对博客进行管理，点击<code>New</code>进行投稿。</li><li>填写标题和内容，issue标题即为文章标题，issue内容即为文章内容，并发布请求。</li></ol><p>好了，至此你的投稿已经完成，你可以在<a href="https://github.com/kunpengcompute/kunpengcompute.github.io/issues">这里</a>看到你的投稿，并进行迭代修改。</p><h2 id="如何发布？"><a href="#如何发布？" class="headerlink" title="如何发布？"></a>如何发布？</h2><p>等到管理员审核通过后，会将你issue打上<code>publish</code>标签，之后，你内容就会自动同步在<a href="https://kunpengcompute.github.io/">博客</a>中啦！</p><p>来吧，还等什么？把你的干货分享起来！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;div class=&quot;tabs is-toggle&quot;&gt;&lt;ul&gt;
&lt;li class=&quot;is-active&quot;&gt;&lt;a onclick=&quot;onTabClick(event)&quot;&gt;
&lt;span&gt;中文&lt;/span&gt;
&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a onclick=&quot;onTabClick(
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
